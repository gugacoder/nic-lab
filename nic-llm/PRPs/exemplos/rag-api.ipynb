{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üîç RAG API - Sistema de Busca Sem√¢ntica\n\n## üìã O que este notebook faz\n\nEste notebook implementa um **sistema de RAG (Retrieval-Augmented Generation)** com busca sem√¢ntica h√≠brida:\n\n- üîç **Busca vetorial** usando embeddings BAAI/bge-m3\n- üè∑Ô∏è **Filtros por metadata** (repo, branch, arquivo)\n- üéØ **Pesquisa h√≠brida** combinando vetores e metadata\n- ‚ö° **Cache inteligente** para queries frequentes\n\n## üåê Endpoints Dispon√≠veis\n\n- `GET /api/v1/search` - Busca sem√¢ntica h√≠brida\n- `POST /api/v1/search` - Busca com payload complexo\n- `GET /api/v1/search/similar/{point_id}` - Documentos similares\n- `GET /api/v1/search/metadata` - Busca apenas por metadata\n- `GET /api/v1/search/stats` - Estat√≠sticas da collection\n- `GET /processa/info` - Informa√ß√µes sobre a Processa Sistemas via RAG\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configura√ß√£o e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport hashlib\nimport time\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import List, Dict, Optional, Any\nfrom collections import defaultdict\n\n# Carregar configura√ß√µes do ambiente (mesma abordagem do ETL)\nfrom dotenv import load_dotenv\n\n# Carregar .env\nenv_file = f\".env.{os.getenv('ENVIRONMENT', 'development')}\"\nif Path(f\"../{env_file}\").exists():\n    load_dotenv(f\"../{env_file}\")\nelif Path(\"../.env\").exists():\n    load_dotenv(\"../.env\")\n\n# Qdrant\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Filter, FieldCondition, MatchValue, Range\n)\n\n# Embeddings\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Configura√ß√£o do ambiente\nQDRANT_URL = os.getenv(\"QDRANT_URL\", \"http://qdrant.codrstudio.dev:6333\")\nQDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\nCOLLECTION_NAME = os.getenv(\"QDRANT_COLLECTION\", \"nic\")\nEMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"BAAI/bge-m3\")\n\n# Par√¢metros de busca\nDEFAULT_TOP_K = 20\nDEFAULT_SCORE_THRESHOLD = float(os.getenv(\"DEFAULT_SCORE_THRESHOLD\", \"0.6\"))\nMAX_TOP_K = 50\n\n# Cache\nCACHE_TTL = 300  # 5 minutos\nquery_cache = {}\n\nprint(f\"üîß Configura√ß√£o RAG API:\")\nprint(f\"  Qdrant URL: {QDRANT_URL}\")\nprint(f\"  Collection: {COLLECTION_NAME}\")\nprint(f\"  Modelo: {EMBEDDING_MODEL}\")\nprint(f\"  Default Top K: {DEFAULT_TOP_K}\")\nprint(f\"  Score Threshold: {DEFAULT_SCORE_THRESHOLD}\")\nprint(f\"  API Key: {'‚úÖ Carregada' if QDRANT_API_KEY else '‚ùå N√£o encontrada'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Conex√£o com Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar cliente Qdrant\n",
    "try:\n",
    "    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "    collection_info = client.get_collection(COLLECTION_NAME)\n",
    "    \n",
    "    print(f\"‚úÖ Conectado ao Qdrant\")\n",
    "    print(f\"üìä Collection '{COLLECTION_NAME}': {collection_info.points_count} pontos\")\n",
    "    \n",
    "    VECTOR_SIZE = collection_info.config.params.vectors.size\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao conectar ao Qdrant: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Modelo de Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo (singleton)\n",
    "_model_instance = None\n",
    "\n",
    "def get_embedding_model():\n",
    "    \"\"\"Retorna inst√¢ncia singleton do modelo de embeddings\"\"\"\n",
    "    global _model_instance\n",
    "    if _model_instance is None:\n",
    "        print(f\"ü§ñ Carregando modelo {EMBEDDING_MODEL}...\")\n",
    "        _model_instance = SentenceTransformer(EMBEDDING_MODEL)\n",
    "        print(f\"‚úÖ Modelo carregado: {_model_instance.get_sentence_embedding_dimension()} dimens√µes\")\n",
    "    return _model_instance\n",
    "\n",
    "def generate_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Gera embedding normalizado para um texto\"\"\"\n",
    "    model = get_embedding_model()\n",
    "    embedding = model.encode(text, normalize_embeddings=True, show_progress_bar=False)\n",
    "    return embedding.tolist()\n",
    "\n",
    "# Inicializar modelo\n",
    "get_embedding_model()\n",
    "print(f\"üìè Modelo pronto para uso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Filtros de Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metadata_filter(filters: Optional[Dict[str, Any]] = None) -> Optional[Filter]:\n",
    "    \"\"\"Constr√≥i filtros Qdrant a partir de par√¢metros de busca\"\"\"\n",
    "    if not filters:\n",
    "        return None\n",
    "    \n",
    "    conditions = []\n",
    "    \n",
    "    # Filtros de string exata\n",
    "    string_fields = ['repo', 'branch', 'relpath', 'source_document', 'lang', 'embed_model_major']\n",
    "    for field in string_fields:\n",
    "        if field in filters and filters[field]:\n",
    "            conditions.append(\n",
    "                FieldCondition(key=field, match=MatchValue(value=filters[field]))\n",
    "            )\n",
    "    \n",
    "    # Filtro de range de datas\n",
    "    if 'date_from' in filters or 'date_to' in filters:\n",
    "        date_range = {}\n",
    "        if 'date_from' in filters:\n",
    "            date_range['gte'] = filters['date_from']\n",
    "        if 'date_to' in filters:\n",
    "            date_range['lte'] = filters['date_to']\n",
    "        conditions.append(\n",
    "            FieldCondition(key='last_updated', range=Range(**date_range))\n",
    "        )\n",
    "    \n",
    "    return Filter(must=conditions) if conditions else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Fun√ß√£o Principal de Busca RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(\n",
    "    query: str,\n",
    "    top_k: int = DEFAULT_TOP_K,\n",
    "    score_threshold: float = DEFAULT_SCORE_THRESHOLD,\n",
    "    filters: Optional[Dict[str, Any]] = None,\n",
    "    include_embeddings: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Realiza busca h√≠brida RAG no Qdrant\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Validar par√¢metros\n",
    "    top_k = min(top_k, MAX_TOP_K)\n",
    "    \n",
    "    # Cache key\n",
    "    cache_key = hashlib.md5(\n",
    "        f\"{query}:{top_k}:{score_threshold}:{json.dumps(filters or {}, sort_keys=True)}:{include_embeddings}\".encode()\n",
    "    ).hexdigest()\n",
    "    \n",
    "    # Verificar cache\n",
    "    if cache_key in query_cache:\n",
    "        cached_result, cached_time = query_cache[cache_key]\n",
    "        if time.time() - cached_time < CACHE_TTL:\n",
    "            cached_result['from_cache'] = True\n",
    "            return cached_result\n",
    "    \n",
    "    # Gerar embedding da query\n",
    "    query_embedding = generate_embedding(query)\n",
    "    \n",
    "    # Construir filtros\n",
    "    search_filter = build_metadata_filter(filters)\n",
    "    \n",
    "    # Buscar no Qdrant\n",
    "    search_results = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query_vector=query_embedding,\n",
    "        query_filter=search_filter,\n",
    "        limit=top_k,\n",
    "        score_threshold=score_threshold,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    \n",
    "    # Formatar resultados\n",
    "    results = []\n",
    "    for hit in search_results:\n",
    "        result = {\n",
    "            'score': round(hit.score, 4),\n",
    "            'text': hit.payload.get('text', ''),\n",
    "            'metadata': {\n",
    "                'chunk_id': hit.payload.get('chunk_id'),\n",
    "                'chunk_index': hit.payload.get('chunk_index'),\n",
    "                'source_document': hit.payload.get('source_document'),\n",
    "                'repo': hit.payload.get('repo'),\n",
    "                'branch': hit.payload.get('branch'),\n",
    "                'commit': hit.payload.get('commit'),\n",
    "                'last_updated': hit.payload.get('last_updated')\n",
    "            },\n",
    "            'point_id': hit.id\n",
    "        }\n",
    "        \n",
    "        # Adicionar highlights\n",
    "        highlights = []\n",
    "        query_words = query.lower().split()\n",
    "        text_lower = result['text'].lower()\n",
    "        for word in query_words:\n",
    "            if len(word) > 2 and word in text_lower:\n",
    "                idx = text_lower.index(word)\n",
    "                start = max(0, idx - 30)\n",
    "                end = min(len(result['text']), idx + len(word) + 30)\n",
    "                highlight = result['text'][start:end]\n",
    "                if start > 0:\n",
    "                    highlight = '...' + highlight\n",
    "                if end < len(result['text']):\n",
    "                    highlight = highlight + '...'\n",
    "                highlights.append(highlight)\n",
    "        \n",
    "        if highlights:\n",
    "            result['highlights'] = highlights[:3]\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    # Preparar resposta\n",
    "    response = {\n",
    "        'query': query,\n",
    "        'total_results': len(results),\n",
    "        'results': results,\n",
    "        'search_metadata': {\n",
    "            'model': EMBEDDING_MODEL,\n",
    "            'collection': COLLECTION_NAME,\n",
    "            'top_k': top_k,\n",
    "            'score_threshold': score_threshold,\n",
    "            'filters_applied': filters or {},\n",
    "            'search_time_ms': round((time.time() - start_time) * 1000, 2),\n",
    "            'from_cache': False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Adicionar embeddings se solicitado\n",
    "    if include_embeddings:\n",
    "        response['query_embedding'] = query_embedding\n",
    "        response['query_embedding_info'] = {\n",
    "            'dimensions': len(query_embedding),\n",
    "            'model': EMBEDDING_MODEL,\n",
    "            'normalized': True,\n",
    "            'magnitude': round(float(np.linalg.norm(query_embedding)), 6)\n",
    "        }\n",
    "    \n",
    "    # Cache da resposta\n",
    "    query_cache[cache_key] = (response, time.time())\n",
    "    \n",
    "    # Limpar cache antigo\n",
    "    if len(query_cache) > 100:\n",
    "        sorted_cache = sorted(query_cache.items(), key=lambda x: x[1][1])\n",
    "        for key, _ in sorted_cache[:50]:\n",
    "            del query_cache[key]\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Endpoint: Busca GET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GET /api/v1/search\ndef parse_query_params():\n    \"\"\"Parse query parameters from REQUEST global\"\"\"\n    try:\n        request = REQUEST\n        query_string = request.get('query', {})\n        \n        # Extrair query com fallback seguro\n        query = \"\"\n        if 'q' in query_string:\n            q_val = query_string['q']\n            query = q_val[0] if isinstance(q_val, list) and q_val else str(q_val) if q_val else \"\"\n        \n        # Extrair top_k com valida√ß√£o\n        top_k = DEFAULT_TOP_K\n        if 'top_k' in query_string:\n            try:\n                tk_val = query_string['top_k']\n                top_k = int(tk_val[0] if isinstance(tk_val, list) else tk_val)\n                top_k = max(1, min(top_k, MAX_TOP_K))  # Clamp entre 1 e MAX_TOP_K\n            except (ValueError, IndexError):\n                top_k = DEFAULT_TOP_K\n        \n        # Extrair score_threshold com valida√ß√£o\n        score_threshold = DEFAULT_SCORE_THRESHOLD\n        if 'score_threshold' in query_string:\n            try:\n                st_val = query_string['score_threshold']\n                score_threshold = float(st_val[0] if isinstance(st_val, list) else st_val)\n                score_threshold = max(0.0, min(score_threshold, 1.0))  # Clamp entre 0 e 1\n            except (ValueError, IndexError):\n                score_threshold = DEFAULT_SCORE_THRESHOLD\n        \n        # Extrair filtros com valida√ß√£o\n        filters = {}\n        for key in ['repo', 'branch', 'relpath', 'source_document', 'lang']:\n            if key in query_string:\n                try:\n                    val = query_string[key]\n                    filter_val = val[0] if isinstance(val, list) and val else str(val) if val else \"\"\n                    if filter_val.strip():\n                        filters[key] = filter_val.strip()\n                except (IndexError, AttributeError):\n                    continue\n        \n        return query, top_k, score_threshold, filters\n        \n    except (NameError, KeyError, AttributeError) as e:\n        print(f\"‚ö†Ô∏è Erro no parse de query params: {e}\")\n        return \"Quem √© a processa?\", DEFAULT_TOP_K, DEFAULT_SCORE_THRESHOLD, {}\n\n# Executar busca\nquery, top_k, score_threshold, filters = parse_query_params()\n\nif not query:\n    response = {\n        'error': 'Query parameter \"q\" is required',\n        'example': '/api/v1/search?q=texto+de+busca&top_k=20'\n    }\nelse:\n    response = hybrid_search(\n        query=query,\n        top_k=top_k,\n        score_threshold=score_threshold,\n        filters=filters if filters else None\n    )\n\nprint(json.dumps(response, indent=2, ensure_ascii=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Endpoint: Busca POST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# POST /api/v1/search\ndef parse_post_body():\n    \"\"\"Parse JSON body from POST request\"\"\"\n    try:\n        request = REQUEST\n        body = request.get('body', '{}')\n        if isinstance(body, str):\n            return json.loads(body)\n        elif isinstance(body, dict):\n            return body\n        else:\n            return {'query': 'Quem √© a processa?', 'top_k': DEFAULT_TOP_K}\n    except (NameError, json.JSONDecodeError, AttributeError) as e:\n        print(f\"‚ö†Ô∏è Erro no parse do body: {e}\")\n        return {'query': 'Quem √© a processa?', 'top_k': DEFAULT_TOP_K}\n\nrequest_data = parse_post_body()\n\nif 'query' not in request_data or not request_data['query'].strip():\n    response = {\n        'error': 'Field \"query\" is required in request body',\n        'example': {\n            'query': 'texto de busca',\n            'top_k': 20,\n            'score_threshold': 0.7,\n            'include_embeddings': True,\n            'filters': {'repo': 'nic/documentacao', 'branch': 'main'}\n        }\n    }\nelse:\n    # Validar e limitar par√¢metros\n    top_k = max(1, min(request_data.get('top_k', DEFAULT_TOP_K), MAX_TOP_K))\n    score_threshold = max(0.0, min(request_data.get('score_threshold', DEFAULT_SCORE_THRESHOLD), 1.0))\n    \n    response = hybrid_search(\n        query=request_data['query'].strip(),\n        top_k=top_k,\n        score_threshold=score_threshold,\n        filters=request_data.get('filters'),\n        include_embeddings=request_data.get('include_embeddings', False)\n    )\n\nprint(json.dumps(response, indent=2, ensure_ascii=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Endpoint: Busca por Similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET /api/v1/search/similar/<point_id>\n",
    "def find_similar_documents(point_id: int, top_k: int = DEFAULT_TOP_K, score_threshold: float = 0.8) -> Dict[str, Any]:\n",
    "    \"\"\"Encontra documentos similares a um documento existente\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Buscar o ponto original\n",
    "        original_points = client.retrieve(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            ids=[point_id],\n",
    "            with_vectors=True,\n",
    "            with_payload=True\n",
    "        )\n",
    "        \n",
    "        if not original_points:\n",
    "            return {'error': f'Point {point_id} not found', 'point_id': point_id}\n",
    "        \n",
    "        original = original_points[0]\n",
    "        \n",
    "        # Buscar similares usando o vetor do documento\n",
    "        similar_results = client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=original.vector,\n",
    "            limit=top_k + 1,  # +1 porque o pr√≥prio documento ser√° retornado\n",
    "            score_threshold=score_threshold,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        \n",
    "        # Filtrar o pr√≥prio documento dos resultados\n",
    "        results = []\n",
    "        for hit in similar_results:\n",
    "            if hit.id != point_id:\n",
    "                results.append({\n",
    "                    'score': round(hit.score, 4),\n",
    "                    'text': hit.payload.get('text', '')[:200] + '...',\n",
    "                    'metadata': {\n",
    "                        'chunk_id': hit.payload.get('chunk_id'),\n",
    "                        'source_document': hit.payload.get('source_document'),\n",
    "                        'chunk_index': hit.payload.get('chunk_index')\n",
    "                    },\n",
    "                    'point_id': hit.id\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'original': {\n",
    "                'point_id': point_id,\n",
    "                'text': original.payload.get('text', '')[:200] + '...',\n",
    "                'source_document': original.payload.get('source_document')\n",
    "            },\n",
    "            'similar_documents': results[:top_k],\n",
    "            'total_similar': len(results),\n",
    "            'search_metadata': {\n",
    "                'collection': COLLECTION_NAME,\n",
    "                'score_threshold': score_threshold,\n",
    "                'search_time_ms': round((time.time() - start_time) * 1000, 2)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e), 'point_id': point_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Endpoint: Busca por Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET /api/v1/search/metadata\n",
    "def search_by_metadata(filters: Dict[str, Any], limit: int = 20, offset: int = 0) -> Dict[str, Any]:\n",
    "    \"\"\"Busca apenas por filtros de metadata, sem usar vetores\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not filters:\n",
    "        return {\n",
    "            'error': 'At least one filter is required',\n",
    "            'available_filters': ['repo', 'branch', 'relpath', 'source_document', 'lang']\n",
    "        }\n",
    "    \n",
    "    search_filter = build_metadata_filter(filters)\n",
    "    if not search_filter:\n",
    "        return {'error': 'Invalid filters provided', 'filters': filters}\n",
    "    \n",
    "    # Buscar com scroll (pagina√ß√£o)\n",
    "    results, next_offset = client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        scroll_filter=search_filter,\n",
    "        limit=limit,\n",
    "        offset=offset,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    \n",
    "    # Agrupar por documento\n",
    "    documents = defaultdict(list)\n",
    "    for point in results:\n",
    "        doc_name = point.payload.get('source_document', 'unknown')\n",
    "        documents[doc_name].append({\n",
    "            'chunk_index': point.payload.get('chunk_index'),\n",
    "            'text_preview': point.payload.get('text', '')[:100] + '...',\n",
    "            'point_id': point.id\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'filters': filters,\n",
    "        'total_points': len(results),\n",
    "        'documents': [\n",
    "            {\n",
    "                'source_document': doc,\n",
    "                'chunks_count': len(chunks),\n",
    "                'chunks': sorted(chunks, key=lambda x: x['chunk_index'])[:3]\n",
    "            }\n",
    "            for doc, chunks in documents.items()\n",
    "        ],\n",
    "        'pagination': {\n",
    "            'limit': limit,\n",
    "            'offset': offset,\n",
    "            'has_more': next_offset is not None,\n",
    "            'next_offset': next_offset\n",
    "        },\n",
    "        'search_metadata': {\n",
    "            'collection': COLLECTION_NAME,\n",
    "            'search_time_ms': round((time.time() - start_time) * 1000, 2)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Endpoint: Estat√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET /api/v1/search/stats\n",
    "def get_collection_stats() -> Dict[str, Any]:\n",
    "    \"\"\"Retorna estat√≠sticas da collection\"\"\"\n",
    "    try:\n",
    "        collection_info = client.get_collection(COLLECTION_NAME)\n",
    "        \n",
    "        # Campos dispon√≠veis\n",
    "        sample_points = client.scroll(collection_name=COLLECTION_NAME, limit=1, with_payload=True, with_vectors=False)[0]\n",
    "        available_fields = list(sample_points[0].payload.keys()) if sample_points else []\n",
    "        \n",
    "        return {\n",
    "            'collection': {\n",
    "                'name': COLLECTION_NAME,\n",
    "                'points_count': collection_info.points_count,\n",
    "                'status': collection_info.status,\n",
    "                'vector_size': collection_info.config.params.vectors.size,\n",
    "                'distance_metric': 'COSINE'\n",
    "            },\n",
    "            'model': {\n",
    "                'name': EMBEDDING_MODEL,\n",
    "                'dimensions': VECTOR_SIZE\n",
    "            },\n",
    "            'search_config': {\n",
    "                'default_top_k': DEFAULT_TOP_K,\n",
    "                'max_top_k': MAX_TOP_K,\n",
    "                'default_score_threshold': DEFAULT_SCORE_THRESHOLD\n",
    "            },\n",
    "            'cache': {\n",
    "                'entries': len(query_cache),\n",
    "                'ttl_seconds': CACHE_TTL\n",
    "            },\n",
    "            'available_metadata_fields': available_fields,\n",
    "            'api_version': '1.0.0',\n",
    "            'timestamp': datetime.now().isoformat() + 'Z'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e), 'collection': COLLECTION_NAME}\n",
    "\n",
    "stats = get_collection_stats()\n",
    "print(json.dumps(stats, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "source": "# GET /processa/info\ndef get_processa_info():\n    \"\"\"Obt√©m informa√ß√µes sobre a Processa Sistemas usando RAG\"\"\"\n    \n    # Parse query parameter opcional\n    try:\n        request = REQUEST\n        query_params = request.get('args', {})\n        custom_query = query_params.get('query', [None])[0] if 'query' in query_params else None\n    except (NameError, KeyError, AttributeError):\n        custom_query = None\n    \n    # Query padr√£o ou customizada\n    query = custom_query or \"Quem √© a Processa Sistemas\"\n    \n    # Buscar usando a fun√ß√£o hybrid_search existente\n    search_results = hybrid_search(\n        query=query,\n        top_k=10,\n        score_threshold=0.5,\n        filters=None,\n        include_embeddings=False\n    )\n    \n    # Processar resultados para formato espec√≠fico da Processa\n    if search_results.get('results'):\n        results = search_results['results']\n        \n        # Extrair descri√ß√£o principal\n        description = results[0]['text'] if results else \"Informa√ß√£o n√£o dispon√≠vel\"\n        if len(description) > 500:\n            description = description[:497] + \"...\"\n        \n        # Extrair pontos principais\n        key_points = []\n        seen = set()\n        for result in results[:5]:\n            sentences = result['text'].split('.')\n            for sentence in sentences[:2]:\n                sentence = sentence.strip()\n                if sentence and len(sentence) > 20 and sentence not in seen:\n                    key_points.append(sentence)\n                    seen.add(sentence)\n                    if len(key_points) >= 5:\n                        break\n            if len(key_points) >= 5:\n                break\n        \n        # Extrair fontes\n        sources = []\n        seen_sources = set()\n        for result in results[:5]:\n            source_name = result['metadata'].get('source_document', 'unknown')\n            if source_name not in seen_sources:\n                sources.append({\n                    \"document\": source_name,\n                    \"relevance_score\": result['score'],\n                    \"metadata\": result['metadata']\n                })\n                seen_sources.add(source_name)\n        \n        response = {\n            \"status\": \"success\",\n            \"query\": query,\n            \"company_info\": {\n                \"name\": \"Processa Sistemas\",\n                \"description\": description,\n                \"key_points\": key_points[:5],\n                \"sources\": sources\n            },\n            \"search_metadata\": {\n                \"total_results\": search_results['total_results'],\n                \"top_score\": results[0]['score'] if results else 0,\n                \"collection\": COLLECTION_NAME,\n                \"model\": EMBEDDING_MODEL,\n                \"search_time_ms\": search_results['search_metadata']['search_time_ms']\n            },\n            \"timestamp\": datetime.now().isoformat()\n        }\n    else:\n        response = {\n            \"status\": \"no_results\",\n            \"message\": \"Nenhuma informa√ß√£o encontrada sobre a Processa\",\n            \"query\": query,\n            \"timestamp\": datetime.now().isoformat()\n        }\n    \n    return response\n\n# Executar e retornar resultado\nresult = get_processa_info()\nprint(json.dumps(result, indent=2, ensure_ascii=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üè¢ Endpoint: Informa√ß√µes da Processa\n\n`GET /processa/info`",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}