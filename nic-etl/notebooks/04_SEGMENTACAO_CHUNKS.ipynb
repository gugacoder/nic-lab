{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”ª ETAPA 4: SEGMENTAÃ‡ÃƒO INTELIGENTE EM CHUNKS\n",
    "\n",
    "## ğŸ¯ **O que esta etapa faz**\n",
    "Divide o texto extraÃ­do em pedaÃ§os menores (chunks) de tamanho adequado para processamento de embeddings, preservando o contexto e a semÃ¢ntica.\n",
    "\n",
    "## ğŸ¤” **Por que esta etapa Ã© necessÃ¡ria**\n",
    "Textos longos sÃ£o difÃ­ceis de processar eficientemente. Precisamos:\n",
    "- âœ‚ï¸ **Dividir em pedaÃ§os menores** para modelos de embedding\n",
    "- ğŸ”— **Manter contexto** com sobreposiÃ§Ã£o entre chunks\n",
    "- ğŸ“ **Respeitar limites** de tokens dos modelos (500 tokens)\n",
    "- ğŸ§  **Preservar semÃ¢ntica** nÃ£o cortando no meio de frases\n",
    "\n",
    "## âš™ï¸ **Como funciona**\n",
    "1. **Carrega texto processado** da etapa anterior\n",
    "2. **Tokeniza inteligentemente** respeitando palavras e frases\n",
    "3. **Cria chunks** de ~500 tokens com overlap de ~100 tokens\n",
    "4. **Ajusta limites** para nÃ£o cortar frases no meio\n",
    "5. **Preserva metadados** de origem e posiÃ§Ã£o\n",
    "6. **Valida qualidade** dos chunks gerados\n",
    "\n",
    "## ğŸ“Š **ParÃ¢metros que vocÃª pode ajustar**\n",
    "- `chunk_size`: Tamanho alvo em tokens (padrÃ£o: 500)\n",
    "- `overlap_size`: SobreposiÃ§Ã£o em tokens (padrÃ£o: 100)\n",
    "- `respect_sentences`: NÃ£o cortar frases (padrÃ£o: True)\n",
    "- `min_chunk_size`: Tamanho mÃ­nimo vÃ¡lido (padrÃ£o: 50)\n",
    "- `max_chunk_size`: Tamanho mÃ¡ximo permitido (padrÃ£o: 750)\n",
    "\n",
    "## ğŸ‘ï¸ **O que esperar de saÃ­da**\n",
    "- ğŸ“ **Pasta `chunks/`** com chunks organizados\n",
    "- ğŸ“„ **Arquivos `*_chunks.json`** com chunks e metadados\n",
    "- ğŸ“Š **EstatÃ­sticas** de segmentaÃ§Ã£o por documento\n",
    "- ğŸ”— **Mapeamento** de chunks para documentos originais\n",
    "\n",
    "## âš ï¸ **Pontos importantes**\n",
    "- **Requer Etapa 3** (Processamento Docling) concluÃ­da primeiro\n",
    "- **TokenizaÃ§Ã£o** Ã© aproximada (baseada em espaÃ§os)\n",
    "- **Overlap** garante continuidade de contexto\n",
    "- **Modo independente** disponÃ­vel para testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ›¡ï¸ VerificaÃ§Ã£o de DependÃªncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ›¡ï¸ VERIFICAÃ‡ÃƒO AUTOMÃTICA DE DEPENDÃŠNCIAS\nimport sys\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Adicionar biblioteca ao path\nsys.path.insert(0, str(Path().parent / \"src\"))\n\nfrom pipeline_utils import (\n    PipelineState, \n    check_prerequisites, \n    show_pipeline_progress\n)\n\nprint(\"ğŸ”ª ETAPA 4: SEGMENTAÃ‡ÃƒO INTELIGENTE EM CHUNKS\")\nprint(\"=\" * 60)\nprint(f\"ğŸ•’ Iniciado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# Verificar dependÃªncias\nCURRENT_STAGE = 4\nprerequisites_ok = check_prerequisites(CURRENT_STAGE)\n\nif not prerequisites_ok:\n    print(\"\\nâŒ ERRO: DependÃªncias nÃ£o atendidas!\")\n    print(\"ğŸ“‹ Execute primeiro as etapas anteriores:\")\n    print(\"   1. 01_FUNDACAO_PREPARACAO.ipynb\")\n    print(\"   2. 02_COLETA_GITLAB.ipynb\")\n    print(\"   3. 03_PROCESSAMENTO_DOCLING.ipynb\")\n    \n    show_pipeline_progress()\n    raise RuntimeError(\"DependÃªncias nÃ£o atendidas - execute etapas anteriores primeiro\")\n\nprint(\"\\nâœ… DependÃªncias verificadas - pode prosseguir\")\nprint(\"ğŸ“ˆ Progresso atual do pipeline:\")\nshow_pipeline_progress()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ConfiguraÃ§Ã£o de Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‹ CONFIGURAÃ‡ÃƒO DOS PARÃ‚METROS DE CHUNKING\n",
    "print(\"ğŸ“‹ Configurando parÃ¢metros de segmentaÃ§Ã£o...\")\n",
    "\n",
    "# ğŸ¯ PARÃ‚METROS AJUSTÃVEIS\n",
    "chunking_config = {\n",
    "    # Tamanhos\n",
    "    \"chunk_size\": 500,          # Tokens por chunk\n",
    "    \"overlap_size\": 100,        # Overlap entre chunks\n",
    "    \"min_chunk_size\": 50,       # Tamanho mÃ­nimo vÃ¡lido\n",
    "    \"max_chunk_size\": 750,      # Tamanho mÃ¡ximo permitido\n",
    "    \n",
    "    # Comportamento\n",
    "    \"respect_sentences\": True,   # NÃ£o cortar frases\n",
    "    \"respect_paragraphs\": False, # Pode cortar parÃ¡grafos\n",
    "    \"preserve_structure\": True,  # Manter info de estrutura\n",
    "    \n",
    "    # Processamento\n",
    "    \"batch_size\": 8,             # Documentos processados simultaneamente\n",
    "    \"skip_empty_chunks\": True,   # Descartar chunks vazios\n",
    "    \n",
    "    # SaÃ­da\n",
    "    \"output_dir\": \"../pipeline_data/chunks\",\n",
    "    \"overwrite_existing\": True\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“Š ParÃ¢metros de chunking configurados:\")\n",
    "print(f\"   ğŸ“ Tamanho do chunk: {chunking_config['chunk_size']} tokens\")\n",
    "print(f\"   ğŸ”— Overlap: {chunking_config['overlap_size']} tokens\")\n",
    "print(f\"   ğŸ“ Faixa vÃ¡lida: {chunking_config['min_chunk_size']}-{chunking_config['max_chunk_size']} tokens\")\n",
    "print(f\"   ğŸ“ Respeitar frases: {chunking_config['respect_sentences']}\")\n",
    "print(f\"   ğŸ—ï¸ Preservar estrutura: {chunking_config['preserve_structure']}\")\n",
    "print(f\"   ğŸ“¦ Lote: {chunking_config['batch_size']} documentos simultÃ¢neos\")\n",
    "print(f\"   ğŸ“ SaÃ­da: {chunking_config['output_dir']}\")\n",
    "\n",
    "# Preparar diretÃ³rio de saÃ­da\n",
    "from pipeline_utils import ensure_directory\n",
    "ensure_directory(chunking_config[\"output_dir\"])\n",
    "\n",
    "print(\"\\nâœ… ConfiguraÃ§Ã£o de chunking pronta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“„ Carregamento dos Documentos Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“„ CARREGAMENTO DOS DOCUMENTOS PROCESSADOS\n",
    "print(\"ğŸ“„ Carregando documentos da etapa anterior...\")\n",
    "\n",
    "# Carregar dados da etapa 3 (Docling)\n",
    "state = PipelineState()\n",
    "try:\n",
    "    docling_data = state.load_stage_data(3)\n",
    "    print(\"âœ… Dados carregados da Etapa 3 (Docling)\")\n",
    "    \n",
    "    processed_docs = docling_data[\"processed_documents\"]\n",
    "    text_files = docling_data[\"next_stage_instructions\"][\"text_files_to_chunk\"]\n",
    "    \n",
    "    print(f\"   ğŸ“„ Documentos processados: {len(processed_docs)}\")\n",
    "    print(f\"   ğŸ“ Arquivos de texto: {len(text_files)}\")\n",
    "    print(f\"   ğŸ“ DiretÃ³rio origem: {docling_data['next_stage_instructions']['processed_directory']}\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"âŒ Erro ao carregar dados: {e}\")\n",
    "    raise\n",
    "\n",
    "# Verificar se arquivos de texto existem\n",
    "available_text_files = []\n",
    "for text_file_path in text_files:\n",
    "    file_path = Path(text_file_path)\n",
    "    if file_path.exists():\n",
    "        file_size = file_path.stat().st_size\n",
    "        if file_size > 0:\n",
    "            available_text_files.append(text_file_path)\n",
    "            print(f\"   âœ… {file_path.name} ({file_size:,} bytes)\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸ {file_path.name} (arquivo vazio)\")\n",
    "    else:\n",
    "        print(f\"   âŒ {file_path.name} (nÃ£o encontrado)\")\n",
    "\n",
    "if not available_text_files:\n",
    "    print(\"\\nâŒ Nenhum arquivo de texto disponÃ­vel para chunking\")\n",
    "    raise ValueError(\"Nenhum arquivo de texto para processar\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Arquivos prontos para chunking: {len(available_text_files)}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ SegmentaÃ§Ã£o em Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ‚ï¸ EXECUÃ‡ÃƒO DA SEGMENTAÃ‡ÃƒO EM CHUNKS\n",
    "print(\"âœ‚ï¸ Iniciando segmentaÃ§Ã£o em chunks...\")\n",
    "\n",
    "from chunking_utils import batch_process_documents_chunking\n",
    "from pipeline_utils import format_file_size\n",
    "import json\n",
    "\n",
    "print(f\"\\nğŸš€ Processando {len(available_text_files)} documentos...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Executar chunking em lote\n",
    "start_time = datetime.now()\n",
    "\n",
    "chunking_results = batch_process_documents_chunking(\n",
    "    document_list=available_text_files,\n",
    "    output_dir=chunking_config[\"output_dir\"],\n",
    "    chunk_size=chunking_config[\"chunk_size\"],\n",
    "    overlap_size=chunking_config[\"overlap_size\"]\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "chunking_duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š RESULTADO DA SEGMENTAÃ‡ÃƒO:\")\n",
    "\n",
    "# Contar sucessos e falhas\n",
    "successful_chunking = [r for r in chunking_results if r[\"status\"] == \"success\"]\n",
    "failed_chunking = [r for r in chunking_results if r[\"status\"] == \"failed\"]\n",
    "\n",
    "total_chunks = sum(r[\"chunk_count\"] for r in successful_chunking)\n",
    "total_tokens = sum(r[\"total_tokens\"] for r in successful_chunking)\n",
    "\n",
    "print(f\"   ğŸ“„ Documentos processados: {len(chunking_results)}\")\n",
    "print(f\"   âœ… Sucessos: {len(successful_chunking)}\")\n",
    "print(f\"   âŒ Falhas: {len(failed_chunking)}\")\n",
    "print(f\"   ğŸ”ª Total de chunks: {total_chunks:,}\")\n",
    "print(f\"   ğŸ¯ Total de tokens: {total_tokens:,}\")\n",
    "print(f\"   â±ï¸ Tempo total: {chunking_duration:.1f} segundos\")\n",
    "\n",
    "if successful_chunking:\n",
    "    avg_chunks_per_doc = total_chunks / len(successful_chunking)\n",
    "    avg_tokens_per_chunk = total_tokens / total_chunks if total_chunks > 0 else 0\n",
    "    print(f\"   ğŸ“ˆ MÃ©dia: {avg_chunks_per_doc:.1f} chunks por documento\")\n",
    "    print(f\"   ğŸ“ MÃ©dia: {avg_tokens_per_chunk:.0f} tokens por chunk\")\n",
    "\n",
    "# Mostrar resultados de sucesso\n",
    "if successful_chunking:\n",
    "    print(\"\\nğŸ“ Documentos segmentados com sucesso:\")\n",
    "    for result in successful_chunking[:10]:  # Mostrar apenas os primeiros 10\n",
    "        filename = Path(result[\"source_document\"]).name\n",
    "        chunks = result[\"chunk_count\"]\n",
    "        tokens = result[\"total_tokens\"]\n",
    "        print(f\"   âœ… {filename}: {chunks} chunks, {tokens:,} tokens\")\n",
    "    \n",
    "    if len(successful_chunking) > 10:\n",
    "        remaining = len(successful_chunking) - 10\n",
    "        print(f\"   ... e mais {remaining} documentos\")\n",
    "\n",
    "# Mostrar falhas se houver\n",
    "if failed_chunking:\n",
    "    print(f\"\\nâš ï¸ Documentos com problemas ({len(failed_chunking)}):\")\n",
    "    for result in failed_chunking[:5]:  # Mostrar apenas os primeiros 5\n",
    "        filename = Path(result[\"source_document\"]).name\n",
    "        error = result[\"error\"]\n",
    "        print(f\"   âŒ {filename}: {error}\")\n",
    "    \n",
    "    if len(failed_chunking) > 5:\n",
    "        remaining = len(failed_chunking) - 5\n",
    "        print(f\"   ... e mais {remaining} erros\")\n",
    "\n",
    "print(\"\\nâœ… SegmentaÃ§Ã£o concluÃ­da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ValidaÃ§Ã£o e Qualidade dos Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š VALIDAÃ‡ÃƒO E ANÃLISE DE QUALIDADE DOS CHUNKS\n",
    "print(\"ğŸ“Š Validando qualidade dos chunks...\")\n",
    "\n",
    "from chunking_utils import validate_chunks\n",
    "from collections import defaultdict\n",
    "\n",
    "# Validar chunks gerados\n",
    "validation_result = validate_chunks(chunking_config[\"output_dir\"])\n",
    "\n",
    "if validation_result[\"valid\"]:\n",
    "    print(\"\\nâœ… ValidaÃ§Ã£o bem-sucedida\")\n",
    "    print(f\"   ğŸ“„ Arquivos de chunks: {validation_result['chunk_files_count']}\")\n",
    "    print(f\"   ğŸ”ª Total de chunks: {validation_result['total_chunks']:,}\")\n",
    "    print(f\"   ğŸ¯ Total de tokens: {validation_result['total_tokens']:,}\")\n",
    "    print(f\"   ğŸ“ MÃ©dia tokens/chunk: {validation_result['avg_tokens_per_chunk']:.0f}\")\nelse:\n",
    "    print(f\"\\nâŒ Problema na validaÃ§Ã£o: {validation_result.get('error', 'Erro desconhecido')}\")\n",
    "\n",
    "# AnÃ¡lise detalhada de qualidade\n",
    "chunks_dir = Path(chunking_config[\"output_dir\"])\n",
    "chunk_files = list(chunks_dir.glob(\"*_chunks.json\"))\n",
    "\n",
    "quality_stats = {\n",
    "    \"size_distribution\": defaultdict(int),\n",
    "    \"total_chunks\": 0,\n",
    "    \"total_tokens\": 0,\n",
    "    \"min_chunk_size\": float('inf'),\n",
    "    \"max_chunk_size\": 0,\n",
    "    \"chunks_with_overlap\": 0,\n",
    "    \"empty_chunks\": 0\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“ˆ AnÃ¡lise detalhada de {len(chunk_files)} arquivos de chunks:\")\n",
    "\n",
    "for chunk_file in chunk_files:\n",
    "    try:\n",
    "        with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "            chunk_data = json.load(f)\n",
    "        \n",
    "        chunks = chunk_data.get(\"chunks\", [])\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            token_count = chunk.get(\"token_count\", 0)\n",
    "            has_overlap = chunk.get(\"has_overlap\", False)\n",
    "            text = chunk.get(\"text\", \"\")\n",
    "            \n",
    "            quality_stats[\"total_chunks\"] += 1\n",
    "            quality_stats[\"total_tokens\"] += token_count\n",
    "            \n",
    "            # DistribuiÃ§Ã£o por tamanho\n",
    "            size_bucket = (token_count // 100) * 100  # Buckets de 100 tokens\n",
    "            quality_stats[\"size_distribution\"][size_bucket] += 1\n",
    "            \n",
    "            # Min/Max\n",
    "            quality_stats[\"min_chunk_size\"] = min(quality_stats[\"min_chunk_size\"], token_count)\n",
    "            quality_stats[\"max_chunk_size\"] = max(quality_stats[\"max_chunk_size\"], token_count)\n",
    "            \n",
    "            # Overlap\n",
    "            if has_overlap:\n",
    "                quality_stats[\"chunks_with_overlap\"] += 1\n",
    "            \n",
    "            # Chunks vazios\n",
    "            if not text.strip():\n",
    "                quality_stats[\"empty_chunks\"] += 1\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Erro ao analisar {chunk_file.name}: {e}\")\n",
    "\n",
    "# EstatÃ­sticas de qualidade\n",
    "if quality_stats[\"total_chunks\"] > 0:\n",
    "    avg_size = quality_stats[\"total_tokens\"] / quality_stats[\"total_chunks\"]\n",
    "    overlap_percentage = (quality_stats[\"chunks_with_overlap\"] / quality_stats[\"total_chunks\"]) * 100\n",
    "    \n",
    "    print(f\"\\nğŸ“ EstatÃ­sticas de tamanho:\")\n",
    "    print(f\"   ğŸ“Š Chunks totais: {quality_stats['total_chunks']:,}\")\n",
    "    print(f\"   ğŸ“ Tamanho mÃ©dio: {avg_size:.0f} tokens\")\n",
    "    print(f\"   ğŸ“‰ Menor chunk: {quality_stats['min_chunk_size']} tokens\")\n",
    "    print(f\"   ğŸ“ˆ Maior chunk: {quality_stats['max_chunk_size']} tokens\")\n",
    "    \n",
    "    print(f\"\\nğŸ”— AnÃ¡lise de overlap:\")\n",
    "    print(f\"   Chunks com overlap: {quality_stats['chunks_with_overlap']:,} ({overlap_percentage:.1f}%)\")\n",
    "    \n",
    "    if quality_stats[\"empty_chunks\"] > 0:\n",
    "        print(f\"\\nâš ï¸ Problemas encontrados:\")\n",
    "        print(f\"   Chunks vazios: {quality_stats['empty_chunks']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š DistribuiÃ§Ã£o por tamanho:\")\n",
    "    for size_bucket in sorted(quality_stats[\"size_distribution\"].keys()):\n",
    "        count = quality_stats[\"size_distribution\"][size_bucket]\n",
    "        percentage = (count / quality_stats[\"total_chunks\"]) * 100\n",
    "        print(f\"   {size_bucket}-{size_bucket+99} tokens: {count:,} chunks ({percentage:.1f}%)\")\n",
    "\n",
    "# Preparar lista de arquivos de chunks para prÃ³xima etapa\n",
    "chunk_files_info = []\n",
    "for result in successful_chunking:\n",
    "    chunk_files_info.append({\n",
    "        \"source_document\": result[\"source_document\"],\n",
    "        \"chunks_file\": result[\"chunks_file\"],\n",
    "        \"chunk_count\": result[\"chunk_count\"],\n",
    "        \"total_tokens\": result[\"total_tokens\"]\n",
    "    })\n",
    "\n",
    "print(f\"\\nğŸ“‹ Arquivos de chunks preparados para prÃ³xima etapa: {len(chunk_files_info)}\")\n",
    "print(\"âœ… ValidaÃ§Ã£o concluÃ­da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Salvamento dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ SALVAMENTO DOS RESULTADOS PARA PRÃ“XIMA ETAPA\n",
    "print(\"ğŸ’¾ Salvando resultados da segmentaÃ§Ã£o...\")\n",
    "\n",
    "from pipeline_utils import get_timestamp\n",
    "\n",
    "# Preparar dados de saÃ­da para prÃ³xima etapa\n",
    "stage_output = {\n",
    "    \"stage_info\": {\n",
    "        \"stage_number\": 4,\n",
    "        \"stage_name\": \"chunking\",\n",
    "        \"completed_at\": get_timestamp(),\n",
    "        \"status\": \"success\" if len(successful_chunking) > 0 else \"completed_with_warnings\",\n",
    "        \"chunking_duration_seconds\": chunking_duration\n",
    "    },\n",
    "    \n",
    "    \"chunking_config\": chunking_config,\n",
    "    \n",
    "    \"input_files\": {\n",
    "        \"total_input_files\": len(text_files),\n",
    "        \"available_files\": len(available_text_files),\n",
    "        \"processed_files\": len(chunking_results)\n",
    "    },\n",
    "    \n",
    "    \"chunk_files\": chunk_files_info,\n",
    "    \n",
    "    \"chunking_results\": {\n",
    "        \"successful_documents\": len(successful_chunking),\n",
    "        \"failed_documents\": len(failed_chunking),\n",
    "        \"total_chunks_created\": total_chunks,\n",
    "        \"total_tokens_processed\": total_tokens\n",
    "    },\n",
    "    \n",
    "    \"quality_analysis\": {\n",
    "        \"total_chunks\": quality_stats[\"total_chunks\"],\n",
    "        \"avg_tokens_per_chunk\": quality_stats[\"total_tokens\"] / quality_stats[\"total_chunks\"] if quality_stats[\"total_chunks\"] > 0 else 0,\n",
    "        \"min_chunk_size\": quality_stats[\"min_chunk_size\"] if quality_stats[\"min_chunk_size\"] != float('inf') else 0,\n",
    "        \"max_chunk_size\": quality_stats[\"max_chunk_size\"],\n",
    "        \"chunks_with_overlap\": quality_stats[\"chunks_with_overlap\"],\n",
    "        \"empty_chunks\": quality_stats[\"empty_chunks\"],\n",
    "        \"size_distribution\": dict(quality_stats[\"size_distribution\"])\n",
    "    },\n",
    "    \n",
    "    \"statistics\": {\n",
    "        \"documents_chunked\": len(successful_chunking),\n",
    "        \"total_chunks\": total_chunks,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"avg_chunks_per_document\": total_chunks / len(successful_chunking) if successful_chunking else 0,\n",
    "        \"avg_tokens_per_chunk\": total_tokens / total_chunks if total_chunks > 0 else 0,\n",
    "        \"chunking_time_seconds\": chunking_duration,\n",
    "        \"chunk_files_generated\": len(chunk_files)\n",
    "    },\n",
    "    \n",
    "    \"errors\": [\n",
    "        {\n",
    "            \"source_document\": result[\"source_document\"],\n",
    "            \"error\": result[\"error\"]\n",
    "        }\n",
    "        for result in failed_chunking\n",
    "    ],\n",
    "    \n",
    "    \"next_stage_instructions\": {\n",
    "        \"chunks_directory\": chunking_config[\"output_dir\"],\n",
    "        \"chunk_files_to_embed\": [info[\"chunks_file\"] for info in chunk_files_info],\n",
    "        \"total_chunks_to_process\": total_chunks,\n",
    "        \"recommended_embedding_model\": \"BAAI/bge-m3\",\n",
    "        \"recommended_batch_size\": 32\n",
    "    }\n",
    "}\n",
    "\n",
    "# Salvar usando PipelineState\n",
    "state = PipelineState()\n",
    "state.save_stage_data(4, stage_output)\n",
    "\n",
    "# Marcar etapa como concluÃ­da\n",
    "state.mark_stage_completed(4)\n",
    "\n",
    "print(f\"âœ… Resultados salvos: {state.metadata_dir / 'stage_04_chunking.json'}\")\n",
    "print(f\"âœ… Checkpoint criado: {state.checkpoints_dir / 'stage_04_completed.lock'}\")\n",
    "\n",
    "# Salvar relatÃ³rio detalhado de chunking\n",
    "chunking_report_path = Path(chunking_config[\"output_dir\"]) / \"chunking_report.json\"\n",
    "with open(chunking_report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        \"timestamp\": get_timestamp(),\n",
    "        \"config\": chunking_config,\n",
    "        \"results\": chunking_results,\n",
    "        \"quality_stats\": quality_stats,\n",
    "        \"validation\": validation_result\n",
    "    }, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"ğŸ“ RelatÃ³rio detalhado salvo: {chunking_report_path}\")\n",
    "\n",
    "# Exibir resumo final\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ ETAPA 4 CONCLUÃDA COM SUCESSO!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"ğŸ“Š Resumo da SegmentaÃ§Ã£o:\")\n",
    "print(f\"   ğŸ“„ Documentos segmentados: {len(successful_chunking)}/{len(available_text_files)}\")\n",
    "print(f\"   ğŸ”ª Chunks criados: {total_chunks:,}\")\n",
    "print(f\"   ğŸ¯ Tokens processados: {total_tokens:,}\")\n",
    "print(f\"   ğŸ“ Tamanho mÃ©dio: {total_tokens/total_chunks:.0f} tokens/chunk\" if total_chunks > 0 else \"   ğŸ“ Tamanho mÃ©dio: N/A\")\n",
    "print(f\"   â±ï¸ Tempo total: {chunking_duration:.1f} segundos\")\n",
    "print(f\"   ğŸ“ LocalizaÃ§Ã£o: {chunking_config['output_dir']}\")\n",
    "\n",
    "if failed_chunking:\n",
    "    print(f\"   âš ï¸ Avisos: {len(failed_chunking)} documentos falharam\")\nelse:\n",
    "    print(f\"   âœ… Status: Todos os documentos segmentados com sucesso\")\n",
    "\n",
    "print(f\"\\nğŸš€ Pronto para prÃ³xima etapa: 05_GERACAO_EMBEDDINGS.ipynb\")\n",
    "print(f\"ğŸ“‹ {total_chunks:,} chunks prontos para geraÃ§Ã£o de embeddings\")\n",
    "\n",
    "# Exibir progresso do pipeline\n",
    "print(\"\\nğŸ“ˆ Progresso do Pipeline:\")\n",
    "show_pipeline_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… **Etapa 4 ConcluÃ­da!**\n",
    "\n",
    "### ğŸ¯ **O que foi feito:**\n",
    "- âœ… Texto segmentado em chunks de tamanho otimizado\n",
    "- âœ… Overlap preservado para manter contexto\n",
    "- âœ… Limites de frases respeitados (sem cortes abruptos)\n",
    "- âœ… Qualidade validada e estatÃ­sticas coletadas\n",
    "- âœ… Chunks organizados e catalogados\n",
    "\n",
    "### ğŸš€ **PrÃ³xima etapa:**\n",
    "**`05_GERACAO_EMBEDDINGS.ipynb`** - GeraÃ§Ã£o de vetores com BAAI/bge-m3\n",
    "\n",
    "### ğŸ“Š **Arquivos gerados:**\n",
    "- `pipeline_data/chunks/` - Chunks organizados por documento\n",
    "- `pipeline_data/chunks/*_chunks.json` - Chunks com metadados completos\n",
    "- `pipeline_data/metadata/stage_04_chunking.json` - EstatÃ­sticas e configuraÃ§Ã£o\n",
    "- `pipeline_data/chunks/chunking_report.json` - RelatÃ³rio detalhado de qualidade\n",
    "- `pipeline_data/checkpoints/stage_04_completed.lock` - Checkpoint de conclusÃ£o\n",
    "\n",
    "### ğŸ“‹ **Dados para prÃ³xima etapa:**\n",
    "- **Chunks otimizados:** Prontos para geraÃ§Ã£o de embeddings\n",
    "- **Metadados preservados:** Origem, posiÃ§Ã£o, overlap\n",
    "- **DistribuiÃ§Ã£o balanceada:** Tamanhos adequados para modelos de embedding\n",
    "\n",
    "### ğŸ”§ **Para executar prÃ³xima etapa:**\n",
    "1. Abra o notebook `05_GERACAO_EMBEDDINGS.ipynb`\n",
    "2. Execute as cÃ©lulas em sequÃªncia\n",
    "3. Ou use o `00_PIPELINE_MASTER.ipynb` para execuÃ§Ã£o automÃ¡tica\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ”ª Texto segmentado inteligentemente! Pronto para vetorizaÃ§Ã£o.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}