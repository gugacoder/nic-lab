{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔪 SEGMENTAÇÃO EM CHUNKS\n",
    "\n",
    "Divide o texto em segmentos para processamento de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "# Configuração\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"500\"))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", \"100\"))\n",
    "\n",
    "# Diretórios\n",
    "processed_dir = Path(\"pipeline_data/processed\")\n",
    "chunks_dir = Path(\"pipeline_data/chunks\")\n",
    "chunks_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Limpar diretório chunks\n",
    "for f in chunks_dir.glob(\"*\"):\n",
    "    if f.is_file():\n",
    "        f.unlink()\n",
    "\n",
    "print(f\"Chunk size: {CHUNK_SIZE} tokens\")\n",
    "print(f\"Overlap: {CHUNK_OVERLAP} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Encontrar arquivos de texto\n",
    "text_files = list(processed_dir.glob(\"*_text.txt\"))\n",
    "print(f\"Arquivos de texto encontrados: {len(text_files)}\")\n",
    "\n",
    "for text_file in text_files:\n",
    "    print(f\"  {text_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processar cada arquivo\n",
    "all_chunks = []\n",
    "chunk_id = 0\n",
    "\n",
    "for text_file in text_files:\n",
    "    try:\n",
    "        print(f\"Processando: {text_file.name}\")\n",
    "        \n",
    "        # Ler texto\n",
    "        with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_content = f.read()\n",
    "        \n",
    "        if not text_content.strip():\n",
    "            print(f\"  ⚠️ Arquivo vazio: {text_file.name}\")\n",
    "            continue\n",
    "        \n",
    "        # Dividir em chunks\n",
    "        chunks = text_splitter.split_text(text_content)\n",
    "        \n",
    "        # Processar cada chunk\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            chunk_data = {\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"source_document\": text_file.stem.replace(\"_text\", \"\"),\n",
    "                \"chunk_index\": i,\n",
    "                \"text\": chunk_text.strip(),\n",
    "                \"char_count\": len(chunk_text)\n",
    "            }\n",
    "            \n",
    "            all_chunks.append(chunk_data)\n",
    "            chunk_id += 1\n",
    "        \n",
    "        print(f\"  ✅ {len(chunks)} chunks criados\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Erro: {str(e)}\")\n",
    "\n",
    "print(f\"\\n📊 Total de chunks: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar chunks\n",
    "chunks_file = chunks_dir / \"chunks.jsonl\"\n",
    "\n",
    "with open(chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in all_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Chunks salvos: {chunks_file}\")\n",
    "\n",
    "# Estatísticas\n",
    "if all_chunks:\n",
    "    avg_chars = sum(chunk[\"char_count\"] for chunk in all_chunks) / len(all_chunks)\n",
    "    max_chars = max(chunk[\"char_count\"] for chunk in all_chunks)\n",
    "    min_chars = min(chunk[\"char_count\"] for chunk in all_chunks)\n",
    "    \n",
    "    print(f\"\\n📊 Estatísticas:\")\n",
    "    print(f\"  Total chunks: {len(all_chunks)}\")\n",
    "    print(f\"  Tamanho médio: {avg_chars:.0f} caracteres\")\n",
    "    print(f\"  Tamanho mínimo: {min_chars} caracteres\")\n",
    "    print(f\"  Tamanho máximo: {max_chars} caracteres\")\n",
    "    \n",
    "    # Mostrar alguns chunks de exemplo\n",
    "    print(f\"\\n📝 Exemplos de chunks:\")\n",
    "    for i, chunk in enumerate(all_chunks[:3]):\n",
    "        preview = chunk[\"text\"][:100] + \"...\" if len(chunk[\"text\"]) > 100 else chunk[\"text\"]\n",
    "        print(f\"  {i+1}. {preview} ({chunk['char_count']} chars)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}