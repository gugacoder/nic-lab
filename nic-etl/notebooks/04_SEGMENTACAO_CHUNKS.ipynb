{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔪 ETAPA 4: SEGMENTAÇÃO INTELIGENTE EM CHUNKS\n",
    "\n",
    "## 🎯 **O que esta etapa faz**\n",
    "Divide o texto extraído em pedaços menores (chunks) de tamanho adequado para processamento de embeddings, preservando o contexto e a semântica.\n",
    "\n",
    "## 🤔 **Por que esta etapa é necessária**\n",
    "Textos longos são difíceis de processar eficientemente. Precisamos:\n",
    "- ✂️ **Dividir em pedaços menores** para modelos de embedding\n",
    "- 🔗 **Manter contexto** com sobreposição entre chunks\n",
    "- 📏 **Respeitar limites** de tokens dos modelos (500 tokens)\n",
    "- 🧠 **Preservar semântica** não cortando no meio de frases\n",
    "\n",
    "## ⚙️ **Como funciona**\n",
    "1. **Carrega texto processado** da etapa anterior\n",
    "2. **Tokeniza inteligentemente** respeitando palavras e frases\n",
    "3. **Cria chunks** de ~500 tokens com overlap de ~100 tokens\n",
    "4. **Ajusta limites** para não cortar frases no meio\n",
    "5. **Preserva metadados** de origem e posição\n",
    "6. **Valida qualidade** dos chunks gerados\n",
    "\n",
    "## 📊 **Parâmetros que você pode ajustar**\n",
    "- `chunk_size`: Tamanho alvo em tokens (padrão: 500)\n",
    "- `overlap_size`: Sobreposição em tokens (padrão: 100)\n",
    "- `respect_sentences`: Não cortar frases (padrão: True)\n",
    "- `min_chunk_size`: Tamanho mínimo válido (padrão: 50)\n",
    "- `max_chunk_size`: Tamanho máximo permitido (padrão: 750)\n",
    "\n",
    "## 👁️ **O que esperar de saída**\n",
    "- 📁 **Pasta `chunks/`** com chunks organizados\n",
    "- 📄 **Arquivos `*_chunks.json`** com chunks e metadados\n",
    "- 📊 **Estatísticas** de segmentação por documento\n",
    "- 🔗 **Mapeamento** de chunks para documentos originais\n",
    "\n",
    "## ⚠️ **Pontos importantes**\n",
    "- **Requer Etapa 3** (Processamento Docling) concluída primeiro\n",
    "- **Tokenização** é aproximada (baseada em espaços)\n",
    "- **Overlap** garante continuidade de contexto\n",
    "- **Modo independente** disponível para testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛡️ Verificação de Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 🛡️ VERIFICAÇÃO AUTOMÁTICA DE DEPENDÊNCIAS\nimport sys\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Adicionar biblioteca ao path\nsys.path.insert(0, str(Path().parent / \"src\"))\n\nfrom pipeline_utils import (\n    PipelineState, \n    check_prerequisites, \n    show_pipeline_progress\n)\n\nprint(\"🔪 ETAPA 4: SEGMENTAÇÃO INTELIGENTE EM CHUNKS\")\nprint(\"=\" * 60)\nprint(f\"🕒 Iniciado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# Verificar dependências\nCURRENT_STAGE = 4\nprerequisites_ok = check_prerequisites(CURRENT_STAGE)\n\nif not prerequisites_ok:\n    print(\"\\n❌ ERRO: Dependências não atendidas!\")\n    print(\"📋 Execute primeiro as etapas anteriores:\")\n    print(\"   1. 01_FUNDACAO_PREPARACAO.ipynb\")\n    print(\"   2. 02_COLETA_GITLAB.ipynb\")\n    print(\"   3. 03_PROCESSAMENTO_DOCLING.ipynb\")\n    \n    show_pipeline_progress()\n    raise RuntimeError(\"Dependências não atendidas - execute etapas anteriores primeiro\")\n\nprint(\"\\n✅ Dependências verificadas - pode prosseguir\")\nprint(\"📈 Progresso atual do pipeline:\")\nshow_pipeline_progress()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Configuração de Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 CONFIGURAÇÃO DOS PARÂMETROS DE CHUNKING\n",
    "print(\"📋 Configurando parâmetros de segmentação...\")\n",
    "\n",
    "# 🎯 PARÂMETROS AJUSTÁVEIS\n",
    "chunking_config = {\n",
    "    # Tamanhos\n",
    "    \"chunk_size\": 500,          # Tokens por chunk\n",
    "    \"overlap_size\": 100,        # Overlap entre chunks\n",
    "    \"min_chunk_size\": 50,       # Tamanho mínimo válido\n",
    "    \"max_chunk_size\": 750,      # Tamanho máximo permitido\n",
    "    \n",
    "    # Comportamento\n",
    "    \"respect_sentences\": True,   # Não cortar frases\n",
    "    \"respect_paragraphs\": False, # Pode cortar parágrafos\n",
    "    \"preserve_structure\": True,  # Manter info de estrutura\n",
    "    \n",
    "    # Processamento\n",
    "    \"batch_size\": 8,             # Documentos processados simultaneamente\n",
    "    \"skip_empty_chunks\": True,   # Descartar chunks vazios\n",
    "    \n",
    "    # Saída\n",
    "    \"output_dir\": \"../pipeline_data/chunks\",\n",
    "    \"overwrite_existing\": True\n",
    "}\n",
    "\n",
    "print(\"\\n📊 Parâmetros de chunking configurados:\")\n",
    "print(f\"   📏 Tamanho do chunk: {chunking_config['chunk_size']} tokens\")\n",
    "print(f\"   🔗 Overlap: {chunking_config['overlap_size']} tokens\")\n",
    "print(f\"   📐 Faixa válida: {chunking_config['min_chunk_size']}-{chunking_config['max_chunk_size']} tokens\")\n",
    "print(f\"   📝 Respeitar frases: {chunking_config['respect_sentences']}\")\n",
    "print(f\"   🏗️ Preservar estrutura: {chunking_config['preserve_structure']}\")\n",
    "print(f\"   📦 Lote: {chunking_config['batch_size']} documentos simultâneos\")\n",
    "print(f\"   📁 Saída: {chunking_config['output_dir']}\")\n",
    "\n",
    "# Preparar diretório de saída\n",
    "from pipeline_utils import ensure_directory\n",
    "ensure_directory(chunking_config[\"output_dir\"])\n",
    "\n",
    "print(\"\\n✅ Configuração de chunking pronta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📄 Carregamento dos Documentos Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📄 CARREGAMENTO DOS DOCUMENTOS PROCESSADOS\n",
    "print(\"📄 Carregando documentos da etapa anterior...\")\n",
    "\n",
    "# Carregar dados da etapa 3 (Docling)\n",
    "state = PipelineState()\n",
    "try:\n",
    "    docling_data = state.load_stage_data(3)\n",
    "    print(\"✅ Dados carregados da Etapa 3 (Docling)\")\n",
    "    \n",
    "    processed_docs = docling_data[\"processed_documents\"]\n",
    "    text_files = docling_data[\"next_stage_instructions\"][\"text_files_to_chunk\"]\n",
    "    \n",
    "    print(f\"   📄 Documentos processados: {len(processed_docs)}\")\n",
    "    print(f\"   📝 Arquivos de texto: {len(text_files)}\")\n",
    "    print(f\"   📁 Diretório origem: {docling_data['next_stage_instructions']['processed_directory']}\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"❌ Erro ao carregar dados: {e}\")\n",
    "    raise\n",
    "\n",
    "# Verificar se arquivos de texto existem\n",
    "available_text_files = []\n",
    "for text_file_path in text_files:\n",
    "    file_path = Path(text_file_path)\n",
    "    if file_path.exists():\n",
    "        file_size = file_path.stat().st_size\n",
    "        if file_size > 0:\n",
    "            available_text_files.append(text_file_path)\n",
    "            print(f\"   ✅ {file_path.name} ({file_size:,} bytes)\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ {file_path.name} (arquivo vazio)\")\n",
    "    else:\n",
    "        print(f\"   ❌ {file_path.name} (não encontrado)\")\n",
    "\n",
    "if not available_text_files:\n",
    "    print(\"\\n❌ Nenhum arquivo de texto disponível para chunking\")\n",
    "    raise ValueError(\"Nenhum arquivo de texto para processar\")\n",
    "\n",
    "print(f\"\\n📊 Arquivos prontos para chunking: {len(available_text_files)}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✂️ Segmentação em Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✂️ EXECUÇÃO DA SEGMENTAÇÃO EM CHUNKS\n",
    "print(\"✂️ Iniciando segmentação em chunks...\")\n",
    "\n",
    "from chunking_utils import batch_process_documents_chunking\n",
    "from pipeline_utils import format_file_size\n",
    "import json\n",
    "\n",
    "print(f\"\\n🚀 Processando {len(available_text_files)} documentos...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Executar chunking em lote\n",
    "start_time = datetime.now()\n",
    "\n",
    "chunking_results = batch_process_documents_chunking(\n",
    "    document_list=available_text_files,\n",
    "    output_dir=chunking_config[\"output_dir\"],\n",
    "    chunk_size=chunking_config[\"chunk_size\"],\n",
    "    overlap_size=chunking_config[\"overlap_size\"]\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "chunking_duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📊 RESULTADO DA SEGMENTAÇÃO:\")\n",
    "\n",
    "# Contar sucessos e falhas\n",
    "successful_chunking = [r for r in chunking_results if r[\"status\"] == \"success\"]\n",
    "failed_chunking = [r for r in chunking_results if r[\"status\"] == \"failed\"]\n",
    "\n",
    "total_chunks = sum(r[\"chunk_count\"] for r in successful_chunking)\n",
    "total_tokens = sum(r[\"total_tokens\"] for r in successful_chunking)\n",
    "\n",
    "print(f\"   📄 Documentos processados: {len(chunking_results)}\")\n",
    "print(f\"   ✅ Sucessos: {len(successful_chunking)}\")\n",
    "print(f\"   ❌ Falhas: {len(failed_chunking)}\")\n",
    "print(f\"   🔪 Total de chunks: {total_chunks:,}\")\n",
    "print(f\"   🎯 Total de tokens: {total_tokens:,}\")\n",
    "print(f\"   ⏱️ Tempo total: {chunking_duration:.1f} segundos\")\n",
    "\n",
    "if successful_chunking:\n",
    "    avg_chunks_per_doc = total_chunks / len(successful_chunking)\n",
    "    avg_tokens_per_chunk = total_tokens / total_chunks if total_chunks > 0 else 0\n",
    "    print(f\"   📈 Média: {avg_chunks_per_doc:.1f} chunks por documento\")\n",
    "    print(f\"   📏 Média: {avg_tokens_per_chunk:.0f} tokens por chunk\")\n",
    "\n",
    "# Mostrar resultados de sucesso\n",
    "if successful_chunking:\n",
    "    print(\"\\n📁 Documentos segmentados com sucesso:\")\n",
    "    for result in successful_chunking[:10]:  # Mostrar apenas os primeiros 10\n",
    "        filename = Path(result[\"source_document\"]).name\n",
    "        chunks = result[\"chunk_count\"]\n",
    "        tokens = result[\"total_tokens\"]\n",
    "        print(f\"   ✅ {filename}: {chunks} chunks, {tokens:,} tokens\")\n",
    "    \n",
    "    if len(successful_chunking) > 10:\n",
    "        remaining = len(successful_chunking) - 10\n",
    "        print(f\"   ... e mais {remaining} documentos\")\n",
    "\n",
    "# Mostrar falhas se houver\n",
    "if failed_chunking:\n",
    "    print(f\"\\n⚠️ Documentos com problemas ({len(failed_chunking)}):\")\n",
    "    for result in failed_chunking[:5]:  # Mostrar apenas os primeiros 5\n",
    "        filename = Path(result[\"source_document\"]).name\n",
    "        error = result[\"error\"]\n",
    "        print(f\"   ❌ {filename}: {error}\")\n",
    "    \n",
    "    if len(failed_chunking) > 5:\n",
    "        remaining = len(failed_chunking) - 5\n",
    "        print(f\"   ... e mais {remaining} erros\")\n",
    "\n",
    "print(\"\\n✅ Segmentação concluída!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Validação e Qualidade dos Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 VALIDAÇÃO E ANÁLISE DE QUALIDADE DOS CHUNKS\n",
    "print(\"📊 Validando qualidade dos chunks...\")\n",
    "\n",
    "from chunking_utils import validate_chunks\n",
    "from collections import defaultdict\n",
    "\n",
    "# Validar chunks gerados\n",
    "validation_result = validate_chunks(chunking_config[\"output_dir\"])\n",
    "\n",
    "if validation_result[\"valid\"]:\n",
    "    print(\"\\n✅ Validação bem-sucedida\")\n",
    "    print(f\"   📄 Arquivos de chunks: {validation_result['chunk_files_count']}\")\n",
    "    print(f\"   🔪 Total de chunks: {validation_result['total_chunks']:,}\")\n",
    "    print(f\"   🎯 Total de tokens: {validation_result['total_tokens']:,}\")\n",
    "    print(f\"   📏 Média tokens/chunk: {validation_result['avg_tokens_per_chunk']:.0f}\")\nelse:\n",
    "    print(f\"\\n❌ Problema na validação: {validation_result.get('error', 'Erro desconhecido')}\")\n",
    "\n",
    "# Análise detalhada de qualidade\n",
    "chunks_dir = Path(chunking_config[\"output_dir\"])\n",
    "chunk_files = list(chunks_dir.glob(\"*_chunks.json\"))\n",
    "\n",
    "quality_stats = {\n",
    "    \"size_distribution\": defaultdict(int),\n",
    "    \"total_chunks\": 0,\n",
    "    \"total_tokens\": 0,\n",
    "    \"min_chunk_size\": float('inf'),\n",
    "    \"max_chunk_size\": 0,\n",
    "    \"chunks_with_overlap\": 0,\n",
    "    \"empty_chunks\": 0\n",
    "}\n",
    "\n",
    "print(f\"\\n📈 Análise detalhada de {len(chunk_files)} arquivos de chunks:\")\n",
    "\n",
    "for chunk_file in chunk_files:\n",
    "    try:\n",
    "        with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "            chunk_data = json.load(f)\n",
    "        \n",
    "        chunks = chunk_data.get(\"chunks\", [])\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            token_count = chunk.get(\"token_count\", 0)\n",
    "            has_overlap = chunk.get(\"has_overlap\", False)\n",
    "            text = chunk.get(\"text\", \"\")\n",
    "            \n",
    "            quality_stats[\"total_chunks\"] += 1\n",
    "            quality_stats[\"total_tokens\"] += token_count\n",
    "            \n",
    "            # Distribuição por tamanho\n",
    "            size_bucket = (token_count // 100) * 100  # Buckets de 100 tokens\n",
    "            quality_stats[\"size_distribution\"][size_bucket] += 1\n",
    "            \n",
    "            # Min/Max\n",
    "            quality_stats[\"min_chunk_size\"] = min(quality_stats[\"min_chunk_size\"], token_count)\n",
    "            quality_stats[\"max_chunk_size\"] = max(quality_stats[\"max_chunk_size\"], token_count)\n",
    "            \n",
    "            # Overlap\n",
    "            if has_overlap:\n",
    "                quality_stats[\"chunks_with_overlap\"] += 1\n",
    "            \n",
    "            # Chunks vazios\n",
    "            if not text.strip():\n",
    "                quality_stats[\"empty_chunks\"] += 1\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Erro ao analisar {chunk_file.name}: {e}\")\n",
    "\n",
    "# Estatísticas de qualidade\n",
    "if quality_stats[\"total_chunks\"] > 0:\n",
    "    avg_size = quality_stats[\"total_tokens\"] / quality_stats[\"total_chunks\"]\n",
    "    overlap_percentage = (quality_stats[\"chunks_with_overlap\"] / quality_stats[\"total_chunks\"]) * 100\n",
    "    \n",
    "    print(f\"\\n📏 Estatísticas de tamanho:\")\n",
    "    print(f\"   📊 Chunks totais: {quality_stats['total_chunks']:,}\")\n",
    "    print(f\"   📐 Tamanho médio: {avg_size:.0f} tokens\")\n",
    "    print(f\"   📉 Menor chunk: {quality_stats['min_chunk_size']} tokens\")\n",
    "    print(f\"   📈 Maior chunk: {quality_stats['max_chunk_size']} tokens\")\n",
    "    \n",
    "    print(f\"\\n🔗 Análise de overlap:\")\n",
    "    print(f\"   Chunks com overlap: {quality_stats['chunks_with_overlap']:,} ({overlap_percentage:.1f}%)\")\n",
    "    \n",
    "    if quality_stats[\"empty_chunks\"] > 0:\n",
    "        print(f\"\\n⚠️ Problemas encontrados:\")\n",
    "        print(f\"   Chunks vazios: {quality_stats['empty_chunks']}\")\n",
    "    \n",
    "    print(f\"\\n📊 Distribuição por tamanho:\")\n",
    "    for size_bucket in sorted(quality_stats[\"size_distribution\"].keys()):\n",
    "        count = quality_stats[\"size_distribution\"][size_bucket]\n",
    "        percentage = (count / quality_stats[\"total_chunks\"]) * 100\n",
    "        print(f\"   {size_bucket}-{size_bucket+99} tokens: {count:,} chunks ({percentage:.1f}%)\")\n",
    "\n",
    "# Preparar lista de arquivos de chunks para próxima etapa\n",
    "chunk_files_info = []\n",
    "for result in successful_chunking:\n",
    "    chunk_files_info.append({\n",
    "        \"source_document\": result[\"source_document\"],\n",
    "        \"chunks_file\": result[\"chunks_file\"],\n",
    "        \"chunk_count\": result[\"chunk_count\"],\n",
    "        \"total_tokens\": result[\"total_tokens\"]\n",
    "    })\n",
    "\n",
    "print(f\"\\n📋 Arquivos de chunks preparados para próxima etapa: {len(chunk_files_info)}\")\n",
    "print(\"✅ Validação concluída!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Salvamento dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 SALVAMENTO DOS RESULTADOS PARA PRÓXIMA ETAPA\n",
    "print(\"💾 Salvando resultados da segmentação...\")\n",
    "\n",
    "from pipeline_utils import get_timestamp\n",
    "\n",
    "# Preparar dados de saída para próxima etapa\n",
    "stage_output = {\n",
    "    \"stage_info\": {\n",
    "        \"stage_number\": 4,\n",
    "        \"stage_name\": \"chunking\",\n",
    "        \"completed_at\": get_timestamp(),\n",
    "        \"status\": \"success\" if len(successful_chunking) > 0 else \"completed_with_warnings\",\n",
    "        \"chunking_duration_seconds\": chunking_duration\n",
    "    },\n",
    "    \n",
    "    \"chunking_config\": chunking_config,\n",
    "    \n",
    "    \"input_files\": {\n",
    "        \"total_input_files\": len(text_files),\n",
    "        \"available_files\": len(available_text_files),\n",
    "        \"processed_files\": len(chunking_results)\n",
    "    },\n",
    "    \n",
    "    \"chunk_files\": chunk_files_info,\n",
    "    \n",
    "    \"chunking_results\": {\n",
    "        \"successful_documents\": len(successful_chunking),\n",
    "        \"failed_documents\": len(failed_chunking),\n",
    "        \"total_chunks_created\": total_chunks,\n",
    "        \"total_tokens_processed\": total_tokens\n",
    "    },\n",
    "    \n",
    "    \"quality_analysis\": {\n",
    "        \"total_chunks\": quality_stats[\"total_chunks\"],\n",
    "        \"avg_tokens_per_chunk\": quality_stats[\"total_tokens\"] / quality_stats[\"total_chunks\"] if quality_stats[\"total_chunks\"] > 0 else 0,\n",
    "        \"min_chunk_size\": quality_stats[\"min_chunk_size\"] if quality_stats[\"min_chunk_size\"] != float('inf') else 0,\n",
    "        \"max_chunk_size\": quality_stats[\"max_chunk_size\"],\n",
    "        \"chunks_with_overlap\": quality_stats[\"chunks_with_overlap\"],\n",
    "        \"empty_chunks\": quality_stats[\"empty_chunks\"],\n",
    "        \"size_distribution\": dict(quality_stats[\"size_distribution\"])\n",
    "    },\n",
    "    \n",
    "    \"statistics\": {\n",
    "        \"documents_chunked\": len(successful_chunking),\n",
    "        \"total_chunks\": total_chunks,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"avg_chunks_per_document\": total_chunks / len(successful_chunking) if successful_chunking else 0,\n",
    "        \"avg_tokens_per_chunk\": total_tokens / total_chunks if total_chunks > 0 else 0,\n",
    "        \"chunking_time_seconds\": chunking_duration,\n",
    "        \"chunk_files_generated\": len(chunk_files)\n",
    "    },\n",
    "    \n",
    "    \"errors\": [\n",
    "        {\n",
    "            \"source_document\": result[\"source_document\"],\n",
    "            \"error\": result[\"error\"]\n",
    "        }\n",
    "        for result in failed_chunking\n",
    "    ],\n",
    "    \n",
    "    \"next_stage_instructions\": {\n",
    "        \"chunks_directory\": chunking_config[\"output_dir\"],\n",
    "        \"chunk_files_to_embed\": [info[\"chunks_file\"] for info in chunk_files_info],\n",
    "        \"total_chunks_to_process\": total_chunks,\n",
    "        \"recommended_embedding_model\": \"BAAI/bge-m3\",\n",
    "        \"recommended_batch_size\": 32\n",
    "    }\n",
    "}\n",
    "\n",
    "# Salvar usando PipelineState\n",
    "state = PipelineState()\n",
    "state.save_stage_data(4, stage_output)\n",
    "\n",
    "# Marcar etapa como concluída\n",
    "state.mark_stage_completed(4)\n",
    "\n",
    "print(f\"✅ Resultados salvos: {state.metadata_dir / 'stage_04_chunking.json'}\")\n",
    "print(f\"✅ Checkpoint criado: {state.checkpoints_dir / 'stage_04_completed.lock'}\")\n",
    "\n",
    "# Salvar relatório detalhado de chunking\n",
    "chunking_report_path = Path(chunking_config[\"output_dir\"]) / \"chunking_report.json\"\n",
    "with open(chunking_report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        \"timestamp\": get_timestamp(),\n",
    "        \"config\": chunking_config,\n",
    "        \"results\": chunking_results,\n",
    "        \"quality_stats\": quality_stats,\n",
    "        \"validation\": validation_result\n",
    "    }, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"📝 Relatório detalhado salvo: {chunking_report_path}\")\n",
    "\n",
    "# Exibir resumo final\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 ETAPA 4 CONCLUÍDA COM SUCESSO!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"📊 Resumo da Segmentação:\")\n",
    "print(f\"   📄 Documentos segmentados: {len(successful_chunking)}/{len(available_text_files)}\")\n",
    "print(f\"   🔪 Chunks criados: {total_chunks:,}\")\n",
    "print(f\"   🎯 Tokens processados: {total_tokens:,}\")\n",
    "print(f\"   📏 Tamanho médio: {total_tokens/total_chunks:.0f} tokens/chunk\" if total_chunks > 0 else \"   📏 Tamanho médio: N/A\")\n",
    "print(f\"   ⏱️ Tempo total: {chunking_duration:.1f} segundos\")\n",
    "print(f\"   📍 Localização: {chunking_config['output_dir']}\")\n",
    "\n",
    "if failed_chunking:\n",
    "    print(f\"   ⚠️ Avisos: {len(failed_chunking)} documentos falharam\")\nelse:\n",
    "    print(f\"   ✅ Status: Todos os documentos segmentados com sucesso\")\n",
    "\n",
    "print(f\"\\n🚀 Pronto para próxima etapa: 05_GERACAO_EMBEDDINGS.ipynb\")\n",
    "print(f\"📋 {total_chunks:,} chunks prontos para geração de embeddings\")\n",
    "\n",
    "# Exibir progresso do pipeline\n",
    "print(\"\\n📈 Progresso do Pipeline:\")\n",
    "show_pipeline_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ **Etapa 4 Concluída!**\n",
    "\n",
    "### 🎯 **O que foi feito:**\n",
    "- ✅ Texto segmentado em chunks de tamanho otimizado\n",
    "- ✅ Overlap preservado para manter contexto\n",
    "- ✅ Limites de frases respeitados (sem cortes abruptos)\n",
    "- ✅ Qualidade validada e estatísticas coletadas\n",
    "- ✅ Chunks organizados e catalogados\n",
    "\n",
    "### 🚀 **Próxima etapa:**\n",
    "**`05_GERACAO_EMBEDDINGS.ipynb`** - Geração de vetores com BAAI/bge-m3\n",
    "\n",
    "### 📊 **Arquivos gerados:**\n",
    "- `pipeline_data/chunks/` - Chunks organizados por documento\n",
    "- `pipeline_data/chunks/*_chunks.json` - Chunks com metadados completos\n",
    "- `pipeline_data/metadata/stage_04_chunking.json` - Estatísticas e configuração\n",
    "- `pipeline_data/chunks/chunking_report.json` - Relatório detalhado de qualidade\n",
    "- `pipeline_data/checkpoints/stage_04_completed.lock` - Checkpoint de conclusão\n",
    "\n",
    "### 📋 **Dados para próxima etapa:**\n",
    "- **Chunks otimizados:** Prontos para geração de embeddings\n",
    "- **Metadados preservados:** Origem, posição, overlap\n",
    "- **Distribuição balanceada:** Tamanhos adequados para modelos de embedding\n",
    "\n",
    "### 🔧 **Para executar próxima etapa:**\n",
    "1. Abra o notebook `05_GERACAO_EMBEDDINGS.ipynb`\n",
    "2. Execute as células em sequência\n",
    "3. Ou use o `00_PIPELINE_MASTER.ipynb` para execução automática\n",
    "\n",
    "---\n",
    "\n",
    "**🔪 Texto segmentado inteligentemente! Pronto para vetorização.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}