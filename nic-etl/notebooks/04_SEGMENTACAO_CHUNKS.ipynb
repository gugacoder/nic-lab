{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî™ SEGMENTA√á√ÉO EM CHUNKS\n",
    "\n",
    "Divide o texto em segmentos para processamento de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 500 tokens\n",
      "Overlap: 100 tokens\n",
      "Documentos encontrados: 31\n",
      "  30-Aprovados/Mapas/Vis√£o Geral do Self Checkout.md\n",
      "  30-Aprovados/Mapas/Vis√£o Geral do NIC.md\n",
      "  30-Aprovados/Mapas/Processa Sistemas.md\n",
      "  30-Aprovados/T√≥picos/Cancelamento de cupom.md\n",
      "  30-Aprovados/T√≥picos/Solicita√ß√£o de ajuda.md\n",
      "  30-Aprovados/T√≥picos/Hist√≥rico de atualiza√ß√µes Self Checkout.md\n",
      "  30-Aprovados/T√≥picos/Funcionalidade do bloqueio.md\n",
      "  30-Aprovados/T√≥picos/Aplica√ß√£o de desconto por item.md\n",
      "  30-Aprovados/T√≥picos/Prop√≥sito do NIC.md\n",
      "  30-Aprovados/T√≥picos/Reimpress√£o do √∫ltimo cupom.md\n",
      "  ... e mais 21 documentos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "# Configura√ß√£o\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"500\"))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", \"100\"))\n",
    "\n",
    "# Diret√≥rios\n",
    "processed_dir = Path(\"pipeline_data/processed\")\n",
    "chunks_dir = Path(\"pipeline_data/chunks\")\n",
    "chunks_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Limpar diret√≥rio chunks recursivamente\n",
    "for f in chunks_dir.rglob(\"*\"):\n",
    "    if f.is_file():\n",
    "        f.unlink()\n",
    "\n",
    "print(f\"Chunk size: {CHUNK_SIZE} tokens\")\n",
    "print(f\"Overlap: {CHUNK_OVERLAP} tokens\")\n",
    "\n",
    "# Listar documentos processados recursivamente com caminhos relativos\n",
    "documents = [str(doc.relative_to(processed_dir)) for doc in processed_dir.rglob(\"*.md\")]\n",
    "\n",
    "print(f\"Documentos encontrados: {len(documents)}\")\n",
    "\n",
    "for i, doc in enumerate(documents[:10]):\n",
    "    print(f\"  {doc}\")\n",
    "\n",
    "if len(documents) > 10:\n",
    "    print(f\"  ... e mais {len(documents) - 10} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ 51 chunks criados para: 30-Aprovados/Mapas/Vis√£o Geral do Self Checkout.md\n",
      "  ‚úÖ 15 chunks criados para: 30-Aprovados/Mapas/Vis√£o Geral do NIC.md\n",
      "  ‚úÖ 13 chunks criados para: 30-Aprovados/Mapas/Processa Sistemas.md\n",
      "  ‚úÖ 7 chunks criados para: 30-Aprovados/T√≥picos/Cancelamento de cupom.md\n",
      "  ‚úÖ 6 chunks criados para: 30-Aprovados/T√≥picos/Solicita√ß√£o de ajuda.md\n",
      "  ‚úÖ 5 chunks criados para: 30-Aprovados/T√≥picos/Hist√≥rico de atualiza√ß√µes Self Checkout.md\n",
      "  ‚úÖ 11 chunks criados para: 30-Aprovados/T√≥picos/Funcionalidade do bloqueio.md\n",
      "  ‚úÖ 7 chunks criados para: 30-Aprovados/T√≥picos/Aplica√ß√£o de desconto por item.md\n",
      "  ‚úÖ 12 chunks criados para: 30-Aprovados/T√≥picos/Prop√≥sito do NIC.md\n",
      "  ‚úÖ 4 chunks criados para: 30-Aprovados/T√≥picos/Reimpress√£o do √∫ltimo cupom.md\n",
      "  ‚úÖ 8 chunks criados para: 30-Aprovados/T√≥picos/Fun√ß√£o do Chat NIC.md\n",
      "  ‚úÖ 9 chunks criados para: 30-Aprovados/T√≥picos/Acesso ao menu do fiscal.md\n",
      "  ‚úÖ 10 chunks criados para: 30-Aprovados/T√≥picos/Cronograma e marcos do projeto.md\n",
      "  ‚úÖ 7 chunks criados para: 30-Aprovados/T√≥picos/Efetuar pagamento.md\n",
      "  ‚úÖ 5 chunks criados para: 30-Aprovados/T√≥picos/Cancelamento de item.md\n",
      "  ‚úÖ 7 chunks criados para: 30-Aprovados/T√≥picos/Opera√ß√£o cont√≠nua e evolu√ß√£o do sistema.md\n",
      "  ‚úÖ 12 chunks criados para: 30-Aprovados/T√≥picos/Finalidade e vis√£o do NIC.md\n",
      "  ‚úÖ 2 chunks criados para: 30-Aprovados/T√≥picos/Iniciar a compra.md\n",
      "  ‚úÖ 8 chunks criados para: 30-Aprovados/T√≥picos/Processo de documenta√ß√£o e valida√ß√£o.md\n",
      "  ‚úÖ 9 chunks criados para: 30-Aprovados/T√≥picos/Infraestrutura t√©cnica e operacional.md\n",
      "  ‚úÖ 7 chunks criados para: 30-Aprovados/T√≥picos/Apresenta√ß√£o do sistema Self Checkout.md\n",
      "  ‚úÖ 8 chunks criados para: 30-Aprovados/T√≥picos/Retornar para registro de venda.md\n",
      "  ‚úÖ 19 chunks criados para: 30-Aprovados/T√≥picos/Componentes principais do sistema.md\n",
      "  ‚úÖ 10 chunks criados para: 30-Aprovados/T√≥picos/Registro de produtos.md\n",
      "  ‚úÖ 17 chunks criados para: 30-Aprovados/T√≥picos/Pr√©-requisitos t√©cnicos.md\n",
      "  ‚úÖ 8 chunks criados para: 30-Aprovados/T√≥picos/Estrutura e fluxo da base de conhecimento.md\n",
      "  ‚úÖ 5 chunks criados para: 30-Aprovados/T√≥picos/Finaliza√ß√£o do movimento di√°rio.md\n",
      "  ‚úÖ 6 chunks criados para: 30-Aprovados/T√≥picos/Identifica√ß√£o do cliente.md\n",
      "  ‚úÖ 10 chunks criados para: 30-Aprovados/T√≥picos/Governan√ßa e pap√©is organizacionais.md\n",
      "  ‚úÖ 15 chunks criados para: 30-Aprovados/T√≥picos/Padr√µes de Documenta√ß√£o do NIC.md\n",
      "  ‚úÖ 9 chunks criados para: 30-Aprovados/T√≥picos/Estrat√©gia de implanta√ß√£o e ado√ß√£o.md\n",
      "\n",
      "üìä Total de chunks: 322\n"
     ]
    }
   ],
   "source": [
    "# Processar cada arquivo\n",
    "all_chunks = []\n",
    "chunk_id = 0\n",
    "\n",
    "for text_file in documents:\n",
    "    try:\n",
    "        # Construir caminho completo para leitura\n",
    "        full_path = processed_dir / text_file\n",
    "        \n",
    "        # Ler texto\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_content = f.read()\n",
    "        \n",
    "        if not text_content.strip():\n",
    "            print(f\"  ‚ö†Ô∏è Arquivo vazio: {Path(text_file).name}\")\n",
    "            continue\n",
    "        \n",
    "        # Dividir em chunks\n",
    "        chunks = text_splitter.split_text(text_content)\n",
    "        \n",
    "        # Processar cada chunk\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            chunk_data = {\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"source_document\": text_file.replace(\".md\", \"\"),  # Usa caminho relativo sem extens√£o\n",
    "                \"chunk_index\": i,\n",
    "                \"text\": chunk_text.strip(),\n",
    "                \"char_count\": len(chunk_text)\n",
    "            }\n",
    "            \n",
    "            # Adicionar ao array com estrutura mais clara\n",
    "            all_chunks.append(chunk_data)\n",
    "            \n",
    "            chunk_id += 1\n",
    "        \n",
    "        print(f\"  ‚úÖ {len(chunks)} chunks criados para: {text_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Erro: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüìä Total de chunks: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Chunks salvos em: pipeline_data/chunks/chunks.jsonl\n",
      "üìä Total de chunks no arquivo: 322\n",
      "\n",
      "üìà Estat√≠sticas:\n",
      "  Total chunks: 322\n",
      "  Tamanho m√©dio: 350 caracteres\n",
      "  Tamanho m√≠nimo: 13 caracteres\n",
      "  Tamanho m√°ximo: 500 caracteres\n",
      "\n",
      "üìÅ Distribui√ß√£o de chunks por arquivo:\n",
      "  30-Aprovados/Mapas/Vis√£o Geral do Self Checkout: 51 chunks\n",
      "  30-Aprovados/T√≥picos/Componentes principais do sistema: 19 chunks\n",
      "  30-Aprovados/T√≥picos/Pr√©-requisitos t√©cnicos: 17 chunks\n",
      "  30-Aprovados/Mapas/Vis√£o Geral do NIC: 15 chunks\n",
      "  30-Aprovados/T√≥picos/Padr√µes de Documenta√ß√£o do NIC: 15 chunks\n",
      "  30-Aprovados/Mapas/Processa Sistemas: 13 chunks\n",
      "  30-Aprovados/T√≥picos/Prop√≥sito do NIC: 12 chunks\n",
      "  30-Aprovados/T√≥picos/Finalidade e vis√£o do NIC: 12 chunks\n",
      "  30-Aprovados/T√≥picos/Funcionalidade do bloqueio: 11 chunks\n",
      "  30-Aprovados/T√≥picos/Cronograma e marcos do projeto: 10 chunks\n",
      "  ... e mais 21 arquivos\n"
     ]
    }
   ],
   "source": [
    "# Salvar todos os chunks em um √∫nico arquivo JSONL\n",
    "jsonl_file = chunks_dir / \"chunks.jsonl\"\n",
    "with open(jsonl_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in all_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\nüíæ Chunks salvos em: {jsonl_file}\")\n",
    "print(f\"üìä Total de chunks no arquivo: {len(all_chunks)}\")\n",
    "\n",
    "# Estat√≠sticas\n",
    "if all_chunks:\n",
    "    avg_chars = sum(chunk[\"char_count\"] for chunk in all_chunks) / len(all_chunks)\n",
    "    max_chars = max(chunk[\"char_count\"] for chunk in all_chunks)\n",
    "    min_chars = min(chunk[\"char_count\"] for chunk in all_chunks)\n",
    "    \n",
    "    print(f\"\\nüìà Estat√≠sticas:\")\n",
    "    print(f\"  Total chunks: {len(all_chunks)}\")\n",
    "    print(f\"  Tamanho m√©dio: {avg_chars:.0f} caracteres\")\n",
    "    print(f\"  Tamanho m√≠nimo: {min_chars} caracteres\")\n",
    "    print(f\"  Tamanho m√°ximo: {max_chars} caracteres\")\n",
    "    \n",
    "    # Mostrar distribui√ß√£o por arquivo\n",
    "    from collections import Counter\n",
    "    \n",
    "    print(f\"\\nüìÅ Distribui√ß√£o de chunks por arquivo:\")\n",
    "    # Usar source_document que j√° est√° no chunk\n",
    "    file_counter = Counter(chunk[\"source_document\"] for chunk in all_chunks)\n",
    "    \n",
    "    # Mostrar top 10 arquivos com mais chunks\n",
    "    for file, count in file_counter.most_common(10):\n",
    "        print(f\"  {file}: {count} chunks\")\n",
    "    \n",
    "    if len(file_counter) > 10:\n",
    "        print(f\"  ... e mais {len(file_counter) - 10} arquivos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
