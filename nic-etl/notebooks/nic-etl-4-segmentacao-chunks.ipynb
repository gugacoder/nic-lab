{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî™ SEGMENTA√á√ÉO EM CHUNKS - Etapa 4/6\n",
    "\n",
    "## üìã O que este notebook faz\n",
    "\n",
    "Este notebook **divide documentos em peda√ßos menores** (chunks) otimizados para embeddings:\n",
    "\n",
    "- üìñ **L√™ documentos** processados em `pipeline-data/processed/`\n",
    "- ‚úÇÔ∏è **Segmenta texto** usando `RecursiveCharacterTextSplitter`\n",
    "- üîó **Aplica overlapping** (100 chars) para manter contexto entre chunks\n",
    "- üè∑Ô∏è **Adiciona metadados** (documento origem, √≠ndice, tamanho)\n",
    "- üíæ **Salva em JSONL** para processamento eficiente\n",
    "\n",
    "## ‚öôÔ∏è Configura√ß√£o de chunking\n",
    "\n",
    "- **Tamanho**: 500 caracteres por chunk\n",
    "- **Overlap**: 100 caracteres entre chunks adjacentes  \n",
    "- **Separadores**: Prioriza quebras sem√¢nticas (`\\n\\n`, `\\n`, `. `, ` `)\n",
    "\n",
    "## üìä Output esperado\n",
    "\n",
    "~322 chunks salvos em `pipeline-data/chunks/chunks.jsonl` com m√©dia de 350 caracteres cada.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T04:14:55.909146Z",
     "iopub.status.busy": "2025-08-18T04:14:55.908979Z",
     "iopub.status.idle": "2025-08-18T04:14:56.063971Z",
     "shell.execute_reply": "2025-08-18T04:14:56.063490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 500 tokens\n",
      "Overlap: 100 tokens\n",
      "Documentos encontrados: 31\n",
      "  30-Aprovados/Mapas/Vis√£o Geral do Self Checkout.md\n",
      "  30-Aprovados/Mapas/Vis√£o Geral do NIC.md\n",
      "  30-Aprovados/Mapas/Processa Sistemas.md\n",
      "  30-Aprovados/T√≥picos/Cancelamento de cupom.md\n",
      "  30-Aprovados/T√≥picos/Solicita√ß√£o de ajuda.md\n",
      "  30-Aprovados/T√≥picos/Hist√≥rico de atualiza√ß√µes Self Checkout.md\n",
      "  30-Aprovados/T√≥picos/Funcionalidade do bloqueio.md\n",
      "  30-Aprovados/T√≥picos/Aplica√ß√£o de desconto por item.md\n",
      "  30-Aprovados/T√≥picos/Prop√≥sito do NIC.md\n",
      "  30-Aprovados/T√≥picos/Reimpress√£o do √∫ltimo cupom.md\n",
      "  ... e mais 21 documentos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "# Configura√ß√£o\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"500\"))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", \"100\"))\n",
    "\n",
    "# Diret√≥rios\n",
    "processed_dir = Path(\"pipeline-data/processed\")\n",
    "chunks_dir = Path(\"pipeline-data/chunks\")\n",
    "chunks_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Limpar diret√≥rio chunks recursivamente\n",
    "for f in chunks_dir.rglob(\"*\"):\n",
    "    if f.is_file():\n",
    "        f.unlink()\n",
    "\n",
    "print(f\"Chunk size: {CHUNK_SIZE} tokens\")\n",
    "print(f\"Overlap: {CHUNK_OVERLAP} tokens\")\n",
    "\n",
    "# Listar documentos processados recursivamente com caminhos relativos\n",
    "documents = [str(doc.relative_to(processed_dir)) for doc in processed_dir.rglob(\"*.md\")]\n",
    "\n",
    "print(f\"Documentos encontrados: {len(documents)}\")\n",
    "\n",
    "for i, doc in enumerate(documents[:10]):\n",
    "    print(f\"  {doc}\")\n",
    "\n",
    "if len(documents) > 10:\n",
    "    print(f\"  ... e mais {len(documents) - 10} documentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Marcar in√≠cio da execu√ß√£o\n",
    "stage_start = time.time()\n",
    "start_timestamp = datetime.now().isoformat() + \"Z\"\n",
    "\n",
    "# Configura√ß√£o\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"500\"))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", \"100\"))\n",
    "\n",
    "# Diret√≥rios\n",
    "processed_dir = Path(\"pipeline-data/processed\")\n",
    "chunks_dir = Path(\"pipeline-data/chunks\")\n",
    "chunks_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Limpar diret√≥rio chunks recursivamente\n",
    "for f in chunks_dir.rglob(\"*\"):\n",
    "    if f.is_file():\n",
    "        f.unlink()\n",
    "\n",
    "print(f\"Chunk size: {CHUNK_SIZE} tokens\")\n",
    "print(f\"Overlap: {CHUNK_OVERLAP} tokens\")\n",
    "\n",
    "# Listar documentos processados recursivamente com caminhos relativos\n",
    "documents = [str(doc.relative_to(processed_dir)) for doc in processed_dir.rglob(\"*.md\")]\n",
    "\n",
    "print(f\"Documentos encontrados: {len(documents)}\")\n",
    "\n",
    "for i, doc in enumerate(documents[:10]):\n",
    "    print(f\"  {doc}\")\n",
    "\n",
    "if len(documents) > 10:\n",
    "    print(f\"  ... e mais {len(documents) - 10} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T04:14:56.095746Z",
     "iopub.status.busy": "2025-08-18T04:14:56.095446Z",
     "iopub.status.idle": "2025-08-18T04:14:56.100051Z",
     "shell.execute_reply": "2025-08-18T04:14:56.099451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text splitter configurado:\n",
      "  Chunk size: 500\n",
      "  Overlap: 100\n",
      "  Separadores: ['\\n\\n', '\\n', '. ', ' ', '']\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Text splitter configurado:\")\n",
    "print(f\"  Chunk size: {CHUNK_SIZE}\")\n",
    "print(f\"  Overlap: {CHUNK_OVERLAP}\")\n",
    "print(f\"  Separadores: {text_splitter._separators}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî™ Segmenta√ß√£o de Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T04:14:56.102009Z",
     "iopub.status.busy": "2025-08-18T04:14:56.101849Z",
     "iopub.status.idle": "2025-08-18T04:14:56.111443Z",
     "shell.execute_reply": "2025-08-18T04:14:56.110888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ 51 chunks criados para: 30-Aprovados/Mapas/Vis√£o Geral do Self Checkout.md\n",
      "  ‚úÖ 15 chunks criados para: 30-Aprovados/Mapas/Vis√£o Geral do NIC.md\n",
      "  ‚úÖ 13 chunks criados para: 30-Aprovados/Mapas/Processa Sistemas.md\n",
      "  ‚úÖ 7 chunks criados para: 30-Aprovados/T√≥picos/Cancelamento de cupom.md\n",
      "  ‚úÖ 6 chunks criados para: 30-Aprovados/T√≥picos/Solicita√ß√£o de ajuda.md\n",
      "  ‚úÖ 5 chunks criados para: 30-Aprovados/T√≥picos/Hist√≥rico de atualiza√ß√µes Self Checkout.md\n",
      "  ‚úÖ 11 chunks criados para: 30-Aprovados/T√≥picos/Funcionalidade do bloqueio.md\n",
      "  ‚úÖ 7 chunks criados para: 30-Aprovados/T√≥picos/Aplica√ß√£o de desconto por item.md\n",
      "  ‚úÖ 12 chunks criados para: 30-Aprovados/T√≥picos/Prop√≥sito do NIC.md\n",
      "  ‚úÖ 4 chunks criados para: 30-Aprovados/T√≥picos/Reimpress√£o do √∫ltimo cupom.md\n",
      "  ‚úÖ 8 chunks criados para: 30-Aprovados/T√≥picos/Fun√ß√£o do Chat NIC.md\n",
      "  ‚úÖ 9 chunks criados para: 30-Aprovados/T√≥picos/Acesso ao menu do fiscal.md\n",
      "  ‚úÖ 10 chunks criados para: 30-Aprovados/T√≥picos/Cronograma e marcos do projeto.md\n",
      "  ‚úÖ 7 chunks criados para: 30-Aprovados/T√≥picos/Efetuar pagamento.md\n",
      "  ‚úÖ 5 chunks criados para: 30-Aprovados/T√≥picos/Cancelamento de item.md\n",
      "  ‚úÖ 7 chunks criados para: 30-Aprovados/T√≥picos/Opera√ß√£o cont√≠nua e evolu√ß√£o do sistema.md\n",
      "  ‚úÖ 12 chunks criados para: 30-Aprovados/T√≥picos/Finalidade e vis√£o do NIC.md\n",
      "  ‚úÖ 2 chunks criados para: 30-Aprovados/T√≥picos/Iniciar a compra.md\n",
      "  ‚úÖ 8 chunks criados para: 30-Aprovados/T√≥picos/Processo de documenta√ß√£o e valida√ß√£o.md\n",
      "  ‚úÖ 9 chunks criados para: 30-Aprovados/T√≥picos/Infraestrutura t√©cnica e operacional.md\n",
      "  ‚úÖ 7 chunks criados para: 30-Aprovados/T√≥picos/Apresenta√ß√£o do sistema Self Checkout.md\n",
      "  ‚úÖ 8 chunks criados para: 30-Aprovados/T√≥picos/Retornar para registro de venda.md\n",
      "  ‚úÖ 19 chunks criados para: 30-Aprovados/T√≥picos/Componentes principais do sistema.md\n",
      "  ‚úÖ 10 chunks criados para: 30-Aprovados/T√≥picos/Registro de produtos.md\n",
      "  ‚úÖ 17 chunks criados para: 30-Aprovados/T√≥picos/Pr√©-requisitos t√©cnicos.md\n",
      "  ‚úÖ 8 chunks criados para: 30-Aprovados/T√≥picos/Estrutura e fluxo da base de conhecimento.md\n",
      "  ‚úÖ 5 chunks criados para: 30-Aprovados/T√≥picos/Finaliza√ß√£o do movimento di√°rio.md\n",
      "  ‚úÖ 6 chunks criados para: 30-Aprovados/T√≥picos/Identifica√ß√£o do cliente.md\n",
      "  ‚úÖ 10 chunks criados para: 30-Aprovados/T√≥picos/Governan√ßa e pap√©is organizacionais.md\n",
      "  ‚úÖ 15 chunks criados para: 30-Aprovados/T√≥picos/Padr√µes de Documenta√ß√£o do NIC.md\n",
      "  ‚úÖ 9 chunks criados para: 30-Aprovados/T√≥picos/Estrat√©gia de implanta√ß√£o e ado√ß√£o.md\n",
      "\n",
      "üìä Total de chunks: 322\n"
     ]
    }
   ],
   "source": [
    "# Processar cada arquivo\n",
    "all_chunks = []\n",
    "chunk_id = 0\n",
    "\n",
    "for text_file in documents:\n",
    "    try:\n",
    "        # Construir caminho completo para leitura\n",
    "        full_path = processed_dir / text_file\n",
    "        \n",
    "        # Ler texto\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_content = f.read()\n",
    "        \n",
    "        if not text_content.strip():\n",
    "            print(f\"  ‚ö†Ô∏è Arquivo vazio: {Path(text_file).name}\")\n",
    "            continue\n",
    "        \n",
    "        # Dividir em chunks\n",
    "        chunks = text_splitter.split_text(text_content)\n",
    "        \n",
    "        # Processar cada chunk\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            chunk_data = {\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"source_document\": text_file.replace(\".md\", \"\"),  # Usa caminho relativo sem extens√£o\n",
    "                \"chunk_index\": i,\n",
    "                \"text\": chunk_text.strip(),\n",
    "                \"char_count\": len(chunk_text)\n",
    "            }\n",
    "            \n",
    "            # Adicionar ao array com estrutura mais clara\n",
    "            all_chunks.append(chunk_data)\n",
    "            \n",
    "            chunk_id += 1\n",
    "        \n",
    "        print(f\"  ‚úÖ {len(chunks)} chunks criados para: {text_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Erro: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüìä Total de chunks: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Armazenamento e Estat√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T04:14:56.113637Z",
     "iopub.status.busy": "2025-08-18T04:14:56.113502Z",
     "iopub.status.idle": "2025-08-18T04:14:56.121283Z",
     "shell.execute_reply": "2025-08-18T04:14:56.120835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Chunks salvos em: pipeline-data/chunks/chunks.jsonl\n",
      "üìä Total de chunks no arquivo: 322\n",
      "\n",
      "üìà Estat√≠sticas:\n",
      "  Total chunks: 322\n",
      "  Tamanho m√©dio: 350 caracteres\n",
      "  Tamanho m√≠nimo: 13 caracteres\n",
      "  Tamanho m√°ximo: 500 caracteres\n",
      "\n",
      "üìÅ Distribui√ß√£o de chunks por arquivo:\n",
      "  30-Aprovados/Mapas/Vis√£o Geral do Self Checkout: 51 chunks\n",
      "  30-Aprovados/T√≥picos/Componentes principais do sistema: 19 chunks\n",
      "  30-Aprovados/T√≥picos/Pr√©-requisitos t√©cnicos: 17 chunks\n",
      "  30-Aprovados/Mapas/Vis√£o Geral do NIC: 15 chunks\n",
      "  30-Aprovados/T√≥picos/Padr√µes de Documenta√ß√£o do NIC: 15 chunks\n",
      "  30-Aprovados/Mapas/Processa Sistemas: 13 chunks\n",
      "  30-Aprovados/T√≥picos/Prop√≥sito do NIC: 12 chunks\n",
      "  30-Aprovados/T√≥picos/Finalidade e vis√£o do NIC: 12 chunks\n",
      "  30-Aprovados/T√≥picos/Funcionalidade do bloqueio: 11 chunks\n",
      "  30-Aprovados/T√≥picos/Cronograma e marcos do projeto: 10 chunks\n",
      "  ... e mais 21 arquivos\n"
     ]
    }
   ],
   "source": [
    "# Salvar todos os chunks em um √∫nico arquivo JSONL\n",
    "jsonl_file = chunks_dir / \"chunks.jsonl\"\n",
    "with open(jsonl_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in all_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\nüíæ Chunks salvos em: {jsonl_file}\")\n",
    "print(f\"üìä Total de chunks no arquivo: {len(all_chunks)}\")\n",
    "\n",
    "# Estat√≠sticas\n",
    "if all_chunks:\n",
    "    avg_chars = sum(chunk[\"char_count\"] for chunk in all_chunks) / len(all_chunks)\n",
    "    max_chars = max(chunk[\"char_count\"] for chunk in all_chunks)\n",
    "    min_chars = min(chunk[\"char_count\"] for chunk in all_chunks)\n",
    "    \n",
    "    print(f\"\\nüìà Estat√≠sticas:\")\n",
    "    print(f\"  Total chunks: {len(all_chunks)}\")\n",
    "    print(f\"  Tamanho m√©dio: {avg_chars:.0f} caracteres\")\n",
    "    print(f\"  Tamanho m√≠nimo: {min_chars} caracteres\")\n",
    "    print(f\"  Tamanho m√°ximo: {max_chars} caracteres\")\n",
    "    \n",
    "    # Mostrar distribui√ß√£o por arquivo\n",
    "    from collections import Counter\n",
    "    \n",
    "    print(f\"\\nüìÅ Distribui√ß√£o de chunks por arquivo:\")\n",
    "    # Usar source_document que j√° est√° no chunk\n",
    "    file_counter = Counter(chunk[\"source_document\"] for chunk in all_chunks)\n",
    "    \n",
    "    # Mostrar top 10 arquivos com mais chunks\n",
    "    for file, count in file_counter.most_common(10):\n",
    "        print(f\"  {file}: {count} chunks\")\n",
    "    \n",
    "    if len(file_counter) > 10:\n",
    "        print(f\"  ... e mais {len(file_counter) - 10} arquivos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Relat√≥rio de Execu√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T04:14:56.123094Z",
     "iopub.status.busy": "2025-08-18T04:14:56.122944Z",
     "iopub.status.idle": "2025-08-18T04:14:56.129426Z",
     "shell.execute_reply": "2025-08-18T04:14:56.128986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Relat√≥rio atualizado: pipeline-data/report.json\n",
      "‚è±Ô∏è Dura√ß√£o da etapa: 0.00s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Se n√£o tiver stage_start, calcular agora\n",
    "if 'stage_start' not in locals():\n",
    "    stage_start = time.time()\n",
    "    start_timestamp = datetime.now().isoformat() + \"Z\"\n",
    "\n",
    "# Calcular dura√ß√£o\n",
    "stage_duration = time.time() - stage_start\n",
    "\n",
    "# Carregar relat√≥rio existente\n",
    "report_path = Path(\"pipeline-data/report.json\")\n",
    "if report_path.exists():\n",
    "    with open(report_path, \"r\") as f:\n",
    "        report = json.load(f)\n",
    "else:\n",
    "    report = {\"stages\": [], \"context\": {}, \"summary\": {}}\n",
    "\n",
    "# Atualizar contexto com configura√ß√µes de chunking\n",
    "report[\"context\"].update({\n",
    "    \"chunk_size\": CHUNK_SIZE,\n",
    "    \"chunk_overlap\": CHUNK_OVERLAP\n",
    "})\n",
    "\n",
    "# Adicionar informa√ß√µes desta etapa\n",
    "stage_report = {\n",
    "    \"stage\": 4,\n",
    "    \"name\": \"Segmenta√ß√£o em Chunks\",\n",
    "    \"status\": \"SUCCESS\" if len(all_chunks) > 0 else \"FAILED\",\n",
    "    \"start_time\": start_timestamp if 'start_timestamp' in locals() else datetime.now().isoformat() + \"Z\",\n",
    "    \"duration_seconds\": round(stage_duration, 2),\n",
    "    \"results\": {\n",
    "        \"documents_processed\": len(documents),\n",
    "        \"total_chunks\": len(all_chunks),\n",
    "        \"avg_chunk_size\": round(avg_chars) if all_chunks else 0,\n",
    "        \"min_chunk_size\": min_chars if all_chunks else 0,\n",
    "        \"max_chunk_size\": max_chars if all_chunks else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Adicionar ou atualizar stage no relat√≥rio\n",
    "stages_updated = False\n",
    "for i, stage in enumerate(report[\"stages\"]):\n",
    "    if stage[\"stage\"] == 4:\n",
    "        report[\"stages\"][i] = stage_report\n",
    "        stages_updated = True\n",
    "        break\n",
    "\n",
    "if not stages_updated:\n",
    "    report[\"stages\"].append(stage_report)\n",
    "\n",
    "# Atualizar timestamp\n",
    "report[\"summary\"][\"last_update\"] = datetime.now().isoformat() + \"Z\"\n",
    "\n",
    "# Salvar relat√≥rio atualizado\n",
    "with open(report_path, \"w\") as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nüìä Relat√≥rio atualizado: {report_path}\")\n",
    "print(f\"‚è±Ô∏è Dura√ß√£o da etapa: {stage_duration:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
