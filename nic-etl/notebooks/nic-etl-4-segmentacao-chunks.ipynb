{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔪 SEGMENTAÇÃO EM CHUNKS - Etapa 4/6\n",
    "\n",
    "## 📋 O que este notebook faz\n",
    "\n",
    "Este notebook **divide documentos em pedaços menores** (chunks) otimizados para embeddings:\n",
    "\n",
    "- 📖 **Lê documentos** processados em `pipeline-data/processed/`\n",
    "- ✂️ **Segmenta texto** usando `RecursiveCharacterTextSplitter`\n",
    "- 🔗 **Aplica overlapping** (100 chars) para manter contexto entre chunks\n",
    "- 🏷️ **Adiciona metadados** (documento origem, índice, tamanho)\n",
    "- 💾 **Salva em JSONL** para processamento eficiente\n",
    "\n",
    "## ⚙️ Configuração de chunking\n",
    "\n",
    "- **Tamanho**: 500 caracteres por chunk\n",
    "- **Overlap**: 100 caracteres entre chunks adjacentes  \n",
    "- **Separadores**: Prioriza quebras semânticas (`\\n\\n`, `\\n`, `. `, ` `)\n",
    "\n",
    "## 📊 Output esperado\n",
    "\n",
    "~322 chunks salvos em `pipeline-data/chunks/chunks.jsonl` com média de 350 caracteres cada.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T04:14:55.909146Z",
     "iopub.status.busy": "2025-08-18T04:14:55.908979Z",
     "iopub.status.idle": "2025-08-18T04:14:56.063971Z",
     "shell.execute_reply": "2025-08-18T04:14:56.063490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 500 tokens\n",
      "Overlap: 100 tokens\n",
      "Documentos encontrados: 31\n",
      "  30-Aprovados/Mapas/Visão Geral do Self Checkout.md\n",
      "  30-Aprovados/Mapas/Visão Geral do NIC.md\n",
      "  30-Aprovados/Mapas/Processa Sistemas.md\n",
      "  30-Aprovados/Tópicos/Cancelamento de cupom.md\n",
      "  30-Aprovados/Tópicos/Solicitação de ajuda.md\n",
      "  30-Aprovados/Tópicos/Histórico de atualizações Self Checkout.md\n",
      "  30-Aprovados/Tópicos/Funcionalidade do bloqueio.md\n",
      "  30-Aprovados/Tópicos/Aplicação de desconto por item.md\n",
      "  30-Aprovados/Tópicos/Propósito do NIC.md\n",
      "  30-Aprovados/Tópicos/Reimpressão do último cupom.md\n",
      "  ... e mais 21 documentos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "# Configuração\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"500\"))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", \"100\"))\n",
    "\n",
    "# Diretórios\n",
    "processed_dir = Path(\"pipeline-data/processed\")\n",
    "chunks_dir = Path(\"pipeline-data/chunks\")\n",
    "chunks_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Limpar diretório chunks recursivamente\n",
    "for f in chunks_dir.rglob(\"*\"):\n",
    "    if f.is_file():\n",
    "        f.unlink()\n",
    "\n",
    "print(f\"Chunk size: {CHUNK_SIZE} tokens\")\n",
    "print(f\"Overlap: {CHUNK_OVERLAP} tokens\")\n",
    "\n",
    "# Listar documentos processados recursivamente com caminhos relativos\n",
    "documents = [str(doc.relative_to(processed_dir)) for doc in processed_dir.rglob(\"*.md\")]\n",
    "\n",
    "print(f\"Documentos encontrados: {len(documents)}\")\n",
    "\n",
    "for i, doc in enumerate(documents[:10]):\n",
    "    print(f\"  {doc}\")\n",
    "\n",
    "if len(documents) > 10:\n",
    "    print(f\"  ... e mais {len(documents) - 10} documentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Marcar início da execução\n",
    "stage_start = time.time()\n",
    "start_timestamp = datetime.now().isoformat() + \"Z\"\n",
    "\n",
    "# Configuração\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"500\"))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", \"100\"))\n",
    "\n",
    "# Diretórios\n",
    "processed_dir = Path(\"pipeline-data/processed\")\n",
    "chunks_dir = Path(\"pipeline-data/chunks\")\n",
    "chunks_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Limpar diretório chunks recursivamente\n",
    "for f in chunks_dir.rglob(\"*\"):\n",
    "    if f.is_file():\n",
    "        f.unlink()\n",
    "\n",
    "print(f\"Chunk size: {CHUNK_SIZE} tokens\")\n",
    "print(f\"Overlap: {CHUNK_OVERLAP} tokens\")\n",
    "\n",
    "# Listar documentos processados recursivamente com caminhos relativos\n",
    "documents = [str(doc.relative_to(processed_dir)) for doc in processed_dir.rglob(\"*.md\")]\n",
    "\n",
    "print(f\"Documentos encontrados: {len(documents)}\")\n",
    "\n",
    "for i, doc in enumerate(documents[:10]):\n",
    "    print(f\"  {doc}\")\n",
    "\n",
    "if len(documents) > 10:\n",
    "    print(f\"  ... e mais {len(documents) - 10} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T04:14:56.095746Z",
     "iopub.status.busy": "2025-08-18T04:14:56.095446Z",
     "iopub.status.idle": "2025-08-18T04:14:56.100051Z",
     "shell.execute_reply": "2025-08-18T04:14:56.099451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text splitter configurado:\n",
      "  Chunk size: 500\n",
      "  Overlap: 100\n",
      "  Separadores: ['\\n\\n', '\\n', '. ', ' ', '']\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(f\"✅ Text splitter configurado:\")\n",
    "print(f\"  Chunk size: {CHUNK_SIZE}\")\n",
    "print(f\"  Overlap: {CHUNK_OVERLAP}\")\n",
    "print(f\"  Separadores: {text_splitter._separators}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔪 Segmentação de Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T04:14:56.102009Z",
     "iopub.status.busy": "2025-08-18T04:14:56.101849Z",
     "iopub.status.idle": "2025-08-18T04:14:56.111443Z",
     "shell.execute_reply": "2025-08-18T04:14:56.110888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ 51 chunks criados para: 30-Aprovados/Mapas/Visão Geral do Self Checkout.md\n",
      "  ✅ 15 chunks criados para: 30-Aprovados/Mapas/Visão Geral do NIC.md\n",
      "  ✅ 13 chunks criados para: 30-Aprovados/Mapas/Processa Sistemas.md\n",
      "  ✅ 7 chunks criados para: 30-Aprovados/Tópicos/Cancelamento de cupom.md\n",
      "  ✅ 6 chunks criados para: 30-Aprovados/Tópicos/Solicitação de ajuda.md\n",
      "  ✅ 5 chunks criados para: 30-Aprovados/Tópicos/Histórico de atualizações Self Checkout.md\n",
      "  ✅ 11 chunks criados para: 30-Aprovados/Tópicos/Funcionalidade do bloqueio.md\n",
      "  ✅ 7 chunks criados para: 30-Aprovados/Tópicos/Aplicação de desconto por item.md\n",
      "  ✅ 12 chunks criados para: 30-Aprovados/Tópicos/Propósito do NIC.md\n",
      "  ✅ 4 chunks criados para: 30-Aprovados/Tópicos/Reimpressão do último cupom.md\n",
      "  ✅ 8 chunks criados para: 30-Aprovados/Tópicos/Função do Chat NIC.md\n",
      "  ✅ 9 chunks criados para: 30-Aprovados/Tópicos/Acesso ao menu do fiscal.md\n",
      "  ✅ 10 chunks criados para: 30-Aprovados/Tópicos/Cronograma e marcos do projeto.md\n",
      "  ✅ 7 chunks criados para: 30-Aprovados/Tópicos/Efetuar pagamento.md\n",
      "  ✅ 5 chunks criados para: 30-Aprovados/Tópicos/Cancelamento de item.md\n",
      "  ✅ 7 chunks criados para: 30-Aprovados/Tópicos/Operação contínua e evolução do sistema.md\n",
      "  ✅ 12 chunks criados para: 30-Aprovados/Tópicos/Finalidade e visão do NIC.md\n",
      "  ✅ 2 chunks criados para: 30-Aprovados/Tópicos/Iniciar a compra.md\n",
      "  ✅ 8 chunks criados para: 30-Aprovados/Tópicos/Processo de documentação e validação.md\n",
      "  ✅ 9 chunks criados para: 30-Aprovados/Tópicos/Infraestrutura técnica e operacional.md\n",
      "  ✅ 7 chunks criados para: 30-Aprovados/Tópicos/Apresentação do sistema Self Checkout.md\n",
      "  ✅ 8 chunks criados para: 30-Aprovados/Tópicos/Retornar para registro de venda.md\n",
      "  ✅ 19 chunks criados para: 30-Aprovados/Tópicos/Componentes principais do sistema.md\n",
      "  ✅ 10 chunks criados para: 30-Aprovados/Tópicos/Registro de produtos.md\n",
      "  ✅ 17 chunks criados para: 30-Aprovados/Tópicos/Pré-requisitos técnicos.md\n",
      "  ✅ 8 chunks criados para: 30-Aprovados/Tópicos/Estrutura e fluxo da base de conhecimento.md\n",
      "  ✅ 5 chunks criados para: 30-Aprovados/Tópicos/Finalização do movimento diário.md\n",
      "  ✅ 6 chunks criados para: 30-Aprovados/Tópicos/Identificação do cliente.md\n",
      "  ✅ 10 chunks criados para: 30-Aprovados/Tópicos/Governança e papéis organizacionais.md\n",
      "  ✅ 15 chunks criados para: 30-Aprovados/Tópicos/Padrões de Documentação do NIC.md\n",
      "  ✅ 9 chunks criados para: 30-Aprovados/Tópicos/Estratégia de implantação e adoção.md\n",
      "\n",
      "📊 Total de chunks: 322\n"
     ]
    }
   ],
   "source": [
    "# Processar cada arquivo\n",
    "all_chunks = []\n",
    "chunk_id = 0\n",
    "\n",
    "for text_file in documents:\n",
    "    try:\n",
    "        # Construir caminho completo para leitura\n",
    "        full_path = processed_dir / text_file\n",
    "        \n",
    "        # Ler texto\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_content = f.read()\n",
    "        \n",
    "        if not text_content.strip():\n",
    "            print(f\"  ⚠️ Arquivo vazio: {Path(text_file).name}\")\n",
    "            continue\n",
    "        \n",
    "        # Dividir em chunks\n",
    "        chunks = text_splitter.split_text(text_content)\n",
    "        \n",
    "        # Processar cada chunk\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            chunk_data = {\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"source_document\": text_file.replace(\".md\", \"\"),  # Usa caminho relativo sem extensão\n",
    "                \"chunk_index\": i,\n",
    "                \"text\": chunk_text.strip(),\n",
    "                \"char_count\": len(chunk_text)\n",
    "            }\n",
    "            \n",
    "            # Adicionar ao array com estrutura mais clara\n",
    "            all_chunks.append(chunk_data)\n",
    "            \n",
    "            chunk_id += 1\n",
    "        \n",
    "        print(f\"  ✅ {len(chunks)} chunks criados para: {text_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Erro: {str(e)}\")\n",
    "\n",
    "print(f\"\\n📊 Total de chunks: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Armazenamento e Estatísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T04:14:56.113637Z",
     "iopub.status.busy": "2025-08-18T04:14:56.113502Z",
     "iopub.status.idle": "2025-08-18T04:14:56.121283Z",
     "shell.execute_reply": "2025-08-18T04:14:56.120835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Chunks salvos em: pipeline-data/chunks/chunks.jsonl\n",
      "📊 Total de chunks no arquivo: 322\n",
      "\n",
      "📈 Estatísticas:\n",
      "  Total chunks: 322\n",
      "  Tamanho médio: 350 caracteres\n",
      "  Tamanho mínimo: 13 caracteres\n",
      "  Tamanho máximo: 500 caracteres\n",
      "\n",
      "📁 Distribuição de chunks por arquivo:\n",
      "  30-Aprovados/Mapas/Visão Geral do Self Checkout: 51 chunks\n",
      "  30-Aprovados/Tópicos/Componentes principais do sistema: 19 chunks\n",
      "  30-Aprovados/Tópicos/Pré-requisitos técnicos: 17 chunks\n",
      "  30-Aprovados/Mapas/Visão Geral do NIC: 15 chunks\n",
      "  30-Aprovados/Tópicos/Padrões de Documentação do NIC: 15 chunks\n",
      "  30-Aprovados/Mapas/Processa Sistemas: 13 chunks\n",
      "  30-Aprovados/Tópicos/Propósito do NIC: 12 chunks\n",
      "  30-Aprovados/Tópicos/Finalidade e visão do NIC: 12 chunks\n",
      "  30-Aprovados/Tópicos/Funcionalidade do bloqueio: 11 chunks\n",
      "  30-Aprovados/Tópicos/Cronograma e marcos do projeto: 10 chunks\n",
      "  ... e mais 21 arquivos\n"
     ]
    }
   ],
   "source": [
    "# Salvar todos os chunks em um único arquivo JSONL\n",
    "jsonl_file = chunks_dir / \"chunks.jsonl\"\n",
    "with open(jsonl_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in all_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\n💾 Chunks salvos em: {jsonl_file}\")\n",
    "print(f\"📊 Total de chunks no arquivo: {len(all_chunks)}\")\n",
    "\n",
    "# Estatísticas\n",
    "if all_chunks:\n",
    "    avg_chars = sum(chunk[\"char_count\"] for chunk in all_chunks) / len(all_chunks)\n",
    "    max_chars = max(chunk[\"char_count\"] for chunk in all_chunks)\n",
    "    min_chars = min(chunk[\"char_count\"] for chunk in all_chunks)\n",
    "    \n",
    "    print(f\"\\n📈 Estatísticas:\")\n",
    "    print(f\"  Total chunks: {len(all_chunks)}\")\n",
    "    print(f\"  Tamanho médio: {avg_chars:.0f} caracteres\")\n",
    "    print(f\"  Tamanho mínimo: {min_chars} caracteres\")\n",
    "    print(f\"  Tamanho máximo: {max_chars} caracteres\")\n",
    "    \n",
    "    # Mostrar distribuição por arquivo\n",
    "    from collections import Counter\n",
    "    \n",
    "    print(f\"\\n📁 Distribuição de chunks por arquivo:\")\n",
    "    # Usar source_document que já está no chunk\n",
    "    file_counter = Counter(chunk[\"source_document\"] for chunk in all_chunks)\n",
    "    \n",
    "    # Mostrar top 10 arquivos com mais chunks\n",
    "    for file, count in file_counter.most_common(10):\n",
    "        print(f\"  {file}: {count} chunks\")\n",
    "    \n",
    "    if len(file_counter) > 10:\n",
    "        print(f\"  ... e mais {len(file_counter) - 10} arquivos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Relatório de Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T04:14:56.123094Z",
     "iopub.status.busy": "2025-08-18T04:14:56.122944Z",
     "iopub.status.idle": "2025-08-18T04:14:56.129426Z",
     "shell.execute_reply": "2025-08-18T04:14:56.128986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Relatório atualizado: pipeline-data/report.json\n",
      "⏱️ Duração da etapa: 0.00s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Se não tiver stage_start, calcular agora\n",
    "if 'stage_start' not in locals():\n",
    "    stage_start = time.time()\n",
    "    start_timestamp = datetime.now().isoformat() + \"Z\"\n",
    "\n",
    "# Calcular duração\n",
    "stage_duration = time.time() - stage_start\n",
    "\n",
    "# Carregar relatório existente\n",
    "report_path = Path(\"pipeline-data/report.json\")\n",
    "if report_path.exists():\n",
    "    with open(report_path, \"r\") as f:\n",
    "        report = json.load(f)\n",
    "else:\n",
    "    report = {\"stages\": [], \"context\": {}, \"summary\": {}}\n",
    "\n",
    "# Atualizar contexto com configurações de chunking\n",
    "report[\"context\"].update({\n",
    "    \"chunk_size\": CHUNK_SIZE,\n",
    "    \"chunk_overlap\": CHUNK_OVERLAP\n",
    "})\n",
    "\n",
    "# Adicionar informações desta etapa\n",
    "stage_report = {\n",
    "    \"stage\": 4,\n",
    "    \"name\": \"Segmentação em Chunks\",\n",
    "    \"status\": \"SUCCESS\" if len(all_chunks) > 0 else \"FAILED\",\n",
    "    \"start_time\": start_timestamp if 'start_timestamp' in locals() else datetime.now().isoformat() + \"Z\",\n",
    "    \"duration_seconds\": round(stage_duration, 2),\n",
    "    \"results\": {\n",
    "        \"documents_processed\": len(documents),\n",
    "        \"total_chunks\": len(all_chunks),\n",
    "        \"avg_chunk_size\": round(avg_chars) if all_chunks else 0,\n",
    "        \"min_chunk_size\": min_chars if all_chunks else 0,\n",
    "        \"max_chunk_size\": max_chars if all_chunks else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Adicionar ou atualizar stage no relatório\n",
    "stages_updated = False\n",
    "for i, stage in enumerate(report[\"stages\"]):\n",
    "    if stage[\"stage\"] == 4:\n",
    "        report[\"stages\"][i] = stage_report\n",
    "        stages_updated = True\n",
    "        break\n",
    "\n",
    "if not stages_updated:\n",
    "    report[\"stages\"].append(stage_report)\n",
    "\n",
    "# Atualizar timestamp\n",
    "report[\"summary\"][\"last_update\"] = datetime.now().isoformat() + \"Z\"\n",
    "\n",
    "# Salvar relatório atualizado\n",
    "with open(report_path, \"w\") as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n📊 Relatório atualizado: {report_path}\")\n",
    "print(f\"⏱️ Duração da etapa: {stage_duration:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
