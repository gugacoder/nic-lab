{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 📊 ETAPA 7: VALIDAÇÃO E RESULTADOS\n",
    "\n",
    "## 🎯 **O que esta etapa faz**\n",
    "Executa testes abrangentes de qualidade, performance e funcionalidade do sistema completo de busca semântica, validando se o pipeline NIC ETL está funcionando corretamente.\n",
    "\n",
    "## 🤔 **Por que esta etapa é necessária**\n",
    "Para garantir que o sistema está pronto para uso, precisamos:\n",
    "- ✅ **Validar qualidade** da busca semântica\n",
    "- ⚡ **Medir performance** de resposta e precisão\n",
    "- 🔍 **Testar cenários reais** de uso\n",
    "- 📊 **Gerar relatórios** de qualidade e métricas\n",
    "- 🛡️ **Verificar integridade** de todos os dados\n",
    "\n",
    "## ⚙️ **Como funciona**\n",
    "1. **Valida pipeline completo** - todas as etapas executadas\n",
    "2. **Testa busca semântica** - consultas variadas e relevância\n",
    "3. **Mede performance** - tempos de resposta e throughput\n",
    "4. **Analisa qualidade** - precisão dos resultados\n",
    "5. **Testa filtragem** - metadados e campos específicos\n",
    "6. **Gera relatório final** - métricas e recomendações\n",
    "\n",
    "## 📊 **Tipos de validação executados**\n",
    "- `Integridade de Dados`: Contagens, consistência, metadados\n",
    "- `Qualidade de Busca`: Relevância, precisão, recall\n",
    "- `Performance`: Tempos de resposta, throughput\n",
    "- `Funcionalidade`: Filtros, ordenação, paginação\n",
    "- `Robustez`: Consultas edge case, stress test\n",
    "\n",
    "## 👁️ **O que esperar de saída**\n",
    "- 📊 **Relatório completo** de validação com métricas\n",
    "- ✅ **Status de qualidade** para cada componente\n",
    "- ⚡ **Benchmarks** de performance\n",
    "- 🔍 **Exemplos de busca** funcionais\n",
    "- 📋 **Recomendações** de otimização (se aplicável)\n",
    "\n",
    "## ⚠️ **Pontos importantes**\n",
    "- **Requer todas as etapas anteriores** concluídas\n",
    "- **Testes não modificam dados** - apenas leitura e consulta\n",
    "- **Pode demorar** dependendo do volume de dados\n",
    "- **Resultados são determinísticos** - repetíveis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 🛡️ Verificação de Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛡️ VERIFICAÇÃO AUTOMÁTICA DE DEPENDÊNCIAS\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Adicionar biblioteca ao path\n",
    "sys.path.insert(0, str(Path().parent / \"src\"))\n",
    "\n",
    "from pipeline_utils import (\n",
    "    PipelineState, \n",
    "    check_prerequisites, \n",
    "    show_pipeline_progress\n",
    ")\n",
    "\n",
    "print(\"📊 ETAPA 7: VALIDAÇÃO E RESULTADOS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"🕒 Iniciado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Verificar dependências\n",
    "CURRENT_STAGE = 7\n",
    "prerequisites_ok = check_prerequisites(CURRENT_STAGE)\n",
    "\n",
    "if not prerequisites_ok:\n",
    "    print(\"\\n❌ ERRO: Dependências não atendidas!\")\n",
    "    print(\"📋 Execute primeiro TODAS as etapas anteriores:\")\n",
    "    print(\"   1. 01_FUNDACAO_PREPARACAO.ipynb\")\n",
    "    print(\"   2. 02_COLETA_GITLAB.ipynb\")\n",
    "    print(\"   3. 03_PROCESSAMENTO_DOCLING.ipynb\")\n",
    "    print(\"   4. 04_SEGMENTACAO_CHUNKS.ipynb\")\n",
    "    print(\"   5. 05_GERACAO_EMBEDDINGS.ipynb\")\n",
    "    print(\"   6. 06_ARMAZENAMENTO_QDRANT.ipynb\")\n",
    "    \n",
    "    show_pipeline_progress()\n",
    "    raise RuntimeError(\"Dependências não atendidas - execute etapas anteriores primeiro\")\n",
    "\n",
    "print(\"\\n✅ Dependências verificadas - pipeline completo!\")\n",
    "print(\"📈 Progresso atual do pipeline:\")\n",
    "show_pipeline_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 📋 Configuração de Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 CONFIGURAÇÃO DOS TESTES DE VALIDAÇÃO\n",
    "print(\"📋 Configurando parâmetros de validação...\")\n",
    "\n",
    "# Carregar configurações de todas as etapas\n",
    "state = PipelineState()\n",
    "all_stage_data = {}\n",
    "\n",
    "for stage in range(1, 7):\n",
    "    try:\n",
    "        all_stage_data[stage] = state.load_stage_data(stage)\n",
    "        print(f\"   ✅ Etapa {stage}: {all_stage_data[stage]['stage_info']['stage_name']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Etapa {stage}: Erro ao carregar - {e}\")\n",
    "        raise\n",
    "\n",
    "# 🎯 PARÂMETROS DE VALIDAÇÃO\n",
    "validation_config = {\n",
    "    # Testes de busca\n",
    "    \"search_test_queries\": [\n",
    "        \"configuração de sistema\",\n",
    "        \"como instalar\",\n",
    "        \"procedimento de backup\",\n",
    "        \"troubleshooting erro\",\n",
    "        \"manual do usuário\"\n",
    "    ],\n",
    "    \"max_search_results\": 10,\n",
    "    \"min_relevance_score\": 0.1,\n",
    "    \n",
    "    # Performance\n",
    "    \"max_search_time_ms\": 5000,  # 5 segundos\n",
    "    \"concurrent_search_tests\": 3,\n",
    "    \"stress_test_queries\": 20,\n",
    "    \n",
    "    # Qualidade\n",
    "    \"min_collection_size\": 100,  # Mínimo de vetores esperado\n",
    "    \"expected_dimensions\": 1024,\n",
    "    \"check_metadata_completeness\": True,\n",
    "    \n",
    "    # Relatórios\n",
    "    \"generate_examples\": True,\n",
    "    \"detailed_analysis\": True,\n",
    "    \"export_metrics\": True\n",
    "}\n",
    "\n",
    "# Extrair configurações importantes\n",
    "qdrant_config = all_stage_data[6][\"qdrant_config\"]\n",
    "collection_status = all_stage_data[6][\"collection_status\"]\n",
    "total_embeddings = collection_status[\"vectors_count\"]\n",
    "\n",
    "print(\"\\n📊 Configuração de validação:\")\n",
    "print(f\"   🔍 Consultas de teste: {len(validation_config['search_test_queries'])}\")\n",
    "print(f\"   📊 Resultados por busca: {validation_config['max_search_results']}\")\n",
    "print(f\"   ⏱️ Tempo máximo: {validation_config['max_search_time_ms']}ms\")\n",
    "print(f\"   🧪 Testes de stress: {validation_config['stress_test_queries']}\")\n",
    "print(f\"   💾 Collection: {qdrant_config['collection_name']}\")\n",
    "print(f\"   📈 Vetores disponíveis: {total_embeddings:,}\")\n",
    "\n",
    "if total_embeddings < validation_config[\"min_collection_size\"]:\n",
    "    print(f\"   ⚠️ Aviso: Collection pequena (< {validation_config['min_collection_size']} vetores)\")\n",
    "\n",
    "print(\"\\n✅ Configuração de validação pronta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 🔍 Validação da Integridade dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 VALIDAÇÃO DA INTEGRIDADE DOS DADOS\n",
    "print(\"🔍 Validando integridade dos dados do pipeline...\")\n",
    "\n",
    "from qdrant_utils import get_collection_info\n",
    "\n",
    "integrity_results = {\n",
    "    \"pipeline_stages\": {},\n",
    "    \"data_flow\": {},\n",
    "    \"consistency_checks\": {},\n",
    "    \"overall_health\": \"unknown\"\n",
    "}\n",
    "\n",
    "# Verificar cada etapa do pipeline\n",
    "print(\"\\n📊 Verificando integridade por etapa:\")\n",
    "\n",
    "stage_names = {\n",
    "    1: \"Fundação\",\n",
    "    2: \"GitLab\", \n",
    "    3: \"Docling\",\n",
    "    4: \"Chunking\",\n",
    "    5: \"Embeddings\",\n",
    "    6: \"Qdrant\"\n",
    "}\n",
    "\n",
    "for stage_num, stage_data in all_stage_data.items():\n",
    "    stage_name = stage_names[stage_num]\n",
    "    \n",
    "    # Verificar status da etapa\n",
    "    stage_status = stage_data.get(\"stage_info\", {}).get(\"status\", \"unknown\")\n",
    "    completed_at = stage_data.get(\"stage_info\", {}).get(\"completed_at\", \"unknown\")\n",
    "    \n",
    "    integrity_results[\"pipeline_stages\"][stage_num] = {\n",
    "        \"name\": stage_name,\n",
    "        \"status\": stage_status,\n",
    "        \"completed_at\": completed_at,\n",
    "        \"has_errors\": len(stage_data.get(\"errors\", [])) > 0,\n",
    "        \"error_count\": len(stage_data.get(\"errors\", []))\n",
    "    }\n",
    "    \n",
    "    status_icon = \"✅\" if stage_status == \"success\" else \"⚠️\" if \"warning\" in stage_status else \"❌\"\n",
    "    error_info = f\" ({len(stage_data.get('errors', []))} erros)\" if stage_data.get(\"errors\") else \"\"\n",
    "    print(f\"   {status_icon} Etapa {stage_num} ({stage_name}): {stage_status}{error_info}\")\n",
    "\n",
    "# Verificar fluxo de dados entre etapas\n",
    "print(\"\\n📈 Verificando fluxo de dados:\")\n",
    "\n",
    "# GitLab -> Docling\n",
    "gitlab_files = len(all_stage_data[2].get(\"local_files\", []))\n",
    "docling_files = len(all_stage_data.get(3, {}).get(\"processed_documents\", []))\n",
    "print(f\"   📥 GitLab → Docling: {gitlab_files} → {docling_files} documentos\")\n",
    "\n",
    "# Docling -> Chunking\n",
    "chunking_files = len(all_stage_data.get(4, {}).get(\"chunk_files\", []))\n",
    "total_chunks = all_stage_data.get(4, {}).get(\"chunking_results\", {}).get(\"total_chunks_created\", 0)\n",
    "print(f\"   ⚙️ Docling → Chunks: {docling_files} → {chunking_files} arquivos ({total_chunks:,} chunks)\")\n",
    "\n",
    "# Chunking -> Embeddings\n",
    "embedding_files = len(all_stage_data.get(5, {}).get(\"embedding_files\", []))\n",
    "total_embeddings_created = all_stage_data.get(5, {}).get(\"embedding_results\", {}).get(\"total_embeddings_created\", 0)\n",
    "print(f\"   🔪 Chunks → Embeddings: {chunking_files} → {embedding_files} arquivos ({total_embeddings_created:,} embeddings)\")\n",
    "\n",
    "# Embeddings -> Qdrant\n",
    "qdrant_stored = all_stage_data.get(6, {}).get(\"collection_status\", {}).get(\"vectors_count\", 0)\n",
    "print(f\"   🧠 Embeddings → Qdrant: {total_embeddings_created:,} → {qdrant_stored:,} vetores\")\n",
    "\n",
    "# Verificar consistência de dados\n",
    "print(\"\\n🔄 Verificando consistência:\")\n",
    "\n",
    "# Taxa de retenção entre etapas\n",
    "if gitlab_files > 0:\n",
    "    docling_retention = (docling_files / gitlab_files) * 100\n",
    "    print(f\"   📊 Retenção GitLab→Docling: {docling_retention:.1f}%\")\n",
    "    \n",
    "if total_embeddings_created > 0:\n",
    "    qdrant_retention = (qdrant_stored / total_embeddings_created) * 100\n",
    "    print(f\"   📊 Retenção Embeddings→Qdrant: {qdrant_retention:.1f}%\")\n",
    "\n",
    "# Verificar estado atual da collection no Qdrant\n",
    "print(\"\\n💾 Verificando estado atual do Qdrant:\")\n",
    "current_collection_info = get_collection_info(\n",
    "    qdrant_config[\"url\"],\n",
    "    qdrant_config[\"collection_name\"],\n",
    "    all_stage_data[1][\"qdrant\"][\"api_key\"]  # API key da etapa 1\n",
    ")\n",
    "\n",
    "if current_collection_info:\n",
    "    print(f\"   📦 Collection: {current_collection_info['collection_name']}\")\n",
    "    print(f\"   📊 Vetores atuais: {current_collection_info['vectors_count']:,}\")\n",
    "    print(f\"   📄 Pontos atuais: {current_collection_info['points_count']:,}\")\n",
    "    print(f\"   📐 Dimensões: {current_collection_info['config']['vector_size']}\")\n",
    "    \n",
    "    # Verificar se dimensões estão corretas\n",
    "    expected_dims = validation_config[\"expected_dimensions\"]\n",
    "    actual_dims = current_collection_info['config']['vector_size']\n",
    "    \n",
    "    if actual_dims == expected_dims:\n",
    "        print(f\"   ✅ Dimensões corretas: {actual_dims}\")\n",
    "    else:\n",
    "        print(f\"   ❌ Dimensões incorretas: {actual_dims} (esperado: {expected_dims})\")\n",
    "else:\n",
    "    print(\"   ❌ Não foi possível verificar collection\")\n",
    "\n",
    "# Determinar saúde geral\n",
    "all_stages_ok = all(status[\"status\"] in [\"success\", \"completed_with_warnings\"] \n",
    "                   for status in integrity_results[\"pipeline_stages\"].values())\n",
    "data_flow_ok = qdrant_stored > validation_config[\"min_collection_size\"]\n",
    "qdrant_accessible = current_collection_info is not None\n",
    "\n",
    "if all_stages_ok and data_flow_ok and qdrant_accessible:\n",
    "    integrity_results[\"overall_health\"] = \"healthy\"\n",
    "    health_icon = \"✅\"\n",
    "    health_msg = \"Sistema íntegro e funcional\"\n",
    "elif all_stages_ok and qdrant_accessible:\n",
    "    integrity_results[\"overall_health\"] = \"functional\"\n",
    "    health_icon = \"⚠️\"\n",
    "    health_msg = \"Sistema funcional com avisos\"\n",
    "else:\n",
    "    integrity_results[\"overall_health\"] = \"issues\"\n",
    "    health_icon = \"❌\"\n",
    "    health_msg = \"Sistema com problemas detectados\"\n",
    "\n",
    "print(f\"\\n{health_icon} Status geral da integridade: {health_msg}\")\n",
    "print(\"\\n✅ Validação de integridade concluída!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 🔍 Testes de Busca Semântica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 TESTES DE BUSCA SEMÂNTICA\n",
    "print(\"🔍 Executando testes de busca semântica...\")\n",
    "\n",
    "from qdrant_utils import search_similar_vectors\n",
    "from embedding_utils import generate_embeddings\n",
    "import statistics\n",
    "\n",
    "search_test_results = {\n",
    "    \"query_tests\": [],\n",
    "    \"performance_metrics\": {},\n",
    "    \"quality_analysis\": {},\n",
    "    \"overall_score\": 0\n",
    "}\n",
    "\n",
    "# Configuração para busca\n",
    "embedding_model = all_stage_data[5][\"embedding_config\"][\"model_name\"]\n",
    "api_key = all_stage_data[1][\"qdrant\"][\"api_key\"]\n",
    "\n",
    "print(f\"\\n🤖 Modelo de embeddings: {embedding_model}\")\n",
    "print(f\"🔍 Executando {len(validation_config['search_test_queries'])} consultas de teste...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_search_times = []\n",
    "all_result_counts = []\n",
    "all_relevance_scores = []\n",
    "\n",
    "for i, query in enumerate(validation_config[\"search_test_queries\"], 1):\n",
    "    print(f\"\\n🔍 Teste {i}: '{query}'\")\n",
    "    \n",
    "    test_result = {\n",
    "        \"query\": query,\n",
    "        \"success\": False,\n",
    "        \"search_time_ms\": 0,\n",
    "        \"result_count\": 0,\n",
    "        \"relevance_scores\": [],\n",
    "        \"avg_relevance\": 0,\n",
    "        \"error\": None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Gerar embedding da consulta\n",
    "        print(f\"   🧠 Gerando embedding...\")\n",
    "        query_start = time.time()\n",
    "        query_embeddings = generate_embeddings([query], embedding_model)\n",
    "        embedding_time = (time.time() - query_start) * 1000\n",
    "        \n",
    "        if not query_embeddings or len(query_embeddings) == 0:\n",
    "            raise ValueError(\"Falha ao gerar embedding da consulta\")\n",
    "        \n",
    "        query_vector = query_embeddings[0]\n",
    "        print(f\"   ✅ Embedding gerado ({len(query_vector)} dims) em {embedding_time:.1f}ms\")\n",
    "        \n",
    "        # Executar busca\n",
    "        print(f\"   🔍 Executando busca...\")\n",
    "        search_start = time.time()\n",
    "        \n",
    "        search_results = search_similar_vectors(\n",
    "            qdrant_config[\"url\"],\n",
    "            qdrant_config[\"collection_name\"],\n",
    "            api_key,\n",
    "            query_vector,\n",
    "            limit=validation_config[\"max_search_results\"]\n",
    "        )\n",
    "        \n",
    "        search_time = (time.time() - search_start) * 1000\n",
    "        total_time = embedding_time + search_time\n",
    "        \n",
    "        # Analisar resultados\n",
    "        if search_results:\n",
    "            result_count = len(search_results)\n",
    "            relevance_scores = [r.get(\"score\", 0) for r in search_results]\n",
    "            avg_relevance = statistics.mean(relevance_scores) if relevance_scores else 0\n",
    "            \n",
    "            print(f\"   ✅ {result_count} resultados em {total_time:.1f}ms\")\n",
    "            print(f\"   📊 Relevância média: {avg_relevance:.3f}\")\n",
    "            \n",
    "            # Mostrar top 3 resultados\n",
    "            for j, result in enumerate(search_results[:3], 1):\n",
    "                score = result.get(\"score\", 0)\n",
    "                result_id = str(result.get(\"id\", \"unknown\"))[:20] + \"...\"\n",
    "                payload = result.get(\"payload\", {})\n",
    "                text_preview = payload.get(\"text\", \"\")[:50] + \"...\" if payload.get(\"text\") else \"sem texto\"\n",
    "                print(f\"     {j}. Score: {score:.3f} | ID: {result_id} | '{text_preview}'\")\n",
    "            \n",
    "            test_result.update({\n",
    "                \"success\": True,\n",
    "                \"search_time_ms\": total_time,\n",
    "                \"result_count\": result_count,\n",
    "                \"relevance_scores\": relevance_scores,\n",
    "                \"avg_relevance\": avg_relevance\n",
    "            })\n",
    "            \n",
    "            # Coletar métricas globais\n",
    "            all_search_times.append(total_time)\n",
    "            all_result_counts.append(result_count)\n",
    "            all_relevance_scores.extend(relevance_scores)\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ⚠️ Busca não retornou resultados\")\n",
    "            test_result[\"success\"] = True  # Tecnicamente funcionou\n",
    "            test_result[\"search_time_ms\"] = total_time\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Erro: {e}\")\n",
    "        test_result[\"error\"] = str(e)\n",
    "    \n",
    "    search_test_results[\"query_tests\"].append(test_result)\n",
    "\n",
    "# Calcular métricas de performance\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"📊 MÉTRICAS DE PERFORMANCE:\")\n",
    "\n",
    "successful_tests = [t for t in search_test_results[\"query_tests\"] if t[\"success\"]]\n",
    "failed_tests = [t for t in search_test_results[\"query_tests\"] if not t[\"success\"]]\n",
    "\n",
    "if all_search_times:\n",
    "    avg_search_time = statistics.mean(all_search_times)\n",
    "    max_search_time = max(all_search_times)\n",
    "    min_search_time = min(all_search_times)\n",
    "    \n",
    "    print(f\"   ⏱️ Tempo médio: {avg_search_time:.1f}ms\")\n",
    "    print(f\"   ⚡ Tempo mínimo: {min_search_time:.1f}ms\")\n",
    "    print(f\"   🐌 Tempo máximo: {max_search_time:.1f}ms\")\n",
    "    \n",
    "    if avg_search_time <= validation_config[\"max_search_time_ms\"]:\n",
    "        print(f\"   ✅ Performance: EXCELENTE (< {validation_config['max_search_time_ms']}ms)\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ Performance: LENTA (> {validation_config['max_search_time_ms']}ms)\")\n",
    "\n",
    "if all_relevance_scores:\n",
    "    avg_relevance = statistics.mean(all_relevance_scores)\n",
    "    max_relevance = max(all_relevance_scores)\n",
    "    min_relevance = min(all_relevance_scores)\n",
    "    \n",
    "    print(f\"   📊 Relevância média: {avg_relevance:.3f}\")\n",
    "    print(f\"   📈 Relevância máxima: {max_relevance:.3f}\")\n",
    "    print(f\"   📉 Relevância mínima: {min_relevance:.3f}\")\n",
    "\n",
    "if all_result_counts:\n",
    "    avg_results = statistics.mean(all_result_counts)\n",
    "    print(f\"   📄 Resultados médios: {avg_results:.1f}\")\n",
    "\n",
    "success_rate = (len(successful_tests) / len(search_test_results[\"query_tests\"]) * 100) if search_test_results[\"query_tests\"] else 0\n",
    "print(f\"   ✅ Taxa de sucesso: {success_rate:.1f}% ({len(successful_tests)}/{len(search_test_results['query_tests'])})\")\n",
    "\n",
    "# Armazenar métricas\n",
    "search_test_results[\"performance_metrics\"] = {\n",
    "    \"avg_search_time_ms\": statistics.mean(all_search_times) if all_search_times else 0,\n",
    "    \"max_search_time_ms\": max(all_search_times) if all_search_times else 0,\n",
    "    \"min_search_time_ms\": min(all_search_times) if all_search_times else 0,\n",
    "    \"avg_relevance_score\": statistics.mean(all_relevance_scores) if all_relevance_scores else 0,\n",
    "    \"avg_results_per_query\": statistics.mean(all_result_counts) if all_result_counts else 0,\n",
    "    \"success_rate_percent\": success_rate\n",
    "}\n",
    "\n",
    "# Calcular score geral\n",
    "performance_score = 100 if avg_search_time <= validation_config[\"max_search_time_ms\"] else max(0, 100 - (avg_search_time - validation_config[\"max_search_time_ms\"]) / 100)\n",
    "relevance_score = (avg_relevance * 100) if all_relevance_scores else 0\n",
    "overall_score = (success_rate + performance_score + relevance_score) / 3\n",
    "\n",
    "search_test_results[\"overall_score\"] = overall_score\n",
    "\n",
    "print(f\"\\n🏆 Score geral de busca: {overall_score:.1f}/100\")\n",
    "print(\"\\n✅ Testes de busca semântica concluídos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## ⚡ Testes de Performance e Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚡ TESTES DE PERFORMANCE E STRESS\n",
    "print(\"⚡ Executando testes de performance e stress...\")\n",
    "\n",
    "import concurrent.futures\n",
    "import random\n",
    "\n",
    "stress_test_results = {\n",
    "    \"concurrent_tests\": [],\n",
    "    \"stress_tests\": [],\n",
    "    \"throughput_metrics\": {},\n",
    "    \"stability_score\": 0\n",
    "}\n",
    "\n",
    "# Teste de busca concorrente\n",
    "print(\"\\n🔄 Teste de busca concorrente...\")\n",
    "\n",
    "def concurrent_search_test(query_text, test_id):\n",
    "    \"\"\"Executa uma busca e mede o tempo\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Gerar embedding\n",
    "        query_embeddings = generate_embeddings([query_text], embedding_model)\n",
    "        if not query_embeddings:\n",
    "            return {\"success\": False, \"error\": \"Falha no embedding\", \"test_id\": test_id}\n",
    "        \n",
    "        # Buscar\n",
    "        results = search_similar_vectors(\n",
    "            qdrant_config[\"url\"],\n",
    "            qdrant_config[\"collection_name\"],\n",
    "            api_key,\n",
    "            query_embeddings[0],\n",
    "            limit=5\n",
    "        )\n",
    "        \n",
    "        total_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"test_id\": test_id,\n",
    "            \"query\": query_text,\n",
    "            \"time_ms\": total_time,\n",
    "            \"result_count\": len(results) if results else 0\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"test_id\": test_id,\n",
    "            \"query\": query_text,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Executar testes concorrentes\n",
    "concurrent_queries = validation_config[\"search_test_queries\"] * validation_config[\"concurrent_search_tests\"]\n",
    "print(f\"   🚀 Executando {len(concurrent_queries)} buscas simultâneas...\")\n",
    "\n",
    "concurrent_start_time = time.time()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=validation_config[\"concurrent_search_tests\"]) as executor:\n",
    "    future_to_query = {\n",
    "        executor.submit(concurrent_search_test, query, i): (query, i) \n",
    "        for i, query in enumerate(concurrent_queries)\n",
    "    }\n",
    "    \n",
    "    concurrent_results = []\n",
    "    for future in concurrent.futures.as_completed(future_to_query):\n",
    "        result = future.result()\n",
    "        concurrent_results.append(result)\n",
    "\n",
    "concurrent_total_time = (time.time() - concurrent_start_time) * 1000\n",
    "\n",
    "# Analisar resultados concorrentes\n",
    "successful_concurrent = [r for r in concurrent_results if r[\"success\"]]\n",
    "failed_concurrent = [r for r in concurrent_results if not r[\"success\"]]\n",
    "\n",
    "if successful_concurrent:\n",
    "    concurrent_times = [r[\"time_ms\"] for r in successful_concurrent]\n",
    "    avg_concurrent_time = statistics.mean(concurrent_times)\n",
    "    total_throughput = len(successful_concurrent) / (concurrent_total_time / 1000)  # queries/second\n",
    "    \n",
    "    print(f\"   ✅ Sucessos: {len(successful_concurrent)}/{len(concurrent_results)}\")\n",
    "    print(f\"   ⏱️ Tempo médio: {avg_concurrent_time:.1f}ms\")\n",
    "    print(f\"   🚀 Throughput: {total_throughput:.1f} consultas/segundo\")\n",
    "    print(f\"   🕒 Tempo total: {concurrent_total_time:.1f}ms\")\nelse:\n",
    "    print(f\"   ❌ Todos os testes concorrentes falharam\")\n\n",
    "# Teste de stress com muitas consultas\n",
    "print(f\"\\n🧪 Teste de stress ({validation_config['stress_test_queries']} consultas)...\")\n",
    "\n",
    "stress_queries = []\n",
    "base_words = [\"sistema\", \"configuração\", \"manual\", \"erro\", \"instalação\", \"backup\", \"usuário\", \"rede\", \"servidor\", \"dados\"]\n",
    "for i in range(validation_config[\"stress_test_queries\"]):\n",
    "    # Gerar consultas variadas\n",
    "    query_words = random.sample(base_words, random.randint(1, 3))\n",
    "    stress_queries.append(\" \".join(query_words))\n",
    "\n",
    "stress_start_time = time.time()\n",
    "stress_results = []\n",
    "\n",
    "for i, query in enumerate(stress_queries):\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"   🔄 Progresso: {i + 1}/{len(stress_queries)}\")\n",
    "    \n",
    "    result = concurrent_search_test(query, f\"stress_{i}\")\n",
    "    stress_results.append(result)\n",
    "\n",
    "stress_total_time = (time.time() - stress_start_time) * 1000\n",
    "\n",
    "# Analisar resultados de stress\n",
    "successful_stress = [r for r in stress_results if r[\"success\"]]\n",
    "failed_stress = [r for r in stress_results if not r[\"success\"]]\n",
    "\n",
    "if successful_stress:\n",
    "    stress_times = [r[\"time_ms\"] for r in successful_stress]\n",
    "    avg_stress_time = statistics.mean(stress_times)\n",
    "    stress_throughput = len(successful_stress) / (stress_total_time / 1000)\n",
    "    \n",
    "    print(f\"   ✅ Sucessos: {len(successful_stress)}/{len(stress_results)}\")\n",
    "    print(f\"   ⏱️ Tempo médio: {avg_stress_time:.1f}ms\")\n",
    "    print(f\"   🚀 Throughput: {stress_throughput:.1f} consultas/segundo\")\n",
    "    print(f\"   🕒 Tempo total: {stress_total_time:.1f}ms\")\n",
    "\n",
    "# Calcular score de estabilidade\n",
    "concurrent_success_rate = (len(successful_concurrent) / len(concurrent_results) * 100) if concurrent_results else 0\n",
    "stress_success_rate = (len(successful_stress) / len(stress_results) * 100) if stress_results else 0\n",
    "avg_stability = (concurrent_success_rate + stress_success_rate) / 2\n",
    "\n",
    "stress_test_results.update({\n",
    "    \"concurrent_tests\": concurrent_results,\n",
    "    \"stress_tests\": stress_results,\n",
    "    \"throughput_metrics\": {\n",
    "        \"concurrent_throughput_qps\": total_throughput if successful_concurrent else 0,\n",
    "        \"stress_throughput_qps\": stress_throughput if successful_stress else 0,\n",
    "        \"concurrent_success_rate\": concurrent_success_rate,\n",
    "        \"stress_success_rate\": stress_success_rate\n",
    "    },\n",
    "    \"stability_score\": avg_stability\n",
    "})\n",
    "\n",
    "print(f\"\\n🏆 Score de estabilidade: {avg_stability:.1f}%\")\n",
    "print(\"\\n✅ Testes de performance concluídos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 📊 Relatório Final de Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 GERAÇÃO DO RELATÓRIO FINAL DE VALIDAÇÃO\n",
    "print(\"📊 Gerando relatório final de validação...\")\n",
    "\n",
    "from pipeline_utils import get_timestamp\n",
    "\n",
    "# Compilar métricas finais\n",
    "final_validation_report = {\n",
    "    \"validation_info\": {\n",
    "        \"stage_number\": 7,\n",
    "        \"stage_name\": \"validation\",\n",
    "        \"completed_at\": get_timestamp(),\n",
    "        \"pipeline_version\": \"NIC ETL v1.0\",\n",
    "        \"validation_duration_minutes\": 0  # Será calculado no final\n",
    "    },\n",
    "    \n",
    "    \"pipeline_summary\": {\n",
    "        \"total_stages\": 7,\n",
    "        \"completed_stages\": len([s for s in integrity_results[\"pipeline_stages\"].values() if s[\"status\"] in [\"success\", \"completed_with_warnings\"]]),\n",
    "        \"total_documents_processed\": gitlab_files,\n",
    "        \"total_chunks_created\": total_chunks,\n",
    "        \"total_embeddings_generated\": total_embeddings_created,\n",
    "        \"total_vectors_stored\": qdrant_stored,\n",
    "        \"overall_pipeline_health\": integrity_results[\"overall_health\"]\n",
    "    },\n",
    "    \n",
    "    \"integrity_results\": integrity_results,\n",
    "    \"search_test_results\": search_test_results,\n",
    "    \"stress_test_results\": stress_test_results,\n",
    "    \n",
    "    \"quality_metrics\": {\n",
    "        \"data_integrity_score\": 100 if integrity_results[\"overall_health\"] == \"healthy\" else 75 if integrity_results[\"overall_health\"] == \"functional\" else 25,\n",
    "        \"search_quality_score\": search_test_results[\"overall_score\"],\n",
    "        \"performance_stability_score\": stress_test_results[\"stability_score\"],\n",
    "        \"overall_system_score\": 0  # Será calculado\n",
    "    },\n",
    "    \n",
    "    \"recommendations\": [],\n",
    "    \"next_steps\": []\n",
    "}\n",
    "\n",
    "# Calcular score geral do sistema\n",
    "quality_metrics = final_validation_report[\"quality_metrics\"]\n",
    "overall_system_score = (\n",
    "    quality_metrics[\"data_integrity_score\"] * 0.3 +\n",
    "    quality_metrics[\"search_quality_score\"] * 0.4 +\n",
    "    quality_metrics[\"performance_stability_score\"] * 0.3\n",
    ")\n",
    "quality_metrics[\"overall_system_score\"] = overall_system_score\n",
    "\n",
    "# Gerar recomendações baseadas nos resultados\n",
    "recommendations = []\n",
    "next_steps = []\n",
    "\n",
    "if integrity_results[\"overall_health\"] == \"healthy\":\n",
    "    recommendations.append(\"✅ Sistema está funcionando corretamente\")\n",
    "    next_steps.append(\"Sistema pronto para uso em produção\")\n",
    "elif integrity_results[\"overall_health\"] == \"functional\":\n",
    "    recommendations.append(\"⚠️ Sistema funcional mas com avisos - revisar logs\")\n",
    "    next_steps.append(\"Investigar avisos antes de usar em produção\")\n",
    "else:\n",
    "    recommendations.append(\"❌ Sistema com problemas - necessário correção\")\n",
    "    next_steps.append(\"Corrigir problemas identificados antes de continuar\")\n",
    "\n",
    "if search_test_results[\"overall_score\"] >= 80:\n",
    "    recommendations.append(\"✅ Qualidade de busca excelente\")\n",
    "elif search_test_results[\"overall_score\"] >= 60:\n",
    "    recommendations.append(\"⚠️ Qualidade de busca adequada mas pode melhorar\")\n",
    "    next_steps.append(\"Considerar fine-tuning do modelo de embeddings\")\n",
    "else:\n",
    "    recommendations.append(\"❌ Qualidade de busca baixa - revisar configurações\")\n",
    "    next_steps.append(\"Revisar modelo de embeddings e parâmetros de busca\")\n",
    "\n",
    "if stress_test_results[\"stability_score\"] >= 95:\n",
    "    recommendations.append(\"✅ Excelente estabilidade sob carga\")\n",
    "elif stress_test_results[\"stability_score\"] >= 80:\n",
    "    recommendations.append(\"⚠️ Boa estabilidade com algumas falhas\")\n",
    "    next_steps.append(\"Monitorar performance em produção\")\n",
    "else:\n",
    "    recommendations.append(\"❌ Problemas de estabilidade sob carga\")\n",
    "    next_steps.append(\"Otimizar performance e configurações de timeout\")\n",
    "\n",
    "if qdrant_stored < validation_config[\"min_collection_size\"]:\n",
    "    recommendations.append(f\"⚠️ Collection pequena ({qdrant_stored} vetores) - adicionar mais documentos\")\n",
    "    next_steps.append(\"Processar mais documentos para melhorar cobertura\")\n",
    "\n",
    "final_validation_report[\"recommendations\"] = recommendations\n",
    "final_validation_report[\"next_steps\"] = next_steps\n",
    "\n",
    "# Exibir relatório resumido\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 RELATÓRIO FINAL DE VALIDAÇÃO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n📊 RESUMO DO PIPELINE:\")\n",
    "print(f\"   📄 Documentos processados: {gitlab_files}\")\n",
    "print(f\"   🔪 Chunks criados: {total_chunks:,}\")\n",
    "print(f\"   🧠 Embeddings gerados: {total_embeddings_created:,}\")\n",
    "print(f\"   💾 Vetores armazenados: {qdrant_stored:,}\")\n",
    "print(f\"   🏥 Saúde do pipeline: {integrity_results['overall_health'].upper()}\")\n",
    "\n",
    "print(f\"\\n🏆 SCORES DE QUALIDADE:\")\n",
    "print(f\"   🔧 Integridade dos dados: {quality_metrics['data_integrity_score']:.1f}/100\")\n",
    "print(f\"   🔍 Qualidade de busca: {quality_metrics['search_quality_score']:.1f}/100\")\n",
    "print(f\"   ⚡ Estabilidade/Performance: {quality_metrics['performance_stability_score']:.1f}/100\")\n",
    "print(f\"   🎯 SCORE GERAL: {quality_metrics['overall_system_score']:.1f}/100\")\n",
    "\n",
    "print(f\"\\n💡 RECOMENDAÇÕES:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "print(f\"\\n🚀 PRÓXIMOS PASSOS:\")\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"   {i}. {step}\")\n",
    "\n",
    "# Determinar status geral\n",
    "if overall_system_score >= 85:\n",
    "    final_status = \"🎉 SISTEMA EXCELENTE - PRONTO PARA PRODUÇÃO\"\n",
    "    status_icon = \"✅\"\n",
    "elif overall_system_score >= 70:\n",
    "    final_status = \"✅ SISTEMA BOM - PODE SER USADO COM MONITORAMENTO\"\n",
    "    status_icon = \"⚠️\"\n",
    "elif overall_system_score >= 50:\n",
    "    final_status = \"⚠️ SISTEMA ADEQUADO - REQUER MELHORIAS\"\n",
    "    status_icon = \"⚠️\"\n",
    "else:\n",
    "    final_status = \"❌ SISTEMA PRECISA DE CORREÇÕES\"\n",
    "    status_icon = \"❌\"\n",
    "\n",
    "print(f\"\\n{status_icon} STATUS FINAL: {final_status}\")\n",
    "\n",
    "# Atualizar status no relatório\n",
    "final_validation_report[\"validation_info\"][\"final_status\"] = final_status\n",
    "final_validation_report[\"validation_info\"][\"status_icon\"] = status_icon\n",
    "\n",
    "print(\"\\n✅ Relatório final gerado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 💾 Salvamento dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 SALVAMENTO DOS RESULTADOS FINAIS\n",
    "print(\"💾 Salvando resultados da validação...\")\n",
    "\n",
    "# Calcular duração total da validação\n",
    "validation_end_time = datetime.now()\n",
    "# Assumindo que começou no início do notebook\n",
    "validation_start_str = final_validation_report[\"validation_info\"][\"completed_at\"]\n",
    "validation_start_time = datetime.fromisoformat(validation_start_str.replace(\"Z\", \"+00:00\"))\n",
    "validation_duration = (validation_end_time - validation_start_time).total_seconds() / 60\n",
    "\n",
    "final_validation_report[\"validation_info\"][\"validation_duration_minutes\"] = validation_duration\n",
    "\n",
    "# Preparar dados de saída\n",
    "stage_output = {\n",
    "    \"stage_info\": {\n",
    "        \"stage_number\": 7,\n",
    "        \"stage_name\": \"validation\",\n",
    "        \"completed_at\": get_timestamp(),\n",
    "        \"status\": \"success\" if overall_system_score >= 70 else \"completed_with_warnings\",\n",
    "        \"validation_duration_minutes\": validation_duration\n",
    "    },\n",
    "    \n",
    "    \"validation_summary\": {\n",
    "        \"overall_system_score\": overall_system_score,\n",
    "        \"final_status\": final_status,\n",
    "        \"pipeline_health\": integrity_results[\"overall_health\"],\n",
    "        \"search_functional\": search_test_results[\"overall_score\"] > 50,\n",
    "        \"performance_acceptable\": stress_test_results[\"stability_score\"] > 80\n",
    "    },\n",
    "    \n",
    "    \"detailed_results\": final_validation_report,\n",
    "    \n",
    "    \"pipeline_completion\": {\n",
    "        \"all_stages_completed\": True,\n",
    "        \"total_documents_processed\": gitlab_files,\n",
    "        \"total_vectors_in_qdrant\": qdrant_stored,\n",
    "        \"search_system_ready\": True,\n",
    "        \"production_ready\": overall_system_score >= 70\n",
    "    },\n",
    "    \n",
    "    \"recommendations\": recommendations,\n",
    "    \"next_steps\": next_steps\n",
    "}\n",
    "\n",
    "# Salvar usando PipelineState\n",
    "state = PipelineState()\n",
    "state.save_stage_data(7, stage_output)\n",
    "\n",
    "# Marcar etapa como concluída\n",
    "state.mark_stage_completed(7)\n",
    "\n",
    "print(f\"✅ Resultados salvos: {state.metadata_dir / 'stage_07_validation.json'}\")\n",
    "print(f\"✅ Checkpoint criado: {state.checkpoints_dir / 'stage_07_completed.lock'}\")\n",
    "\n",
    "# Salvar relatório detalhado de validação\n",
    "validation_report_path = Path(\"../pipeline_data/metadata\") / \"final_validation_report.json\"\n",
    "with open(validation_report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_validation_report, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"📝 Relatório detalhado salvo: {validation_report_path}\")\n",
    "\n",
    "# Criar arquivo de resumo executivo\n",
    "executive_summary = {\n",
    "    \"pipeline_name\": \"NIC ETL Pipeline\",\n",
    "    \"completion_date\": get_timestamp(),\n",
    "    \"overall_score\": f\"{overall_system_score:.1f}/100\",\n",
    "    \"status\": final_status,\n",
    "    \"key_metrics\": {\n",
    "        \"documents_processed\": gitlab_files,\n",
    "        \"vectors_stored\": qdrant_stored,\n",
    "        \"search_quality\": f\"{search_test_results['overall_score']:.1f}/100\",\n",
    "        \"system_stability\": f\"{stress_test_results['stability_score']:.1f}%\"\n",
    "    },\n",
    "    \"production_ready\": overall_system_score >= 70,\n",
    "    \"recommendations_count\": len(recommendations)\n",
    "}\n",
    "\n",
    "executive_summary_path = Path(\"../pipeline_data\") / \"EXECUTIVE_SUMMARY.json\"\n",
    "with open(executive_summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(executive_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"🎯 Resumo executivo salvo: {executive_summary_path}\")\n",
    "\n",
    "# Exibir resumo final\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 ETAPA 7 CONCLUÍDA - VALIDAÇÃO COMPLETA!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"📊 Resumo da Validação:\")\n",
    "print(f\"   🕒 Duração: {validation_duration:.1f} minutos\")\n",
    "print(f\"   🔍 Testes de busca: {len(search_test_results['query_tests'])}\")\n",
    "print(f\"   ⚡ Testes de stress: {len(stress_test_results['stress_tests'])}\")\n",
    "print(f\"   🏆 Score final: {overall_system_score:.1f}/100\")\n",
    "print(f\"   📋 Recomendações: {len(recommendations)}\")\n",
    "\n",
    "print(f\"\\n{status_icon} {final_status}\")\n",
    "\n",
    "if overall_system_score >= 70:\n",
    "    print(f\"\\n🚀 Sistema validado e pronto para uso!\")\n",
    "    print(f\"📍 Collection Qdrant: {qdrant_config['collection_name']}\")\n",
    "    print(f\"💾 Vetores disponíveis: {qdrant_stored:,}\")\n",
    "    print(f\"🔍 Busca semântica funcional\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Sistema requer atenção antes do uso em produção\")\n",
    "    print(f\"📋 Consulte as recomendações no relatório detalhado\")\n",
    "\n",
    "# Exibir progresso final do pipeline\n",
    "print(\"\\n📈 Progresso Final do Pipeline:\")\n",
    "show_pipeline_progress()\n",
    "\n",
    "print(\"\\n🎊 PIPELINE NIC ETL CONCLUÍDO COM SUCESSO!\")\n",
    "print(\"📊 Todos os arquivos de relatório estão disponíveis em pipeline_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ **Pipeline NIC ETL Concluído!**\n",
    "\n",
    "### 🎯 **O que foi validado:**\n",
    "- ✅ Integridade completa do pipeline (7 etapas)\n",
    "- ✅ Qualidade da busca semântica\n",
    "- ✅ Performance e estabilidade do sistema\n",
    "- ✅ Funcionalidade do Qdrant\n",
    "- ✅ Consistência dos dados\n",
    "- ✅ Métricas de produção\n",
    "\n",
    "### 📊 **Relatórios gerados:**\n",
    "- `pipeline_data/metadata/stage_07_validation.json` - Resultados técnicos completos\n",
    "- `pipeline_data/metadata/final_validation_report.json` - Relatório detalhado de validação\n",
    "- `pipeline_data/EXECUTIVE_SUMMARY.json` - Resumo executivo para gestores\n",
    "- `pipeline_data/checkpoints/stage_07_completed.lock` - Checkpoint final\n",
    "\n",
    "### 🏆 **Sistema pronto para:**\n",
    "- **Busca Semântica:** Consultas em linguagem natural\n",
    "- **Filtragem por Metadados:** Documentos, chunks, origem\n",
    "- **Integração via API:** Qdrant REST/gRPC\n",
    "- **Monitoramento:** Métricas de performance coletadas\n",
    "\n",
    "### 🔧 **Para usar o sistema:**\n",
    "1. **Acesse o Qdrant:** `{qdrant_url}`\n",
    "2. **Collection:** `{collection_name}`\n",
    "3. **Use a API de busca:** Vetores de 1024 dimensões (BAAI/bge-m3)\n",
    "4. **Monitore performance:** Baseado nos benchmarks gerados\n",
    "\n",
    "### 🎊 **Parabéns!**\n",
    "O pipeline NIC ETL foi executado com sucesso e está pronto para uso em produção.\n",
    "\n",
    "---\n",
    "\n",
    "**📊 Sistema de busca semântica NIC operacional! Base de conhecimento digitalizada e pesquisável.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}