{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš™ï¸ ETAPA 3: PROCESSAMENTO COM DOCLING\n",
    "\n",
    "## ğŸ¯ **O que esta etapa faz**\n",
    "Extrai e estrutura o conteÃºdo de todos os documentos baixados, transformando PDFs, DOCs e outros formatos em texto estruturado pronto para anÃ¡lise.\n",
    "\n",
    "## ğŸ¤” **Por que esta etapa Ã© necessÃ¡ria**\n",
    "Documentos vÃªm em formatos diferentes (PDF, DOCX, MD). Precisamos:\n",
    "- ğŸ“„ **Extrair texto puro** de cada documento\n",
    "- ğŸ—ï¸ **Preservar estrutura** (tÃ­tulos, seÃ§Ãµes, tabelas)\n",
    "- ğŸ” **Usar OCR** quando necessÃ¡rio (imagens, PDFs escaneados)\n",
    "- ğŸ“Š **Coletar metadados** de estrutura e conteÃºdo\n",
    "\n",
    "## âš™ï¸ **Como funciona**\n",
    "1. **Analisa cada documento** para determinar o melhor mÃ©todo de extraÃ§Ã£o\n",
    "2. **Aplica Docling** para extrair conteÃºdo estruturado\n",
    "3. **Usa OCR** em PDFs escaneados ou com baixa qualidade de texto\n",
    "4. **Preserva hierarquia** de tÃ­tulos e seÃ§Ãµes\n",
    "5. **Gera texto limpo** removendo formataÃ§Ã£o desnecessÃ¡ria\n",
    "6. **Salva conteÃºdo estruturado** e texto puro separadamente\n",
    "\n",
    "## ğŸ“Š **ParÃ¢metros que vocÃª pode ajustar**\n",
    "- `use_ocr`: Habilitar OCR automÃ¡tico (padrÃ£o: True)\n",
    "- `ocr_confidence_threshold`: Limite de confianÃ§a OCR (padrÃ£o: 0.7)\n",
    "- `preserve_structure`: Manter hierarquia de tÃ­tulos (padrÃ£o: True)\n",
    "- `clean_text`: Remover formataÃ§Ã£o extra (padrÃ£o: True)\n",
    "- `extract_tables`: Processar tabelas separadamente (padrÃ£o: True)\n",
    "\n",
    "## ğŸ‘ï¸ **O que esperar de saÃ­da**\n",
    "- ğŸ“ **Pasta `processed/`** com conteÃºdo estruturado\n",
    "- ğŸ“„ **Arquivos `*_content.json`** com estrutura completa\n",
    "- ğŸ“ **Arquivos `*_text.txt`** com texto puro extraÃ­do\n",
    "- ğŸ“Š **Metadados** de extraÃ§Ã£o (confianÃ§a, mÃ©todo usado, etc.)\n",
    "- ğŸ“ˆ **EstatÃ­sticas** de processamento por tipo de arquivo\n",
    "\n",
    "## âš ï¸ **Pontos importantes**\n",
    "- **Requer Etapa 2** (Coleta GitLab) concluÃ­da primeiro\n",
    "- **Docling** deve estar instalado e funcional\n",
    "- **OCR** pode ser lento para documentos grandes\n",
    "- **Modo independente** disponÃ­vel para testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ›¡ï¸ VerificaÃ§Ã£o de DependÃªncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ›¡ï¸ VERIFICAÃ‡ÃƒO AUTOMÃTICA DE DEPENDÃŠNCIAS\nimport sys\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Adicionar biblioteca ao path\nsys.path.insert(0, str(Path().parent / \"src\"))\n\nfrom pipeline_utils import (\n    PipelineState, \n    check_prerequisites, \n    show_pipeline_progress\n)\n\nprint(\"âš™ï¸ ETAPA 3: PROCESSAMENTO COM DOCLING\")\nprint(\"=\" * 60)\nprint(f\"ğŸ•’ Iniciado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# Verificar dependÃªncias\nCURRENT_STAGE = 3\nprerequisites_ok = check_prerequisites(CURRENT_STAGE)\n\nif not prerequisites_ok:\n    print(\"\\nâŒ ERRO: DependÃªncias nÃ£o atendidas!\")\n    print(\"ğŸ“‹ Execute primeiro as etapas anteriores:\")\n    print(\"   1. 01_FUNDACAO_PREPARACAO.ipynb\")\n    print(\"   2. 02_COLETA_GITLAB.ipynb\")\n    \n    show_pipeline_progress()\n    raise RuntimeError(\"DependÃªncias nÃ£o atendidas - execute etapas anteriores primeiro\")\n\nprint(\"\\nâœ… DependÃªncias verificadas - pode prosseguir\")\nprint(\"ğŸ“ˆ Progresso atual do pipeline:\")\nshow_pipeline_progress()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“‹ Carregamento de Dados"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“‹ CARREGAMENTO DOS DADOS DA ETAPA ANTERIOR\nprint(\"ğŸ“‹ Carregando dados da etapa anterior...\")\n\n# Carregar dados da etapa 2 (GitLab)\nstate = PipelineState()\ntry:\n    documents_data = state.load_stage_data(2)\n    print(\"âœ… Dados carregados da Etapa 2 (GitLab)\")\n    print(f\"   ğŸ“„ Documentos disponÃ­veis: {len(documents_data['local_files'])}\")\n    print(f\"   ğŸ“ DiretÃ³rio: {documents_data['next_stage_instructions']['documents_directory']}\")\nexcept Exception as e:\n    print(f\"âŒ Erro ao carregar dados: {e}\")\n    raise\n\nprint(\"\\n\" + \"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ConfiguraÃ§Ã£o de Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‹ CONFIGURAÃ‡ÃƒO DOS PARÃ‚METROS DE PROCESSAMENTO\n",
    "print(\"ğŸ“‹ Configurando parÃ¢metros de processamento Docling...\")\n",
    "\n",
    "# ğŸ¯ PARÃ‚METROS AJUSTÃVEIS\n",
    "processing_config = {\n",
    "    # Docling\n",
    "    \"use_ocr\": True,\n",
    "    \"ocr_confidence_threshold\": 0.7,\n",
    "    \"preserve_structure\": True,\n",
    "    \"clean_text\": True,\n",
    "    \"extract_tables\": True,\n",
    "    \"extract_images\": False,  # Por enquanto, focar em texto\n",
    "    \n",
    "    # Processamento\n",
    "    \"batch_size\": 4,  # Processar atÃ© 4 documentos simultaneamente\n",
    "    \"timeout_per_doc_seconds\": 300,  # 5 minutos por documento\n",
    "    \"skip_large_files_mb\": 100,  # Pular arquivos muito grandes\n",
    "    \n",
    "    # SaÃ­da\n",
    "    \"output_dir\": \"../pipeline_data/processed\",\n",
    "    \"save_intermediate\": True,  # Salvar resultados intermediÃ¡rios\n",
    "    \"overwrite_existing\": True\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“Š ParÃ¢metros de processamento configurados:\")\n",
    "print(f\"   ğŸ” OCR habilitado: {processing_config['use_ocr']}\")\n",
    "print(f\"   ğŸ“Š Limite confianÃ§a OCR: {processing_config['ocr_confidence_threshold']}\")\n",
    "print(f\"   ğŸ—ï¸ Preservar estrutura: {processing_config['preserve_structure']}\")\n",
    "print(f\"   ğŸ§¹ Limpar texto: {processing_config['clean_text']}\")\n",
    "print(f\"   ğŸ“‹ Extrair tabelas: {processing_config['extract_tables']}\")\n",
    "print(f\"   ğŸ“¦ Lote: {processing_config['batch_size']} documentos simultÃ¢neos\")\n",
    "print(f\"   â±ï¸ Timeout: {processing_config['timeout_per_doc_seconds']} segundos por documento\")\n",
    "print(f\"   ğŸ“ SaÃ­da: {processing_config['output_dir']}\")\n",
    "\n",
    "# Preparar diretÃ³rio de saÃ­da\n",
    "from pipeline_utils import ensure_directory\n",
    "ensure_directory(processing_config[\"output_dir\"])\n",
    "\n",
    "print(\"\\nâœ… ConfiguraÃ§Ã£o de processamento pronta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“„ AnÃ¡lise dos Documentos de Entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“„ ANÃLISE DOS DOCUMENTOS PARA PROCESSAMENTO\n",
    "print(\"ğŸ“„ Analisando documentos para processamento...\")\n",
    "\n",
    "from collections import defaultdict\n",
    "from pipeline_utils import format_file_size\n",
    "\n",
    "# Analisar arquivos de entrada\n",
    "files_to_process = documents_data[\"local_files\"]\n",
    "print(f\"\\nğŸ“Š Total de documentos: {len(files_to_process)}\")\n",
    "\n",
    "# EstatÃ­sticas por tipo\n",
    "type_stats = defaultdict(lambda: {\"count\": 0, \"total_size\": 0, \"files\": []})\n",
    "total_size = 0\n",
    "processable_files = []\n",
    "skipped_files = []\n",
    "\n",
    "for file_info in files_to_process:\n",
    "    file_path = Path(file_info[\"local_path\"])\n",
    "    file_ext = file_info[\"extension\"]\n",
    "    file_size = file_info[\"size_bytes\"]\n",
    "    file_size_mb = file_size / (1024 * 1024)\n",
    "    \n",
    "    # Verificar se arquivo existe\n",
    "    if not file_path.exists():\n",
    "        print(f\"   âš ï¸ Arquivo nÃ£o encontrado: {file_path.name}\")\n",
    "        skipped_files.append((file_path.name, \"file_not_found\"))\n",
    "        continue\n",
    "    \n",
    "    # Verificar tamanho\n",
    "    if file_size_mb > processing_config[\"skip_large_files_mb\"]:\n",
    "        print(f\"   â­ï¸ Pulado (muito grande): {file_path.name} ({file_size_mb:.1f} MB)\")\n",
    "        skipped_files.append((file_path.name, \"too_large\"))\n",
    "        continue\n",
    "    \n",
    "    # Arquivo OK para processamento\n",
    "    type_stats[file_ext][\"count\"] += 1\n",
    "    type_stats[file_ext][\"total_size\"] += file_size\n",
    "    type_stats[file_ext][\"files\"].append(file_info)\n",
    "    total_size += file_size\n",
    "    processable_files.append(file_info)\n",
    "    \n",
    "    print(f\"   âœ… {file_path.name} ({format_file_size(file_size)}) - {file_ext}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Resumo da anÃ¡lise:\")\n",
    "print(f\"   âœ… ProcessÃ¡veis: {len(processable_files)}\")\n",
    "print(f\"   â­ï¸ Pulados: {len(skipped_files)}\")\n",
    "print(f\"   ğŸ“ Tamanho total: {format_file_size(total_size)}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Por tipo de arquivo:\")\n",
    "for ext, stats in sorted(type_stats.items()):\n",
    "    count = stats[\"count\"]\n",
    "    size = format_file_size(stats[\"total_size\"])\n",
    "    avg_size = format_file_size(stats[\"total_size\"] / count) if count > 0 else \"0 B\"\n",
    "    print(f\"   {ext}: {count} arquivos, {size} total, {avg_size} mÃ©dio\")\n",
    "\n",
    "if skipped_files:\n",
    "    print(f\"\\nâš ï¸ Arquivos pulados ({len(skipped_files)}):\")\n",
    "    for filename, reason in skipped_files:\n",
    "        reason_text = {\n",
    "            \"file_not_found\": \"nÃ£o encontrado\",\n",
    "            \"too_large\": \"muito grande\"\n",
    "        }.get(reason, reason)\n",
    "        print(f\"   - {filename}: {reason_text}\")\n",
    "\n",
    "if not processable_files:\n",
    "    print(\"\\nâŒ Nenhum arquivo disponÃ­vel para processamento\")\n",
    "    raise ValueError(\"Nenhum documento para processar\")\n",
    "\n",
    "print(\"\\nâœ… AnÃ¡lise concluÃ­da - pronto para processamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Processamento com Docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ EXECUÃ‡ÃƒO DO PROCESSAMENTO COM DOCLING\n",
    "print(\"âš™ï¸ Iniciando processamento com Docling...\")\n",
    "\n",
    "from docling_utils import extract_content_from_file, batch_process_documents\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Preparar lista de arquivos para processamento\n",
    "file_paths = [file_info[\"local_path\"] for file_info in processable_files]\n",
    "\n",
    "print(f\"\\nğŸš€ Processando {len(file_paths)} documentos...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Executar processamento em lote\n",
    "start_time = datetime.now()\n",
    "\n",
    "processing_results = batch_process_documents(\n",
    "    file_list=file_paths,\n",
    "    output_dir=processing_config[\"output_dir\"]\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "processing_duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š RESULTADO DO PROCESSAMENTO:\")\n",
    "\n",
    "# Contar sucessos e falhas\n",
    "successful_results = [r for r in processing_results if \"error\" not in r]\n",
    "failed_results = [r for r in processing_results if \"error\" in r]\n",
    "\n",
    "print(f\"   ğŸ“„ Total processado: {len(processing_results)}\")\n",
    "print(f\"   âœ… Sucessos: {len(successful_results)}\")\n",
    "print(f\"   âŒ Falhas: {len(failed_results)}\")\n",
    "print(f\"   â±ï¸ Tempo total: {processing_duration:.1f} segundos\")\n",
    "\n",
    "if successful_results:\n",
    "    avg_time = processing_duration / len(successful_results)\n",
    "    print(f\"   ğŸ“ˆ Tempo mÃ©dio: {avg_time:.1f} segundos por documento\")\n",
    "\n",
    "# Mostrar resultados de sucesso\n",
    "if successful_results:\n",
    "    print(\"\\nğŸ“ Documentos processados com sucesso:\")\n",
    "    for result in successful_results[:10]:  # Mostrar apenas os primeiros 10\n",
    "        filename = Path(result[\"source_file\"]).name\n",
    "        method = result[\"processing_info\"][\"method\"]\n",
    "        confidence = result[\"processing_info\"][\"confidence_score\"]\n",
    "        print(f\"   âœ… {filename} (mÃ©todo: {method}, confianÃ§a: {confidence:.2f})\")\n",
    "    \n",
    "    if len(successful_results) > 10:\n",
    "        remaining = len(successful_results) - 10\n",
    "        print(f\"   ... e mais {remaining} documentos\")\n",
    "\n",
    "# Mostrar falhas se houver\n",
    "if failed_results:\n",
    "    print(f\"\\nâš ï¸ Documentos com problemas ({len(failed_results)}):\")\n",
    "    for result in failed_results[:5]:  # Mostrar apenas os primeiros 5\n",
    "        filename = Path(result[\"source_file\"]).name\n",
    "        error = result[\"error\"]\n",
    "        print(f\"   âŒ {filename}: {error}\")\n",
    "    \n",
    "    if len(failed_results) > 5:\n",
    "        remaining = len(failed_results) - 5\n",
    "        print(f\"   ... e mais {remaining} erros\")\n",
    "\n",
    "print(\"\\nâœ… Processamento concluÃ­do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ValidaÃ§Ã£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š VALIDAÃ‡ÃƒO DOS RESULTADOS PROCESSADOS\n",
    "print(\"ğŸ“Š Validando resultados do processamento...\")\n",
    "\n",
    "from docling_utils import validate_processed_content\n",
    "import os\n",
    "\n",
    "# Validar conteÃºdo processado\n",
    "validation_result = validate_processed_content(processing_config[\"output_dir\"])\n",
    "\n",
    "if validation_result[\"valid\"]:\n",
    "    print(\"\\nâœ… ValidaÃ§Ã£o bem-sucedida\")\n",
    "    print(f\"   ğŸ“„ Arquivos JSON: {validation_result['json_files_count']}\")\n",
    "    print(f\"   ğŸ“ Arquivos TXT: {validation_result['text_files_count']}\")\n",
    "    print(f\"   ğŸ“Š Total de arquivos: {validation_result['total_files']}\")\nelse:\n",
    "    print(f\"\\nâŒ Problema na validaÃ§Ã£o: {validation_result.get('error', 'Erro desconhecido')}\")\n",
    "\n",
    "# Coletar estatÃ­sticas detalhadas\n",
    "processed_dir = Path(processing_config[\"output_dir\"])\n",
    "content_files = list(processed_dir.glob(\"*_content.json\"))\n",
    "text_files = list(processed_dir.glob(\"*_text.txt\"))\n",
    "\n",
    "print(f\"\\nğŸ“ Arquivos gerados no diretÃ³rio {processed_dir}:\")\n",
    "print(f\"   ğŸ“„ ConteÃºdo estruturado: {len(content_files)} arquivos JSON\")\n",
    "print(f\"   ğŸ“ Texto puro: {len(text_files)} arquivos TXT\")\n",
    "\n",
    "# Verificar qualidade do texto extraÃ­do\n",
    "text_quality_stats = {\n",
    "    \"total_characters\": 0,\n",
    "    \"total_words\": 0,\n",
    "    \"files_with_content\": 0,\n",
    "    \"empty_files\": 0\n",
    "}\n",
    "\n",
    "for text_file in text_files:\n",
    "    try:\n",
    "        with open(text_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "        \n",
    "        if content:\n",
    "            text_quality_stats[\"total_characters\"] += len(content)\n",
    "            text_quality_stats[\"total_words\"] += len(content.split())\n",
    "            text_quality_stats[\"files_with_content\"] += 1\n",
    "        else:\n",
    "            text_quality_stats[\"empty_files\"] += 1\n",
    "            print(f\"   âš ï¸ Arquivo vazio: {text_file.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Erro ao ler {text_file.name}: {e}\")\n",
    "\n",
    "if text_quality_stats[\"files_with_content\"] > 0:\n",
    "    avg_chars = text_quality_stats[\"total_characters\"] / text_quality_stats[\"files_with_content\"]\n",
    "    avg_words = text_quality_stats[\"total_words\"] / text_quality_stats[\"files_with_content\"]\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Qualidade do texto extraÃ­do:\")\n",
    "    print(f\"   ğŸ“ Arquivos com conteÃºdo: {text_quality_stats['files_with_content']}\")\n",
    "    print(f\"   ğŸ”¤ Total de caracteres: {text_quality_stats['total_characters']:,}\")\n",
    "    print(f\"   ğŸ“– Total de palavras: {text_quality_stats['total_words']:,}\")\n",
    "    print(f\"   ğŸ“Š MÃ©dia por arquivo: {avg_chars:.0f} caracteres, {avg_words:.0f} palavras\")\n",
    "    \n",
    "    if text_quality_stats[\"empty_files\"] > 0:\n",
    "        print(f\"   âš ï¸ Arquivos vazios: {text_quality_stats['empty_files']}\")\nelse:\n",
    "    print(\"\\nâŒ Nenhum conteÃºdo de texto foi extraÃ­do com sucesso\")\n",
    "\n",
    "# Preparar dados para prÃ³xima etapa\n",
    "processed_files_info = []\n",
    "for result in successful_results:\n",
    "    content_file = result.get(\"content_file\")\n",
    "    text_file = result.get(\"text_file\")\n",
    "    \n",
    "    if content_file and text_file:\n",
    "        processed_files_info.append({\n",
    "            \"source_document\": result[\"source_file\"],\n",
    "            \"content_file\": content_file,\n",
    "            \"text_file\": text_file,\n",
    "            \"processing_method\": result[\"processing_info\"][\"method\"],\n",
    "            \"confidence_score\": result[\"processing_info\"][\"confidence_score\"],\n",
    "            \"structure_info\": result[\"structure\"]\n",
    "        })\n",
    "\n",
    "print(f\"\\nğŸ“‹ Documentos preparados para prÃ³xima etapa: {len(processed_files_info)}\")\n",
    "print(\"âœ… ValidaÃ§Ã£o concluÃ­da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Salvamento dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ’¾ SALVAMENTO DOS RESULTADOS PARA PRÃ“XIMA ETAPA\nprint(\"ğŸ’¾ Salvando resultados do processamento...\")\n\nfrom pipeline_utils import get_timestamp\n\n# Preparar dados de saÃ­da para prÃ³xima etapa\nstage_output = {\n    \"stage_info\": {\n        \"stage_number\": 3,\n        \"stage_name\": \"docling\",\n        \"completed_at\": get_timestamp(),\n        \"status\": \"success\" if len(successful_results) > 0 else \"completed_with_warnings\",\n        \"processing_duration_seconds\": processing_duration\n    },\n    \n    \"processing_config\": processing_config,\n    \n    \"input_files\": {\n        \"total_input_files\": len(files_to_process),\n        \"processable_files\": len(processable_files),\n        \"skipped_files\": len(skipped_files)\n    },\n    \n    \"processed_documents\": processed_files_info,\n    \n    \"processing_results\": {\n        \"successful_extractions\": len(successful_results),\n        \"failed_extractions\": len(failed_results),\n        \"total_processing_time_seconds\": processing_duration\n    },\n    \n    \"text_quality\": text_quality_stats,\n    \n    \"statistics\": {\n        \"files_processed\": len(processing_results),\n        \"successful_extractions\": len(successful_results),\n        \"failed_extractions\": len(failed_results),\n        \"total_text_characters\": text_quality_stats[\"total_characters\"],\n        \"total_text_words\": text_quality_stats[\"total_words\"],\n        \"avg_processing_time_seconds\": processing_duration / len(successful_results) if successful_results else 0,\n        \"content_files_generated\": len(content_files),\n        \"text_files_generated\": len(text_files)\n    },\n    \n    \"errors\": [\n        {\n            \"source_file\": result[\"source_file\"],\n            \"error\": result[\"error\"]\n        }\n        for result in failed_results\n    ],\n    \n    \"next_stage_instructions\": {\n        \"processed_directory\": processing_config[\"output_dir\"],\n        \"text_files_to_chunk\": [info[\"text_file\"] for info in processed_files_info],\n        \"recommended_chunk_size\": 500,\n        \"recommended_overlap\": 100\n    }\n}\n\n# Salvar usando PipelineState\nstate = PipelineState()\nstate.save_stage_data(3, stage_output)\n\n# Marcar etapa como concluÃ­da\nstate.mark_stage_completed(3)\n\nprint(f\"âœ… Resultados salvos: {state.metadata_dir / 'stage_03_docling.json'}\")\nprint(f\"âœ… Checkpoint criado: {state.checkpoints_dir / 'stage_03_completed.lock'}\")\n\n# Salvar relatÃ³rio detalhado de processamento\nprocessing_report_path = Path(processing_config[\"output_dir\"]) / \"processing_report.json\"\nwith open(processing_report_path, 'w', encoding='utf-8') as f:\n    json.dump({\n        \"timestamp\": get_timestamp(),\n        \"config\": processing_config,\n        \"results\": processing_results,\n        \"quality_stats\": text_quality_stats,\n        \"file_type_stats\": dict(type_stats)\n    }, f, indent=2, ensure_ascii=False, default=str)\n\nprint(f\"ğŸ“ RelatÃ³rio detalhado salvo: {processing_report_path}\")\n\n# Exibir resumo final\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ğŸ‰ ETAPA 3 CONCLUÃDA COM SUCESSO!\")\nprint(\"=\" * 60)\n\nprint(f\"ğŸ“Š Resumo do Processamento Docling:\")\nprint(f\"   ğŸ“„ Documentos processados: {len(successful_results)}/{len(processable_files)}\")\nprint(f\"   ğŸ“ Texto extraÃ­do: {text_quality_stats['total_words']:,} palavras\")\nprint(f\"   â±ï¸ Tempo total: {processing_duration:.1f} segundos\")\nprint(f\"   ğŸ“ Arquivos gerados: {len(content_files)} JSON + {len(text_files)} TXT\")\nprint(f\"   ğŸ“ LocalizaÃ§Ã£o: {processing_config['output_dir']}\")\n\nif failed_results:\n    print(f\"   âš ï¸ Avisos: {len(failed_results)} documentos falharam\")\nelse:\n    print(f\"   âœ… Status: Todos os processamentos bem-sucedidos\")\n\nprint(f\"\\nğŸš€ Pronto para prÃ³xima etapa: 04_SEGMENTACAO_CHUNKS.ipynb\")\nprint(f\"ğŸ“‹ {len(processed_files_info)} documentos prontos para segmentaÃ§Ã£o\")\n\n# Exibir progresso do pipeline\nprint(\"\\nğŸ“ˆ Progresso do Pipeline:\")\nshow_pipeline_progress()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… **Etapa 3 ConcluÃ­da!**\n",
    "\n",
    "### ğŸ¯ **O que foi feito:**\n",
    "- âœ… Documentos analisados por tipo e tamanho\n",
    "- âœ… ConteÃºdo extraÃ­do com Docling (OCR quando necessÃ¡rio)\n",
    "- âœ… Estrutura preservada (tÃ­tulos, seÃ§Ãµes, tabelas)\n",
    "- âœ… Texto limpo e metadados coletados\n",
    "- âœ… ValidaÃ§Ã£o de qualidade realizada\n",
    "\n",
    "### ğŸš€ **PrÃ³xima etapa:**\n",
    "**`04_SEGMENTACAO_CHUNKS.ipynb`** - SegmentaÃ§Ã£o inteligente de texto\n",
    "\n",
    "### ğŸ“Š **Arquivos gerados:**\n",
    "- `pipeline_data/processed/` - ConteÃºdo estruturado e texto extraÃ­do\n",
    "- `pipeline_data/processed/*_content.json` - Estrutura completa de cada documento\n",
    "- `pipeline_data/processed/*_text.txt` - Texto puro extraÃ­do\n",
    "- `pipeline_data/metadata/stage_03_docling.json` - Metadados e estatÃ­sticas\n",
    "- `pipeline_data/processed/processing_report.json` - RelatÃ³rio detalhado\n",
    "- `pipeline_data/checkpoints/stage_03_completed.lock` - Checkpoint de conclusÃ£o\n",
    "\n",
    "### ğŸ“‹ **Dados para prÃ³xima etapa:**\n",
    "- **Texto estruturado:** Pronto para segmentaÃ§Ã£o em chunks\n",
    "- **Metadados de estrutura:** SeÃ§Ãµes, tabelas, confianÃ§a de extraÃ§Ã£o\n",
    "- **Qualidade validada:** Texto limpo e consistente\n",
    "\n",
    "### ğŸ”§ **Para executar prÃ³xima etapa:**\n",
    "1. Abra o notebook `04_SEGMENTACAO_CHUNKS.ipynb`\n",
    "2. Execute as cÃ©lulas em sequÃªncia\n",
    "3. Ou use o `00_PIPELINE_MASTER.ipynb` para execuÃ§Ã£o automÃ¡tica\n",
    "\n",
    "---\n",
    "\n",
    "**âš™ï¸ ConteÃºdo extraÃ­do e estruturado! Prontos para segmentaÃ§Ã£o inteligente.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}