{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è ETAPA 3: PROCESSAMENTO COM DOCLING\n",
    "\n",
    "## üéØ **O que esta etapa faz**\n",
    "Extrai e estrutura o conte√∫do de todos os documentos baixados, transformando PDFs, DOCs e outros formatos em texto estruturado pronto para an√°lise.\n",
    "\n",
    "## ü§î **Por que esta etapa √© necess√°ria**\n",
    "Documentos v√™m em formatos diferentes (PDF, DOCX, MD). Precisamos:\n",
    "- üìÑ **Extrair texto puro** de cada documento\n",
    "- üèóÔ∏è **Preservar estrutura** (t√≠tulos, se√ß√µes, tabelas)\n",
    "- üîç **Usar OCR** quando necess√°rio (imagens, PDFs escaneados)\n",
    "- üìä **Coletar metadados** de estrutura e conte√∫do\n",
    "\n",
    "## ‚öôÔ∏è **Como funciona**\n",
    "1. **Analisa cada documento** para determinar o melhor m√©todo de extra√ß√£o\n",
    "2. **Aplica Docling** para extrair conte√∫do estruturado\n",
    "3. **Usa OCR** em PDFs escaneados ou com baixa qualidade de texto\n",
    "4. **Preserva hierarquia** de t√≠tulos e se√ß√µes\n",
    "5. **Gera texto limpo** removendo formata√ß√£o desnecess√°ria\n",
    "6. **Salva conte√∫do estruturado** e texto puro separadamente\n",
    "\n",
    "## üìä **Par√¢metros que voc√™ pode ajustar**\n",
    "- `use_ocr`: Habilitar OCR autom√°tico (padr√£o: True)\n",
    "- `ocr_confidence_threshold`: Limite de confian√ßa OCR (padr√£o: 0.7)\n",
    "- `preserve_structure`: Manter hierarquia de t√≠tulos (padr√£o: True)\n",
    "- `clean_text`: Remover formata√ß√£o extra (padr√£o: True)\n",
    "- `extract_tables`: Processar tabelas separadamente (padr√£o: True)\n",
    "\n",
    "## üëÅÔ∏è **O que esperar de sa√≠da**\n",
    "- üìÅ **Pasta `processed/`** com conte√∫do estruturado\n",
    "- üìÑ **Arquivos `*_content.json`** com estrutura completa\n",
    "- üìù **Arquivos `*_text.txt`** com texto puro extra√≠do\n",
    "- üìä **Metadados** de extra√ß√£o (confian√ßa, m√©todo usado, etc.)\n",
    "- üìà **Estat√≠sticas** de processamento por tipo de arquivo\n",
    "\n",
    "## ‚ö†Ô∏è **Pontos importantes**\n",
    "- **Requer Etapa 2** (Coleta GitLab) conclu√≠da primeiro\n",
    "- **Docling** deve estar instalado e funcional\n",
    "- **OCR** pode ser lento para documentos grandes\n",
    "- **Modo independente** dispon√≠vel para testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Verifica√ß√£o de Depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# üõ°Ô∏è VERIFICA√á√ÉO AUTOM√ÅTICA DE DEPEND√äNCIAS\nimport sys\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Adicionar biblioteca ao path\nsys.path.insert(0, str(Path().parent / \"src\"))\n\nfrom pipeline_utils import (\n    PipelineState, \n    check_prerequisites, \n    show_pipeline_progress\n)\n\nprint(\"‚öôÔ∏è ETAPA 3: PROCESSAMENTO COM DOCLING\")\nprint(\"=\" * 60)\nprint(f\"üïí Iniciado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# Verificar depend√™ncias\nCURRENT_STAGE = 3\nprerequisites_ok = check_prerequisites(CURRENT_STAGE)\n\nif not prerequisites_ok:\n    print(\"\\n‚ùå ERRO: Depend√™ncias n√£o atendidas!\")\n    print(\"üìã Execute primeiro as etapas anteriores:\")\n    print(\"   1. 01_FUNDACAO_PREPARACAO.ipynb\")\n    print(\"   2. 02_COLETA_GITLAB.ipynb\")\n    \n    show_pipeline_progress()\n    raise RuntimeError(\"Depend√™ncias n√£o atendidas - execute etapas anteriores primeiro\")\n\nprint(\"\\n‚úÖ Depend√™ncias verificadas - pode prosseguir\")\nprint(\"üìà Progresso atual do pipeline:\")\nshow_pipeline_progress()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìã Carregamento de Dados"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# üìã CARREGAMENTO DOS DADOS DA ETAPA ANTERIOR\nprint(\"üìã Carregando dados da etapa anterior...\")\n\n# Carregar dados da etapa 2 (GitLab)\nstate = PipelineState()\ntry:\n    documents_data = state.load_stage_data(2)\n    print(\"‚úÖ Dados carregados da Etapa 2 (GitLab)\")\n    print(f\"   üìÑ Documentos dispon√≠veis: {len(documents_data['local_files'])}\")\n    print(f\"   üìÅ Diret√≥rio: {documents_data['next_stage_instructions']['documents_directory']}\")\nexcept Exception as e:\n    print(f\"‚ùå Erro ao carregar dados: {e}\")\n    raise\n\nprint(\"\\n\" + \"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Configura√ß√£o de Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã CONFIGURA√á√ÉO DOS PAR√ÇMETROS DE PROCESSAMENTO\n",
    "print(\"üìã Configurando par√¢metros de processamento Docling...\")\n",
    "\n",
    "# üéØ PAR√ÇMETROS AJUST√ÅVEIS\n",
    "processing_config = {\n",
    "    # Docling\n",
    "    \"use_ocr\": True,\n",
    "    \"ocr_confidence_threshold\": 0.7,\n",
    "    \"preserve_structure\": True,\n",
    "    \"clean_text\": True,\n",
    "    \"extract_tables\": True,\n",
    "    \"extract_images\": False,  # Por enquanto, focar em texto\n",
    "    \n",
    "    # Processamento\n",
    "    \"batch_size\": 4,  # Processar at√© 4 documentos simultaneamente\n",
    "    \"timeout_per_doc_seconds\": 300,  # 5 minutos por documento\n",
    "    \"skip_large_files_mb\": 100,  # Pular arquivos muito grandes\n",
    "    \n",
    "    # Sa√≠da\n",
    "    \"output_dir\": \"../pipeline_data/processed\",\n",
    "    \"save_intermediate\": True,  # Salvar resultados intermedi√°rios\n",
    "    \"overwrite_existing\": True\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Par√¢metros de processamento configurados:\")\n",
    "print(f\"   üîç OCR habilitado: {processing_config['use_ocr']}\")\n",
    "print(f\"   üìä Limite confian√ßa OCR: {processing_config['ocr_confidence_threshold']}\")\n",
    "print(f\"   üèóÔ∏è Preservar estrutura: {processing_config['preserve_structure']}\")\n",
    "print(f\"   üßπ Limpar texto: {processing_config['clean_text']}\")\n",
    "print(f\"   üìã Extrair tabelas: {processing_config['extract_tables']}\")\n",
    "print(f\"   üì¶ Lote: {processing_config['batch_size']} documentos simult√¢neos\")\n",
    "print(f\"   ‚è±Ô∏è Timeout: {processing_config['timeout_per_doc_seconds']} segundos por documento\")\n",
    "print(f\"   üìÅ Sa√≠da: {processing_config['output_dir']}\")\n",
    "\n",
    "# Preparar diret√≥rio de sa√≠da\n",
    "from pipeline_utils import ensure_directory\n",
    "ensure_directory(processing_config[\"output_dir\"])\n",
    "\n",
    "print(\"\\n‚úÖ Configura√ß√£o de processamento pronta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ An√°lise dos Documentos de Entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÑ AN√ÅLISE DOS DOCUMENTOS PARA PROCESSAMENTO\n",
    "print(\"üìÑ Analisando documentos para processamento...\")\n",
    "\n",
    "from collections import defaultdict\n",
    "from pipeline_utils import format_file_size\n",
    "\n",
    "# Analisar arquivos de entrada\n",
    "files_to_process = documents_data[\"local_files\"]\n",
    "print(f\"\\nüìä Total de documentos: {len(files_to_process)}\")\n",
    "\n",
    "# Estat√≠sticas por tipo\n",
    "type_stats = defaultdict(lambda: {\"count\": 0, \"total_size\": 0, \"files\": []})\n",
    "total_size = 0\n",
    "processable_files = []\n",
    "skipped_files = []\n",
    "\n",
    "for file_info in files_to_process:\n",
    "    file_path = Path(file_info[\"local_path\"])\n",
    "    file_ext = file_info[\"extension\"]\n",
    "    file_size = file_info[\"size_bytes\"]\n",
    "    file_size_mb = file_size / (1024 * 1024)\n",
    "    \n",
    "    # Verificar se arquivo existe\n",
    "    if not file_path.exists():\n",
    "        print(f\"   ‚ö†Ô∏è Arquivo n√£o encontrado: {file_path.name}\")\n",
    "        skipped_files.append((file_path.name, \"file_not_found\"))\n",
    "        continue\n",
    "    \n",
    "    # Verificar tamanho\n",
    "    if file_size_mb > processing_config[\"skip_large_files_mb\"]:\n",
    "        print(f\"   ‚è≠Ô∏è Pulado (muito grande): {file_path.name} ({file_size_mb:.1f} MB)\")\n",
    "        skipped_files.append((file_path.name, \"too_large\"))\n",
    "        continue\n",
    "    \n",
    "    # Arquivo OK para processamento\n",
    "    type_stats[file_ext][\"count\"] += 1\n",
    "    type_stats[file_ext][\"total_size\"] += file_size\n",
    "    type_stats[file_ext][\"files\"].append(file_info)\n",
    "    total_size += file_size\n",
    "    processable_files.append(file_info)\n",
    "    \n",
    "    print(f\"   ‚úÖ {file_path.name} ({format_file_size(file_size)}) - {file_ext}\")\n",
    "\n",
    "print(f\"\\nüìà Resumo da an√°lise:\")\n",
    "print(f\"   ‚úÖ Process√°veis: {len(processable_files)}\")\n",
    "print(f\"   ‚è≠Ô∏è Pulados: {len(skipped_files)}\")\n",
    "print(f\"   üìè Tamanho total: {format_file_size(total_size)}\")\n",
    "\n",
    "print(f\"\\nüìä Por tipo de arquivo:\")\n",
    "for ext, stats in sorted(type_stats.items()):\n",
    "    count = stats[\"count\"]\n",
    "    size = format_file_size(stats[\"total_size\"])\n",
    "    avg_size = format_file_size(stats[\"total_size\"] / count) if count > 0 else \"0 B\"\n",
    "    print(f\"   {ext}: {count} arquivos, {size} total, {avg_size} m√©dio\")\n",
    "\n",
    "if skipped_files:\n",
    "    print(f\"\\n‚ö†Ô∏è Arquivos pulados ({len(skipped_files)}):\")\n",
    "    for filename, reason in skipped_files:\n",
    "        reason_text = {\n",
    "            \"file_not_found\": \"n√£o encontrado\",\n",
    "            \"too_large\": \"muito grande\"\n",
    "        }.get(reason, reason)\n",
    "        print(f\"   - {filename}: {reason_text}\")\n",
    "\n",
    "if not processable_files:\n",
    "    print(\"\\n‚ùå Nenhum arquivo dispon√≠vel para processamento\")\n",
    "    raise ValueError(\"Nenhum documento para processar\")\n",
    "\n",
    "print(\"\\n‚úÖ An√°lise conclu√≠da - pronto para processamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Processamento com Docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è EXECU√á√ÉO DO PROCESSAMENTO COM DOCLING\n",
    "print(\"‚öôÔ∏è Iniciando processamento com Docling...\")\n",
    "\n",
    "from docling_utils import extract_content_from_file, batch_process_documents\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Preparar lista de arquivos para processamento\n",
    "file_paths = [file_info[\"local_path\"] for file_info in processable_files]\n",
    "\n",
    "print(f\"\\nüöÄ Processando {len(file_paths)} documentos...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Executar processamento em lote\n",
    "start_time = datetime.now()\n",
    "\n",
    "processing_results = batch_process_documents(\n",
    "    file_list=file_paths,\n",
    "    output_dir=processing_config[\"output_dir\"]\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "processing_duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä RESULTADO DO PROCESSAMENTO:\")\n",
    "\n",
    "# Contar sucessos e falhas\n",
    "successful_results = [r for r in processing_results if \"error\" not in r]\n",
    "failed_results = [r for r in processing_results if \"error\" in r]\n",
    "\n",
    "print(f\"   üìÑ Total processado: {len(processing_results)}\")\n",
    "print(f\"   ‚úÖ Sucessos: {len(successful_results)}\")\n",
    "print(f\"   ‚ùå Falhas: {len(failed_results)}\")\n",
    "print(f\"   ‚è±Ô∏è Tempo total: {processing_duration:.1f} segundos\")\n",
    "\n",
    "if successful_results:\n",
    "    avg_time = processing_duration / len(successful_results)\n",
    "    print(f\"   üìà Tempo m√©dio: {avg_time:.1f} segundos por documento\")\n",
    "\n",
    "# Mostrar resultados de sucesso\n",
    "if successful_results:\n",
    "    print(\"\\nüìÅ Documentos processados com sucesso:\")\n",
    "    for result in successful_results[:10]:  # Mostrar apenas os primeiros 10\n",
    "        filename = Path(result[\"source_file\"]).name\n",
    "        method = result[\"processing_info\"][\"method\"]\n",
    "        confidence = result[\"processing_info\"][\"confidence_score\"]\n",
    "        print(f\"   ‚úÖ {filename} (m√©todo: {method}, confian√ßa: {confidence:.2f})\")\n",
    "    \n",
    "    if len(successful_results) > 10:\n",
    "        remaining = len(successful_results) - 10\n",
    "        print(f\"   ... e mais {remaining} documentos\")\n",
    "\n",
    "# Mostrar falhas se houver\n",
    "if failed_results:\n",
    "    print(f\"\\n‚ö†Ô∏è Documentos com problemas ({len(failed_results)}):\")\n",
    "    for result in failed_results[:5]:  # Mostrar apenas os primeiros 5\n",
    "        filename = Path(result[\"source_file\"]).name\n",
    "        error = result[\"error\"]\n",
    "        print(f\"   ‚ùå {filename}: {error}\")\n",
    "    \n",
    "    if len(failed_results) > 5:\n",
    "        remaining = len(failed_results) - 5\n",
    "        print(f\"   ... e mais {remaining} erros\")\n",
    "\n",
    "print(\"\\n‚úÖ Processamento conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Valida√ß√£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä VALIDA√á√ÉO DOS RESULTADOS PROCESSADOS\n",
    "print(\"üìä Validando resultados do processamento...\")\n",
    "\n",
    "from docling_utils import validate_processed_content\n",
    "import os\n",
    "\n",
    "# Validar conte√∫do processado\n",
    "validation_result = validate_processed_content(processing_config[\"output_dir\"])\n",
    "\n",
    "if validation_result[\"valid\"]:\n",
    "    print(\"\\n‚úÖ Valida√ß√£o bem-sucedida\")\n",
    "    print(f\"   üìÑ Arquivos JSON: {validation_result['json_files_count']}\")\n",
    "    print(f\"   üìù Arquivos TXT: {validation_result['text_files_count']}\")\n",
    "    print(f\"   üìä Total de arquivos: {validation_result['total_files']}\")\nelse:\n",
    "    print(f\"\\n‚ùå Problema na valida√ß√£o: {validation_result.get('error', 'Erro desconhecido')}\")\n",
    "\n",
    "# Coletar estat√≠sticas detalhadas\n",
    "processed_dir = Path(processing_config[\"output_dir\"])\n",
    "content_files = list(processed_dir.glob(\"*_content.json\"))\n",
    "text_files = list(processed_dir.glob(\"*_text.txt\"))\n",
    "\n",
    "print(f\"\\nüìÅ Arquivos gerados no diret√≥rio {processed_dir}:\")\n",
    "print(f\"   üìÑ Conte√∫do estruturado: {len(content_files)} arquivos JSON\")\n",
    "print(f\"   üìù Texto puro: {len(text_files)} arquivos TXT\")\n",
    "\n",
    "# Verificar qualidade do texto extra√≠do\n",
    "text_quality_stats = {\n",
    "    \"total_characters\": 0,\n",
    "    \"total_words\": 0,\n",
    "    \"files_with_content\": 0,\n",
    "    \"empty_files\": 0\n",
    "}\n",
    "\n",
    "for text_file in text_files:\n",
    "    try:\n",
    "        with open(text_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "        \n",
    "        if content:\n",
    "            text_quality_stats[\"total_characters\"] += len(content)\n",
    "            text_quality_stats[\"total_words\"] += len(content.split())\n",
    "            text_quality_stats[\"files_with_content\"] += 1\n",
    "        else:\n",
    "            text_quality_stats[\"empty_files\"] += 1\n",
    "            print(f\"   ‚ö†Ô∏è Arquivo vazio: {text_file.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erro ao ler {text_file.name}: {e}\")\n",
    "\n",
    "if text_quality_stats[\"files_with_content\"] > 0:\n",
    "    avg_chars = text_quality_stats[\"total_characters\"] / text_quality_stats[\"files_with_content\"]\n",
    "    avg_words = text_quality_stats[\"total_words\"] / text_quality_stats[\"files_with_content\"]\n",
    "    \n",
    "    print(f\"\\nüìà Qualidade do texto extra√≠do:\")\n",
    "    print(f\"   üìù Arquivos com conte√∫do: {text_quality_stats['files_with_content']}\")\n",
    "    print(f\"   üî§ Total de caracteres: {text_quality_stats['total_characters']:,}\")\n",
    "    print(f\"   üìñ Total de palavras: {text_quality_stats['total_words']:,}\")\n",
    "    print(f\"   üìä M√©dia por arquivo: {avg_chars:.0f} caracteres, {avg_words:.0f} palavras\")\n",
    "    \n",
    "    if text_quality_stats[\"empty_files\"] > 0:\n",
    "        print(f\"   ‚ö†Ô∏è Arquivos vazios: {text_quality_stats['empty_files']}\")\nelse:\n",
    "    print(\"\\n‚ùå Nenhum conte√∫do de texto foi extra√≠do com sucesso\")\n",
    "\n",
    "# Preparar dados para pr√≥xima etapa\n",
    "processed_files_info = []\n",
    "for result in successful_results:\n",
    "    content_file = result.get(\"content_file\")\n",
    "    text_file = result.get(\"text_file\")\n",
    "    \n",
    "    if content_file and text_file:\n",
    "        processed_files_info.append({\n",
    "            \"source_document\": result[\"source_file\"],\n",
    "            \"content_file\": content_file,\n",
    "            \"text_file\": text_file,\n",
    "            \"processing_method\": result[\"processing_info\"][\"method\"],\n",
    "            \"confidence_score\": result[\"processing_info\"][\"confidence_score\"],\n",
    "            \"structure_info\": result[\"structure\"]\n",
    "        })\n",
    "\n",
    "print(f\"\\nüìã Documentos preparados para pr√≥xima etapa: {len(processed_files_info)}\")\n",
    "print(\"‚úÖ Valida√ß√£o conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Salvamento dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# üíæ SALVAMENTO DOS RESULTADOS PARA PR√ìXIMA ETAPA\nprint(\"üíæ Salvando resultados do processamento...\")\n\nfrom pipeline_utils import get_timestamp\n\n# Preparar dados de sa√≠da para pr√≥xima etapa\nstage_output = {\n    \"stage_info\": {\n        \"stage_number\": 3,\n        \"stage_name\": \"docling\",\n        \"completed_at\": get_timestamp(),\n        \"status\": \"success\" if len(successful_results) > 0 else \"completed_with_warnings\",\n        \"processing_duration_seconds\": processing_duration\n    },\n    \n    \"processing_config\": processing_config,\n    \n    \"input_files\": {\n        \"total_input_files\": len(files_to_process),\n        \"processable_files\": len(processable_files),\n        \"skipped_files\": len(skipped_files)\n    },\n    \n    \"processed_documents\": processed_files_info,\n    \n    \"processing_results\": {\n        \"successful_extractions\": len(successful_results),\n        \"failed_extractions\": len(failed_results),\n        \"total_processing_time_seconds\": processing_duration\n    },\n    \n    \"text_quality\": text_quality_stats,\n    \n    \"statistics\": {\n        \"files_processed\": len(processing_results),\n        \"successful_extractions\": len(successful_results),\n        \"failed_extractions\": len(failed_results),\n        \"total_text_characters\": text_quality_stats[\"total_characters\"],\n        \"total_text_words\": text_quality_stats[\"total_words\"],\n        \"avg_processing_time_seconds\": processing_duration / len(successful_results) if successful_results else 0,\n        \"content_files_generated\": len(content_files),\n        \"text_files_generated\": len(text_files)\n    },\n    \n    \"errors\": [\n        {\n            \"source_file\": result[\"source_file\"],\n            \"error\": result[\"error\"]\n        }\n        for result in failed_results\n    ],\n    \n    \"next_stage_instructions\": {\n        \"processed_directory\": processing_config[\"output_dir\"],\n        \"text_files_to_chunk\": [info[\"text_file\"] for info in processed_files_info],\n        \"recommended_chunk_size\": 500,\n        \"recommended_overlap\": 100\n    }\n}\n\n# Salvar usando PipelineState\nstate = PipelineState()\nstate.save_stage_data(3, stage_output)\n\n# Marcar etapa como conclu√≠da\nstate.mark_stage_completed(3)\n\nprint(f\"‚úÖ Resultados salvos: {state.metadata_dir / 'stage_03_docling.json'}\")\nprint(f\"‚úÖ Checkpoint criado: {state.checkpoints_dir / 'stage_03_completed.lock'}\")\n\n# Salvar relat√≥rio detalhado de processamento\nprocessing_report_path = Path(processing_config[\"output_dir\"]) / \"processing_report.json\"\nwith open(processing_report_path, 'w', encoding='utf-8') as f:\n    json.dump({\n        \"timestamp\": get_timestamp(),\n        \"config\": processing_config,\n        \"results\": processing_results,\n        \"quality_stats\": text_quality_stats,\n        \"file_type_stats\": dict(type_stats)\n    }, f, indent=2, ensure_ascii=False, default=str)\n\nprint(f\"üìù Relat√≥rio detalhado salvo: {processing_report_path}\")\n\n# Exibir resumo final\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üéâ ETAPA 3 CONCLU√çDA COM SUCESSO!\")\nprint(\"=\" * 60)\n\nprint(f\"üìä Resumo do Processamento Docling:\")\nprint(f\"   üìÑ Documentos processados: {len(successful_results)}/{len(processable_files)}\")\nprint(f\"   üìù Texto extra√≠do: {text_quality_stats['total_words']:,} palavras\")\nprint(f\"   ‚è±Ô∏è Tempo total: {processing_duration:.1f} segundos\")\nprint(f\"   üìÅ Arquivos gerados: {len(content_files)} JSON + {len(text_files)} TXT\")\nprint(f\"   üìç Localiza√ß√£o: {processing_config['output_dir']}\")\n\nif failed_results:\n    print(f\"   ‚ö†Ô∏è Avisos: {len(failed_results)} documentos falharam\")\nelse:\n    print(f\"   ‚úÖ Status: Todos os processamentos bem-sucedidos\")\n\nprint(f\"\\nüöÄ Pronto para pr√≥xima etapa: 04_SEGMENTACAO_CHUNKS.ipynb\")\nprint(f\"üìã {len(processed_files_info)} documentos prontos para segmenta√ß√£o\")\n\n# Exibir progresso do pipeline\nprint(\"\\nüìà Progresso do Pipeline:\")\nshow_pipeline_progress()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ **Etapa 3 Conclu√≠da!**\n",
    "\n",
    "### üéØ **O que foi feito:**\n",
    "- ‚úÖ Documentos analisados por tipo e tamanho\n",
    "- ‚úÖ Conte√∫do extra√≠do com Docling (OCR quando necess√°rio)\n",
    "- ‚úÖ Estrutura preservada (t√≠tulos, se√ß√µes, tabelas)\n",
    "- ‚úÖ Texto limpo e metadados coletados\n",
    "- ‚úÖ Valida√ß√£o de qualidade realizada\n",
    "\n",
    "### üöÄ **Pr√≥xima etapa:**\n",
    "**`04_SEGMENTACAO_CHUNKS.ipynb`** - Segmenta√ß√£o inteligente de texto\n",
    "\n",
    "### üìä **Arquivos gerados:**\n",
    "- `pipeline_data/processed/` - Conte√∫do estruturado e texto extra√≠do\n",
    "- `pipeline_data/processed/*_content.json` - Estrutura completa de cada documento\n",
    "- `pipeline_data/processed/*_text.txt` - Texto puro extra√≠do\n",
    "- `pipeline_data/metadata/stage_03_docling.json` - Metadados e estat√≠sticas\n",
    "- `pipeline_data/processed/processing_report.json` - Relat√≥rio detalhado\n",
    "- `pipeline_data/checkpoints/stage_03_completed.lock` - Checkpoint de conclus√£o\n",
    "\n",
    "### üìã **Dados para pr√≥xima etapa:**\n",
    "- **Texto estruturado:** Pronto para segmenta√ß√£o em chunks\n",
    "- **Metadados de estrutura:** Se√ß√µes, tabelas, confian√ßa de extra√ß√£o\n",
    "- **Qualidade validada:** Texto limpo e consistente\n",
    "\n",
    "### üîß **Para executar pr√≥xima etapa:**\n",
    "1. Abra o notebook `04_SEGMENTACAO_CHUNKS.ipynb`\n",
    "2. Execute as c√©lulas em sequ√™ncia\n",
    "3. Ou use o `00_PIPELINE_MASTER.ipynb` para execu√ß√£o autom√°tica\n",
    "\n",
    "---\n",
    "\n",
    "**‚öôÔ∏è Conte√∫do extra√≠do e estruturado! Prontos para segmenta√ß√£o inteligente.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}