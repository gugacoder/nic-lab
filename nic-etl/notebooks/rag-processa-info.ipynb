{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏢 RAG: Informações sobre a Processa\n",
    "\n",
    "Este notebook implementa consulta semântica ao QDrant para obter informações sobre a Processa Sistemas.\n",
    "\n",
    "## 📋 Funcionalidades\n",
    "\n",
    "- Conexão com QDrant\n",
    "- Geração de embeddings para query\n",
    "- Busca semântica com reranking\n",
    "- Formatação de resposta estruturada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Importações e Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance, Filter, FieldCondition, MatchValue\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, asdict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega configurações do ambiente\n",
    "env_file = Path('.env.development')\n",
    "if not env_file.exists():\n",
    "    env_file = Path('.env')\n",
    "    \n",
    "load_dotenv(env_file)\n",
    "\n",
    "# Configuração do QDrant\n",
    "QDRANT_URL = os.getenv('QDRANT_URL', 'https://qdrant.codrstudio.dev/')\n",
    "QDRANT_API_KEY = os.getenv('QDRANT_API_KEY')\n",
    "QDRANT_COLLECTION = os.getenv('QDRANT_COLLECTION', 'nic_dev')\n",
    "\n",
    "# Modelo de embeddings\n",
    "EMBEDDING_MODEL = 'BAAI/bge-m3'\n",
    "\n",
    "print(f\"📡 QDrant URL: {QDRANT_URL}\")\n",
    "print(f\"📦 Collection: {QDRANT_COLLECTION}\")\n",
    "print(f\"🧠 Modelo: {EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Classe de Busca RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"Resultado de busca estruturado\"\"\"\n",
    "    text: str\n",
    "    score: float\n",
    "    metadata: Dict[str, Any]\n",
    "    source: str\n",
    "    \n",
    "class ProcessaInfoRAG:\n",
    "    \"\"\"Sistema RAG para informações sobre a Processa\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Inicializa conexões e modelo\"\"\"\n",
    "        # Conexão QDrant\n",
    "        self.client = QdrantClient(\n",
    "            url=QDRANT_URL,\n",
    "            api_key=QDRANT_API_KEY,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        # Modelo de embeddings\n",
    "        self.model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Configurações de busca\n",
    "        self.top_k = 10\n",
    "        self.score_threshold = 0.5\n",
    "        \n",
    "    def generate_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Gera embedding para texto\"\"\"\n",
    "        with torch.no_grad():\n",
    "            embedding = self.model.encode(\n",
    "                text,\n",
    "                normalize_embeddings=True,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "        return embedding\n",
    "    \n",
    "    def search_qdrant(self, query: str, limit: int = 10) -> List[SearchResult]:\n",
    "        \"\"\"Busca semântica no QDrant\"\"\"\n",
    "        # Gera embedding da query\n",
    "        query_vector = self.generate_embedding(query).tolist()\n",
    "        \n",
    "        # Busca no QDrant\n",
    "        results = self.client.search(\n",
    "            collection_name=QDRANT_COLLECTION,\n",
    "            query_vector=query_vector,\n",
    "            limit=limit,\n",
    "            with_payload=True\n",
    "        )\n",
    "        \n",
    "        # Formata resultados\n",
    "        search_results = []\n",
    "        for hit in results:\n",
    "            if hit.score >= self.score_threshold:\n",
    "                result = SearchResult(\n",
    "                    text=hit.payload.get('text', ''),\n",
    "                    score=hit.score,\n",
    "                    metadata=hit.payload.get('metadata', {}),\n",
    "                    source=hit.payload.get('source', 'unknown')\n",
    "                )\n",
    "                search_results.append(result)\n",
    "                \n",
    "        return search_results\n",
    "    \n",
    "    def rerank_results(self, query: str, results: List[SearchResult]) -> List[SearchResult]:\n",
    "        \"\"\"Reordena resultados por relevância\"\"\"\n",
    "        if not results:\n",
    "            return results\n",
    "            \n",
    "        # Calcula scores de cross-encoding para reranking\n",
    "        texts = [r.text for r in results]\n",
    "        pairs = [[query, text] for text in texts]\n",
    "        \n",
    "        # Usa o modelo para calcular similaridade\n",
    "        with torch.no_grad():\n",
    "            query_embedding = self.model.encode(query, normalize_embeddings=True)\n",
    "            text_embeddings = self.model.encode(texts, normalize_embeddings=True)\n",
    "            \n",
    "            # Calcula similaridade cosseno\n",
    "            similarities = np.dot(text_embeddings, query_embedding)\n",
    "        \n",
    "        # Combina scores originais com reranking\n",
    "        for i, result in enumerate(results):\n",
    "            combined_score = (result.score * 0.7) + (similarities[i] * 0.3)\n",
    "            result.score = float(combined_score)\n",
    "        \n",
    "        # Reordena por score combinado\n",
    "        results.sort(key=lambda x: x.score, reverse=True)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_processa_info(self, query: str = \"Quem é a Processa Sistemas\") -> Dict[str, Any]:\n",
    "        \"\"\"Obtém informações sobre a Processa\"\"\"\n",
    "        try:\n",
    "            # Busca inicial\n",
    "            results = self.search_qdrant(query, limit=self.top_k)\n",
    "            \n",
    "            if not results:\n",
    "                return {\n",
    "                    \"status\": \"no_results\",\n",
    "                    \"message\": \"Nenhuma informação encontrada sobre a Processa\",\n",
    "                    \"query\": query,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "            \n",
    "            # Reranking\n",
    "            results = self.rerank_results(query, results)\n",
    "            \n",
    "            # Extrai informações principais\n",
    "            top_results = results[:5]  # Top 5 mais relevantes\n",
    "            \n",
    "            # Monta resposta estruturada\n",
    "            response = {\n",
    "                \"status\": \"success\",\n",
    "                \"query\": query,\n",
    "                \"company_info\": {\n",
    "                    \"name\": \"Processa Sistemas\",\n",
    "                    \"description\": self._extract_description(top_results),\n",
    "                    \"key_points\": self._extract_key_points(top_results),\n",
    "                    \"sources\": self._extract_sources(top_results)\n",
    "                },\n",
    "                \"search_metadata\": {\n",
    "                    \"total_results\": len(results),\n",
    "                    \"top_score\": float(results[0].score) if results else 0,\n",
    "                    \"collection\": QDRANT_COLLECTION,\n",
    "                    \"model\": EMBEDDING_MODEL\n",
    "                },\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": str(e),\n",
    "                \"query\": query,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def _extract_description(self, results: List[SearchResult]) -> str:\n",
    "        \"\"\"Extrai descrição consolidada\"\"\"\n",
    "        if not results:\n",
    "            return \"Informação não disponível\"\n",
    "        \n",
    "        # Pega o texto mais relevante\n",
    "        main_text = results[0].text\n",
    "        \n",
    "        # Limita tamanho\n",
    "        if len(main_text) > 500:\n",
    "            main_text = main_text[:497] + \"...\"\n",
    "            \n",
    "        return main_text\n",
    "    \n",
    "    def _extract_key_points(self, results: List[SearchResult]) -> List[str]:\n",
    "        \"\"\"Extrai pontos principais\"\"\"\n",
    "        key_points = []\n",
    "        seen = set()\n",
    "        \n",
    "        for result in results:\n",
    "            # Extrai frases importantes\n",
    "            sentences = result.text.split('.')\n",
    "            for sentence in sentences[:2]:  # Primeiras 2 frases\n",
    "                sentence = sentence.strip()\n",
    "                if sentence and len(sentence) > 20 and sentence not in seen:\n",
    "                    key_points.append(sentence)\n",
    "                    seen.add(sentence)\n",
    "                    \n",
    "                if len(key_points) >= 5:\n",
    "                    break\n",
    "                    \n",
    "            if len(key_points) >= 5:\n",
    "                break\n",
    "                \n",
    "        return key_points\n",
    "    \n",
    "    def _extract_sources(self, results: List[SearchResult]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extrai fontes dos documentos\"\"\"\n",
    "        sources = []\n",
    "        seen_sources = set()\n",
    "        \n",
    "        for result in results[:5]:\n",
    "            source_name = result.source\n",
    "            if source_name not in seen_sources:\n",
    "                sources.append({\n",
    "                    \"document\": source_name,\n",
    "                    \"relevance_score\": float(result.score),\n",
    "                    \"metadata\": result.metadata\n",
    "                })\n",
    "                seen_sources.add(source_name)\n",
    "                \n",
    "        return sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Funções para Integração com API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instância global para reutilização\n",
    "_rag_instance = None\n",
    "\n",
    "def get_rag_instance() -> ProcessaInfoRAG:\n",
    "    \"\"\"Retorna instância singleton do RAG\"\"\"\n",
    "    global _rag_instance\n",
    "    if _rag_instance is None:\n",
    "        _rag_instance = ProcessaInfoRAG()\n",
    "    return _rag_instance\n",
    "\n",
    "def get_processa_info_api(custom_query: Optional[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Função principal para uso em API\"\"\"\n",
    "    rag = get_rag_instance()\n",
    "    \n",
    "    # Query padrão ou customizada\n",
    "    query = custom_query or \"Quem é a Processa Sistemas\"\n",
    "    \n",
    "    # Busca informações\n",
    "    result = rag.get_processa_info(query)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def search_processa_documents(query: str, limit: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Busca genérica em documentos sobre a Processa\"\"\"\n",
    "    rag = get_rag_instance()\n",
    "    \n",
    "    try:\n",
    "        # Busca\n",
    "        results = rag.search_qdrant(query, limit=limit * 2)\n",
    "        \n",
    "        # Reranking\n",
    "        results = rag.rerank_results(query, results)\n",
    "        \n",
    "        # Formata resposta\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"query\": query,\n",
    "            \"results\": [\n",
    "                {\n",
    "                    \"text\": r.text[:300] + \"...\" if len(r.text) > 300 else r.text,\n",
    "                    \"score\": float(r.score),\n",
    "                    \"source\": r.source,\n",
    "                    \"metadata\": r.metadata\n",
    "                }\n",
    "                for r in results[:limit]\n",
    "            ],\n",
    "            \"total_found\": len(results),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": str(e),\n",
    "            \"query\": query,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Teste do Sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_processa_rag():\n",
    "    \"\"\"Testa o sistema RAG\"\"\"\n",
    "    print(\"🧪 Testando RAG Processa Info\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Teste 1: Query padrão\n",
    "    print(\"\\n📍 Teste 1: Query padrão\")\n",
    "    result = get_processa_info_api()\n",
    "    \n",
    "    if result['status'] == 'success':\n",
    "        print(\"✅ Busca realizada com sucesso!\")\n",
    "        print(f\"\\n📝 Descrição encontrada:\")\n",
    "        print(result['company_info']['description'][:200] + \"...\")\n",
    "        print(f\"\\n🎯 Pontos principais: {len(result['company_info']['key_points'])} encontrados\")\n",
    "        print(f\"📚 Fontes: {len(result['company_info']['sources'])} documentos\")\n",
    "    else:\n",
    "        print(f\"❌ Erro: {result.get('message', 'Desconhecido')}\")\n",
    "    \n",
    "    # Teste 2: Query customizada\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\\n📍 Teste 2: Query customizada\")\n",
    "    custom_result = search_processa_documents(\"serviços da Processa\", limit=3)\n",
    "    \n",
    "    if custom_result['status'] == 'success':\n",
    "        print(\"✅ Busca customizada realizada!\")\n",
    "        print(f\"\\n📊 Resultados encontrados: {custom_result['total_found']}\")\n",
    "        print(f\"🔝 Top {len(custom_result['results'])} resultados retornados\")\n",
    "        \n",
    "        if custom_result['results']:\n",
    "            print(f\"\\n🥇 Melhor resultado (score: {custom_result['results'][0]['score']:.3f}):\")\n",
    "            print(custom_result['results'][0]['text'][:150] + \"...\")\n",
    "    else:\n",
    "        print(f\"❌ Erro: {custom_result.get('message', 'Desconhecido')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\\n✨ Testes concluídos!\")\n",
    "\n",
    "# Executa teste\n",
    "if __name__ == \"__main__\":\n",
    "    test_processa_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Exportação de Funções para API\n",
    "\n",
    "As funções abaixo são exportadas para uso no notebook `rest-api.ipynb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções públicas para API\n",
    "__all__ = [\n",
    "    'get_processa_info_api',\n",
    "    'search_processa_documents',\n",
    "    'ProcessaInfoRAG'\n",
    "]\n",
    "\n",
    "print(\"✅ Notebook RAG Processa Info carregado com sucesso!\")\n",
    "print(\"\\n📌 Funções disponíveis:\")\n",
    "print(\"  - get_processa_info_api(): Obtém informações sobre a Processa\")\n",
    "print(\"  - search_processa_documents(query, limit): Busca genérica em documentos\")\n",
    "print(\"\\n🔗 Para integrar com API, importe este notebook no rest-api.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}