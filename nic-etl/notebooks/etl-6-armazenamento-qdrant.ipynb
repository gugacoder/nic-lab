{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíæ ARMAZENAMENTO QDRANT - Etapa 6/6\n",
    "\n",
    "## üìã O que este notebook faz\n",
    "\n",
    "Este notebook **armazena os embeddings no banco vetorial Qdrant** para busca sem√¢ntica:\n",
    "\n",
    "- üîó **Conecta ao Qdrant** usando URL e API key configurados\n",
    "- üìä **Cria/verifica collection** com dimens√µes e m√©trica COSINE apropriadas\n",
    "- üîÑ **Reindexa√ß√£o at√¥mica** - deleta e insere chunks por arquivo de forma consistente\n",
    "- üè∑Ô∏è **Metadados completos** - inclui repo, branch, commit, versionamento e hashes\n",
    "- üÜî **IDs determin√≠sticos** - garante que re-execu√ß√µes n√£o criem duplicatas\n",
    "\n",
    "## üéØ Configura√ß√£o necess√°ria\n",
    "\n",
    "- `QDRANT_URL` - URL do servidor Qdrant (padr√£o: http://qdrant.codrstudio.dev:6333)\n",
    "- `QDRANT_API_KEY` - Chave de acesso (obrigat√≥rio)\n",
    "- `QDRANT_COLLECTION` - Nome da collection (padr√£o: \"nic\")\n",
    "\n",
    "## üîß Funcionalidades principais\n",
    "\n",
    "- **Modo FULL**: Reindexa√ß√£o completa por arquivo (DELETE + UPSERT)\n",
    "- **Versionamento**: Controle de vers√µes de modelo e tokenizador\n",
    "- **Metadados ricos**: Rastreabilidade completa do GitLab ao Qdrant\n",
    "- **IDs consistentes**: Baseados em hash determin√≠stico para evitar duplicatas\n",
    "\n",
    "## üìä Output esperado\n",
    "\n",
    "Todos os chunks (~200-400) inseridos no Qdrant com metadados completos para busca sem√¢ntica eficiente.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configura√ß√£o e Conex√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T11:53:56.702146Z",
     "iopub.status.busy": "2025-08-18T11:53:56.701833Z",
     "iopub.status.idle": "2025-08-18T11:53:57.510180Z",
     "shell.execute_reply": "2025-08-18T11:53:57.509392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Commit lido do contexto: e9c8a430\n",
      "Qdrant URL: http://qdrant.codrstudio.dev:6333\n",
      "Collection: nic\n",
      "Repository: nic/documentacao/base-de-conhecimento\n",
      "Branch: main\n",
      "API Key: ***d857\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    Distance, VectorParams, PointStruct,\n",
    "    Filter, FieldCondition, MatchValue, FilterSelector\n",
    ")\n",
    "import time\n",
    "\n",
    "# Marcar in√≠cio da execu√ß√£o\n",
    "stage_start = time.time()\n",
    "start_timestamp = datetime.now().isoformat() + \"Z\"\n",
    "\n",
    "# Ler commit do contexto salvo pelo notebook 2\n",
    "try:\n",
    "    with open(\"pipeline-data/context.json\", \"r\") as f:\n",
    "        context = json.load(f)\n",
    "    GITLAB_COMMIT = context[\"gitlab_commit\"]\n",
    "    print(f\"üìç Commit lido do contexto: {GITLAB_COMMIT[:8]}\")\n",
    "except:\n",
    "    GITLAB_COMMIT = \"unknown\"\n",
    "    print(\"‚ö†Ô∏è Contexto n√£o encontrado, usando commit 'unknown'\")\n",
    "\n",
    "# Configura√ß√£o\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\", \"http://qdrant.codrstudio.dev:6333\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "COLLECTION_NAME = os.getenv(\"QDRANT_COLLECTION\", \"nic\")\n",
    "\n",
    "# Configura√ß√£o do GitLab (para metadados)\n",
    "GITLAB_REPO = \"nic/documentacao/base-de-conhecimento\"\n",
    "GITLAB_BRANCH = os.getenv(\"GITLAB_BRANCH\", \"main\")\n",
    "\n",
    "# Versionamento\n",
    "EMBED_MODEL_MAJOR = \"v1\"  # Vers√£o major do modelo\n",
    "EMBED_MODEL_FULL = \"BAAI/bge-m3\"\n",
    "TOKENIZER_MAJOR = \"v1\"\n",
    "TOKENIZER_FULL = \"RecursiveCharacterTextSplitter-500-100\"\n",
    "\n",
    "if not QDRANT_API_KEY:\n",
    "    raise ValueError(\"QDRANT_API_KEY √© obrigat√≥rio\")\n",
    "\n",
    "# Diret√≥rios\n",
    "embeddings_dir = Path(\"pipeline-data/embeddings\")\n",
    "\n",
    "print(f\"Qdrant URL: {QDRANT_URL}\")\n",
    "print(f\"Collection: {COLLECTION_NAME}\")\n",
    "print(f\"Repository: {GITLAB_REPO}\")\n",
    "print(f\"Branch: {GITLAB_BRANCH}\")\n",
    "print(f\"API Key: ***{QDRANT_API_KEY[-4:] if len(QDRANT_API_KEY) > 4 else '***'}\")\n",
    "\n",
    "def calculate_content_hash(text: str) -> str:\n",
    "    \"\"\"Calcula hash SHA256 do conte√∫do\"\"\"\n",
    "    return hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "def generate_deterministic_id(repo: str, relpath: str, chunk_index: int, \n",
    "                            tokenizer_major: str, embed_model_major: str) -> str:\n",
    "    \"\"\"Gera ID determin√≠stico para pontos do Qdrant\"\"\"\n",
    "    # Combinar todos os elementos que tornam o chunk √∫nico\n",
    "    id_string = f\"{repo}:{relpath}:{chunk_index}:{tokenizer_major}:{embed_model_major}\"\n",
    "    \n",
    "    # Usar hash para gerar ID num√©rico determin√≠stico\n",
    "    hash_object = hashlib.sha256(id_string.encode('utf-8'))\n",
    "    # Converter para inteiro usando os primeiros 8 bytes do hash\n",
    "    return int.from_bytes(hash_object.digest()[:8], byteorder='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T11:53:57.540001Z",
     "iopub.status.busy": "2025-08-18T11:53:57.539592Z",
     "iopub.status.idle": "2025-08-18T11:53:57.785488Z",
     "shell.execute_reply": "2025-08-18T11:53:57.784910Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1096035/3416817368.py:2: UserWarning: Api key is used with an insecure connection.\n",
      "  client = QdrantClient(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conectado ao Qdrant\n",
      "Collections existentes: 3\n",
      "  - documents\n",
      "  - nic_storage\n",
      "  - nic\n"
     ]
    }
   ],
   "source": [
    "# Conectar ao Qdrant\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY\n",
    ")\n",
    "\n",
    "# Verificar conex√£o\n",
    "collections = client.get_collections()\n",
    "print(f\"‚úÖ Conectado ao Qdrant\")\n",
    "print(f\"Collections existentes: {len(collections.collections)}\")\n",
    "\n",
    "for col in collections.collections:\n",
    "    print(f\"  - {col.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Prepara√ß√£o de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T11:53:57.787711Z",
     "iopub.status.busy": "2025-08-18T11:53:57.787566Z",
     "iopub.status.idle": "2025-08-18T11:53:57.901502Z",
     "shell.execute_reply": "2025-08-18T11:53:57.900941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings carregados: 322\n",
      "Dimens√µes do embedding: 1024\n",
      "‚úÖ Metadados adicionados a todos os embeddings\n",
      "\n",
      "Exemplo de metadados:\n",
      "  repo: nic/documentacao/base-de-conhecimento\n",
      "  branch: main\n",
      "  relpath: 30-Aprovados/Mapas/Vis√£o Geral do Self Checkout\n",
      "  commit: e9c8a430b8bc05c306cc8fb342f42e7b45a18744\n",
      "  last_updated: 2025-08-18T11:53:57.896605\n",
      "  embed_model_major: v1\n",
      "  embed_model_full: BAAI/bge-m3\n",
      "  tokenizer_major: v1\n",
      "  tokenizer_full: RecursiveCharacterTextSplitter-500-100\n",
      "  content_sha256: 369066d281dea4e27277770c635a21cc61edf93e0a73a18d5ef5b26f59ce3edf\n"
     ]
    }
   ],
   "source": [
    "# Carregar embeddings\n",
    "embeddings_file = embeddings_dir / \"embeddings.jsonl\"\n",
    "\n",
    "if not embeddings_file.exists():\n",
    "    raise FileNotFoundError(f\"Arquivo de embeddings n√£o encontrado: {embeddings_file}\")\n",
    "\n",
    "embeddings_data = []\n",
    "with open(embeddings_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        embeddings_data.append(data)\n",
    "\n",
    "print(f\"Embeddings carregados: {len(embeddings_data)}\")\n",
    "\n",
    "# Verificar dimens√µes\n",
    "if embeddings_data:\n",
    "    embedding_dim = len(embeddings_data[0][\"embedding\"])\n",
    "    print(f\"Dimens√µes do embedding: {embedding_dim}\")\n",
    "else:\n",
    "    raise ValueError(\"Nenhum embedding encontrado\")\n",
    "\n",
    "# Enriquecer com metadados completos\n",
    "timestamp_now = datetime.now().isoformat()\n",
    "\n",
    "for embedding in embeddings_data:\n",
    "    # Adicionar metadados expandidos\n",
    "    embedding[\"metadata\"] = {\n",
    "        # Identifica√ß√£o do reposit√≥rio\n",
    "        \"repo\": GITLAB_REPO,\n",
    "        \"branch\": GITLAB_BRANCH,\n",
    "        \"relpath\": embedding[\"source_document\"],  # Caminho relativo\n",
    "        \"commit\": GITLAB_COMMIT,\n",
    "        \"last_updated\": timestamp_now,\n",
    "        \n",
    "        # Versionamento de embeddings\n",
    "        \"embed_model_major\": EMBED_MODEL_MAJOR,\n",
    "        \"embed_model_full\": EMBED_MODEL_FULL,\n",
    "        \"tokenizer_major\": TOKENIZER_MAJOR,\n",
    "        \"tokenizer_full\": TOKENIZER_FULL,\n",
    "        \n",
    "        # Hashes para valida√ß√£o\n",
    "        \"content_sha256\": calculate_content_hash(embedding[\"text\"]),\n",
    "        # doc_sha256 seria o hash do documento completo (implementar se necess√°rio)\n",
    "        \n",
    "        # Outros metadados\n",
    "        \"lang\": \"pt-BR\",  # Portugu√™s do Brasil\n",
    "        \"processing_date\": timestamp_now,\n",
    "        \"pipeline_version\": \"1.0.0\"\n",
    "    }\n",
    "\n",
    "print(f\"‚úÖ Metadados adicionados a todos os embeddings\")\n",
    "\n",
    "# Exemplo de metadados\n",
    "if embeddings_data:\n",
    "    print(f\"\\nExemplo de metadados:\")\n",
    "    sample = embeddings_data[0][\"metadata\"]\n",
    "    for key, value in sample.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "        if len(str(value)) > 50:  # Truncar valores longos\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T11:53:57.903887Z",
     "iopub.status.busy": "2025-08-18T11:53:57.903753Z",
     "iopub.status.idle": "2025-08-18T11:53:57.914724Z",
     "shell.execute_reply": "2025-08-18T11:53:57.914305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Collection 'nic' j√° existe:\n",
      "  Pontos: 644\n",
      "  Status: green\n",
      "\n",
      "üîÑ Modo FULL: Reindexa√ß√£o completa ser√° realizada\n",
      "  Arquivos existentes ser√£o substitu√≠dos atomicamente\n",
      "‚úÖ Collection 'nic' verificada\n"
     ]
    }
   ],
   "source": [
    "# Verificar/criar collection\n",
    "collection_exists = False\n",
    "try:\n",
    "    collection_info = client.get_collection(COLLECTION_NAME)\n",
    "    collection_exists = True\n",
    "    print(f\"\\nüìä Collection '{COLLECTION_NAME}' j√° existe:\")\n",
    "    print(f\"  Pontos: {collection_info.points_count}\")\n",
    "    print(f\"  Status: {collection_info.status}\")\n",
    "    \n",
    "    # Em modo FULL, vamos reindexar tudo\n",
    "    print(f\"\\nüîÑ Modo FULL: Reindexa√ß√£o completa ser√° realizada\")\n",
    "    print(f\"  Arquivos existentes ser√£o substitu√≠dos atomicamente\")\n",
    "    \n",
    "except Exception:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' n√£o existe, criando...\")\n",
    "\n",
    "if not collection_exists:\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(\n",
    "            size=embedding_dim,\n",
    "            distance=Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "    print(f\"‚úÖ Collection '{COLLECTION_NAME}' criada\")\n",
    "    print(f\"  Dimens√µes: {embedding_dim}\")\n",
    "    print(f\"  Dist√¢ncia: COSINE\")\n",
    "else:\n",
    "    print(f\"‚úÖ Collection '{COLLECTION_NAME}' verificada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T11:53:57.916420Z",
     "iopub.status.busy": "2025-08-18T11:53:57.916251Z",
     "iopub.status.idle": "2025-08-18T11:53:57.923769Z",
     "shell.execute_reply": "2025-08-18T11:53:57.923374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Arquivos √∫nicos: 31\n",
      "üìä Total de chunks: 322\n",
      "\n",
      "üìà Distribui√ß√£o de chunks por arquivo:\n",
      "  30-Aprovados/Mapas/Vis√£o Geral do Self Checkout: 51 chunks\n",
      "  30-Aprovados/T√≥picos/Componentes principais do sistema: 19 chunks\n",
      "  30-Aprovados/T√≥picos/Pr√©-requisitos t√©cnicos: 17 chunks\n",
      "  30-Aprovados/Mapas/Vis√£o Geral do NIC: 15 chunks\n",
      "  30-Aprovados/T√≥picos/Padr√µes de Documenta√ß√£o do NIC: 15 chunks\n",
      "  ... e mais 26 arquivos\n",
      "\n",
      "üß™ Preparando reindexa√ß√£o:\n",
      "  Exemplo ID determin√≠stico: 4458554930477711609\n",
      "  Arquivo de teste: 30-Aprovados/Mapas/Vis√£o Geral do Self Checkout\n"
     ]
    }
   ],
   "source": [
    "# Agrupar embeddings por arquivo para reindexa√ß√£o at√¥mica\n",
    "chunks_by_file = defaultdict(list)\n",
    "\n",
    "for embedding_data in embeddings_data:\n",
    "    source_doc = embedding_data[\"source_document\"]\n",
    "    chunks_by_file[source_doc].append(embedding_data)\n",
    "\n",
    "print(f\"üìÅ Arquivos √∫nicos: {len(chunks_by_file)}\")\n",
    "print(f\"üìä Total de chunks: {sum(len(chunks) for chunks in chunks_by_file.values())}\")\n",
    "print(f\"\\nüìà Distribui√ß√£o de chunks por arquivo:\")\n",
    "\n",
    "# Mostrar estat√≠sticas\n",
    "for doc, chunks in sorted(chunks_by_file.items(), \n",
    "                          key=lambda x: len(x[1]), reverse=True)[:5]:\n",
    "    print(f\"  {doc}: {len(chunks)} chunks\")\n",
    "    \n",
    "if len(chunks_by_file) > 5:\n",
    "    print(f\"  ... e mais {len(chunks_by_file) - 5} arquivos\")\n",
    "\n",
    "# Fun√ß√£o de reindexa√ß√£o por arquivo\n",
    "def reindex_file(client, collection_name, source_document, chunks_data):\n",
    "    \"\"\"\n",
    "    Reindexa√ß√£o at√¥mica por arquivo com metadados completos.\n",
    "    Implementa a diretriz: DELETE por filtro + UPSERT com IDs determin√≠sticos\n",
    "    \"\"\"\n",
    "    # PASSO 1: Deletar chunks existentes do arquivo\n",
    "    try:\n",
    "        # Filtro para deletar apenas chunks deste arquivo espec√≠fico\n",
    "        delete_filter = Filter(\n",
    "            must=[\n",
    "                FieldCondition(key=\"repo\", match=MatchValue(value=GITLAB_REPO)),\n",
    "                FieldCondition(key=\"branch\", match=MatchValue(value=GITLAB_BRANCH)),\n",
    "                FieldCondition(key=\"relpath\", match=MatchValue(value=source_document))\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        delete_result = client.delete(\n",
    "            collection_name=collection_name,\n",
    "            points_selector=FilterSelector(filter=delete_filter)\n",
    "        )\n",
    "        # N√£o mostrar mensagem se n√£o deletou nada (primeira execu√ß√£o)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Ignorar erro se n√£o h√° dados para deletar\n",
    "        pass\n",
    "    \n",
    "    # PASSO 2: Preparar novos pontos com IDs determin√≠sticos e metadados completos\n",
    "    points = []\n",
    "    for chunk_data in chunks_data:\n",
    "        # Gerar ID determin√≠stico\n",
    "        point_id = generate_deterministic_id(\n",
    "            repo=GITLAB_REPO,\n",
    "            relpath=chunk_data[\"source_document\"],\n",
    "            chunk_index=chunk_data[\"chunk_index\"],\n",
    "            tokenizer_major=TOKENIZER_MAJOR,\n",
    "            embed_model_major=EMBED_MODEL_MAJOR\n",
    "        )\n",
    "        \n",
    "        # Payload completo com todos os metadados\n",
    "        payload = {\n",
    "            # Dados originais do chunk\n",
    "            \"chunk_id\": chunk_data[\"chunk_id\"],\n",
    "            \"chunk_index\": chunk_data[\"chunk_index\"],\n",
    "            \"text\": chunk_data[\"text\"],\n",
    "            \"char_count\": chunk_data[\"char_count\"],\n",
    "            \n",
    "            # Metadados expandidos (todos os campos da diretriz)\n",
    "            \"repo\": GITLAB_REPO,\n",
    "            \"branch\": GITLAB_BRANCH,\n",
    "            \"relpath\": chunk_data[\"source_document\"],\n",
    "            \"source_document\": chunk_data[\"source_document\"],  # Manter compatibilidade\n",
    "            \"commit\": GITLAB_COMMIT,\n",
    "            \"last_updated\": chunk_data[\"metadata\"][\"last_updated\"],\n",
    "            \n",
    "            # Versionamento\n",
    "            \"embed_model_major\": EMBED_MODEL_MAJOR,\n",
    "            \"embed_model_full\": EMBED_MODEL_FULL,\n",
    "            \"tokenizer_major\": TOKENIZER_MAJOR,\n",
    "            \"tokenizer_full\": TOKENIZER_FULL,\n",
    "            \"embedding_model\": chunk_data[\"embedding_model\"],  # Manter compatibilidade\n",
    "            \n",
    "            # Hashes\n",
    "            \"content_sha256\": chunk_data[\"metadata\"][\"content_sha256\"],\n",
    "            \n",
    "            # Outros\n",
    "            \"lang\": \"pt-BR\",\n",
    "            \"processing_date\": chunk_data[\"metadata\"][\"processing_date\"],\n",
    "            \"pipeline_version\": chunk_data[\"metadata\"][\"pipeline_version\"]\n",
    "        }\n",
    "        \n",
    "        point = PointStruct(\n",
    "            id=point_id,\n",
    "            vector=chunk_data[\"embedding\"],\n",
    "            payload=payload\n",
    "        )\n",
    "        points.append(point)\n",
    "    \n",
    "    # PASSO 3: Upsert (inserir ou atualizar) novos pontos\n",
    "    if points:\n",
    "        client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=points,\n",
    "            wait=True  # Esperar confirma√ß√£o\n",
    "        )\n",
    "    \n",
    "    return len(points)\n",
    "\n",
    "# Teste da fun√ß√£o com um arquivo\n",
    "test_file = list(chunks_by_file.keys())[0]\n",
    "test_chunks = chunks_by_file[test_file][:1]  # Apenas 1 chunk para teste\n",
    "\n",
    "print(f\"\\nüß™ Preparando reindexa√ß√£o:\")\n",
    "print(f\"  Exemplo ID determin√≠stico: {generate_deterministic_id(GITLAB_REPO, test_file, 0, TOKENIZER_MAJOR, EMBED_MODEL_MAJOR)}\")\n",
    "print(f\"  Arquivo de teste: {test_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Reindexa√ß√£o e Armazenamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T11:53:57.925142Z",
     "iopub.status.busy": "2025-08-18T11:53:57.925024Z",
     "iopub.status.idle": "2025-08-18T11:53:58.557829Z",
     "shell.execute_reply": "2025-08-18T11:53:58.557213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ INICIANDO REINDEXA√á√ÉO FULL\n",
      "  Modo: DELETE + UPSERT por arquivo\n",
      "  Repo: nic/documentacao/base-de-conhecimento\n",
      "  Branch: main\n",
      "  Commit: e9c8a430b8bc05c306cc8fb342f42e7b45a18744\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 3.2% - 1/31 arquivos\r",
      "[‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 6.5% - 2/31 arquivos\r",
      "[‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 9.7% - 3/31 arquivos\r",
      "[‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 12.9% - 4/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 16.1% - 5/31 arquivos"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 19.4% - 6/31 arquivos"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 22.6% - 7/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 25.8% - 8/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 29.0% - 9/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 32.3% - 10/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 35.5% - 11/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 38.7% - 12/31 arquivos"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 41.9% - 13/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 45.2% - 14/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 48.4% - 15/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 51.6% - 16/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 54.8% - 17/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 58.1% - 18/31 arquivos"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 61.3% - 19/31 arquivos"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 64.5% - 20/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 67.7% - 21/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 71.0% - 22/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 74.2% - 23/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë] 77.4% - 24/31 arquivos"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë] 80.6% - 25/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë] 83.9% - 26/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë] 87.1% - 27/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 90.3% - 28/31 arquivos\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 93.5% - 29/31 arquivos"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë] 96.8% - 30/31 arquivos"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0% - 31/31 arquivos\n",
      "============================================================\n",
      "‚úÖ REINDEXA√á√ÉO COMPLETA!\n",
      "  üìÅ Arquivos processados: 31\n",
      "  üìä Chunks inseridos: 322\n",
      "  ‚ùå Erros: 0\n",
      "\n",
      "‚úÖ Metadados da inser√ß√£o salvos no contexto do stage\n"
     ]
    }
   ],
   "source": [
    "# Modo FULL: Reindexar todos os arquivos\n",
    "print(f\"üöÄ INICIANDO REINDEXA√á√ÉO FULL\")\n",
    "print(f\"  Modo: DELETE + UPSERT por arquivo\")\n",
    "print(f\"  Repo: {GITLAB_REPO}\")\n",
    "print(f\"  Branch: {GITLAB_BRANCH}\")\n",
    "print(f\"  Commit: {GITLAB_COMMIT}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_inserted = 0\n",
    "total_files = len(chunks_by_file)\n",
    "errors = []\n",
    "\n",
    "for idx, (source_document, file_chunks) in enumerate(chunks_by_file.items(), 1):\n",
    "    try:\n",
    "        # Reindexar arquivo\n",
    "        inserted = reindex_file(\n",
    "            client=client,\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            source_document=source_document,\n",
    "            chunks_data=file_chunks\n",
    "        )\n",
    "        \n",
    "        total_inserted += inserted\n",
    "        \n",
    "        # Progress bar simples\n",
    "        progress = idx / total_files * 100\n",
    "        bar = \"‚ñà\" * int(progress / 5) + \"‚ñë\" * (20 - int(progress / 5))\n",
    "        print(f\"\\r[{bar}] {progress:.1f}% - {idx}/{total_files} arquivos\", end=\"\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors.append((source_document, str(e)))\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ REINDEXA√á√ÉO COMPLETA!\")\n",
    "print(f\"  üìÅ Arquivos processados: {total_files}\")\n",
    "print(f\"  üìä Chunks inseridos: {total_inserted}\")\n",
    "print(f\"  ‚ùå Erros: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\n‚ö†Ô∏è Arquivos com erro:\")\n",
    "    for doc, error in errors[:5]:\n",
    "        print(f\"  {doc}: {error}\")\n",
    "\n",
    "# Salvar metadados da inser√ß√£o no contexto  \n",
    "stage_metadata = {\n",
    "    \"files_processed\": total_files,\n",
    "    \"chunks_inserted\": total_inserted,\n",
    "    \"errors_count\": len(errors),\n",
    "    \"qdrant_collection\": COLLECTION_NAME,\n",
    "    \"reindex_mode\": \"FULL\",\n",
    "    \"gitlab_commit\": GITLAB_COMMIT,\n",
    "    \"pipeline_versions\": {\n",
    "        \"embed_model_major\": EMBED_MODEL_MAJOR,\n",
    "        \"embed_model_full\": EMBED_MODEL_FULL,\n",
    "        \"tokenizer_major\": TOKENIZER_MAJOR,\n",
    "        \"tokenizer_full\": TOKENIZER_FULL\n",
    "    },\n",
    "    \"errors\": errors[:10] if errors else []  # Limitar erros salvos\n",
    "}\n",
    "\n",
    "# Salvar no arquivo de contexto\n",
    "with open(\"pipeline-data/context.json\", \"r\") as f:\n",
    "    context_data = json.load(f)\n",
    "\n",
    "context_data[\"stage_06_armazenamento_qdrant\"] = stage_metadata\n",
    "\n",
    "with open(\"pipeline-data/context.json\", \"w\") as f:\n",
    "    json.dump(context_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Metadados da inser√ß√£o salvos no contexto do stage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Relat√≥rio Final de Execu√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T11:53:58.559773Z",
     "iopub.status.busy": "2025-08-18T11:53:58.559599Z",
     "iopub.status.idle": "2025-08-18T11:53:58.570004Z",
     "shell.execute_reply": "2025-08-18T11:53:58.569446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Relat√≥rio FINAL salvo: pipeline-data/report.json\n",
      "‚è±Ô∏è Dura√ß√£o da etapa: 1.06s\n",
      "‚è±Ô∏è Dura√ß√£o total do pipeline: 100.35s\n",
      "\n",
      "‚úÖ Pipeline Status: SUCCESS\n",
      "üìà Data Flow Validation: PASSED\n",
      "\n",
      "üìã RESUMO DO PIPELINE:\n",
      "  ‚Ä¢ Arquivos baixados: 39\n",
      "  ‚Ä¢ Documentos processados: 31\n",
      "  ‚Ä¢ Chunks criados: 322\n",
      "  ‚Ä¢ Embeddings gerados: 322\n",
      "  ‚Ä¢ Vetores no Qdrant: 322\n"
     ]
    }
   ],
   "source": [
    "# Calcular dura√ß√£o\n",
    "stage_duration = time.time() - stage_start\n",
    "\n",
    "# Carregar relat√≥rio existente\n",
    "report_path = Path(\"pipeline-data/report.json\")\n",
    "if report_path.exists():\n",
    "    with open(report_path, \"r\") as f:\n",
    "        report = json.load(f)\n",
    "else:\n",
    "    report = {\"stages\": [], \"context\": {}, \"summary\": {}}\n",
    "\n",
    "# Adicionar informa√ß√µes finais ao contexto\n",
    "report[\"context\"].update({\n",
    "    \"qdrant_url\": QDRANT_URL,\n",
    "    \"qdrant_collection\": COLLECTION_NAME\n",
    "})\n",
    "\n",
    "# Adicionar informa√ß√µes desta etapa\n",
    "stage_report = {\n",
    "    \"stage\": 6,\n",
    "    \"name\": \"Armazenamento Qdrant\",\n",
    "    \"status\": \"SUCCESS\" if len(errors) == 0 else \"FAILED\",\n",
    "    \"start_time\": start_timestamp,\n",
    "    \"duration_seconds\": round(stage_duration, 2),\n",
    "    \"results\": {\n",
    "        \"embeddings_loaded\": len(embeddings_data),\n",
    "        \"collection_name\": COLLECTION_NAME,\n",
    "        \"collection_exists\": collection_exists,\n",
    "        \"files_processed\": total_files,\n",
    "        \"chunks_inserted\": total_inserted,\n",
    "        \"insertion_errors\": len(errors),\n",
    "        \"reindex_mode\": \"FULL\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Se houve erros, adicionar detalhes\n",
    "if errors:\n",
    "    stage_report[\"errors\"] = [{\"file\": doc, \"error\": err} for doc, err in errors[:5]]\n",
    "\n",
    "# Adicionar ou atualizar stage no relat√≥rio\n",
    "stages_updated = False\n",
    "for i, stage in enumerate(report[\"stages\"]):\n",
    "    if stage[\"stage\"] == 6:\n",
    "        report[\"stages\"][i] = stage_report\n",
    "        stages_updated = True\n",
    "        break\n",
    "\n",
    "if not stages_updated:\n",
    "    report[\"stages\"].append(stage_report)\n",
    "\n",
    "# Calcular summary final\n",
    "total_duration = sum(stage.get(\"duration_seconds\", 0) for stage in report[\"stages\"])\n",
    "failed_stages = [stage[\"name\"] for stage in report[\"stages\"] if stage[\"status\"] == \"FAILED\"]\n",
    "\n",
    "# Obter totais das etapas\n",
    "input_files = next((s[\"results\"][\"files_downloaded\"] for s in report[\"stages\"] if s[\"stage\"] == 2), 0)\n",
    "processed_docs = next((s[\"results\"][\"total_processed\"] for s in report[\"stages\"] if s[\"stage\"] == 3), 0)\n",
    "total_chunks = next((s[\"results\"][\"total_chunks\"] for s in report[\"stages\"] if s[\"stage\"] == 4), 0)\n",
    "embeddings_gen = next((s[\"results\"][\"embeddings_generated\"] for s in report[\"stages\"] if s[\"stage\"] == 5), 0)\n",
    "qdrant_vectors = next((s[\"results\"][\"chunks_inserted\"] for s in report[\"stages\"] if s[\"stage\"] == 6), 0)\n",
    "\n",
    "# Valida√ß√£o do fluxo de dados\n",
    "chunks_to_embeddings = \"PASSED\" if total_chunks == embeddings_gen else \"FAILED\"\n",
    "embeddings_to_qdrant = \"PASSED\" if embeddings_gen == qdrant_vectors else \"FAILED\"\n",
    "\n",
    "# Atualizar summary\n",
    "report[\"summary\"] = {\n",
    "    \"pipeline_status\": \"SUCCESS\" if len(failed_stages) == 0 else \"FAILED\",\n",
    "    \"total_duration_seconds\": round(total_duration, 2),\n",
    "    \"last_update\": datetime.now().isoformat() + \"Z\",\n",
    "    \"data_flow\": {\n",
    "        \"input_files\": input_files,\n",
    "        \"processed_documents\": processed_docs,\n",
    "        \"total_chunks\": total_chunks,\n",
    "        \"embeddings_generated\": embeddings_gen,\n",
    "        \"vectors_stored\": qdrant_vectors\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"chunks_to_embeddings\": chunks_to_embeddings,\n",
    "        \"embeddings_to_qdrant\": embeddings_to_qdrant,\n",
    "        \"overall\": \"PASSED\" if chunks_to_embeddings == \"PASSED\" and embeddings_to_qdrant == \"PASSED\" else \"FAILED\"\n",
    "    }\n",
    "}\n",
    "\n",
    "if failed_stages:\n",
    "    report[\"summary\"][\"failed_stages\"] = failed_stages\n",
    "\n",
    "# Atualizar last_execution em pipeline_info\n",
    "if \"pipeline_info\" in report:\n",
    "    report[\"pipeline_info\"][\"last_execution\"] = report[\"summary\"][\"last_update\"]\n",
    "\n",
    "# Salvar relat√≥rio final\n",
    "with open(report_path, \"w\") as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"üìä Relat√≥rio FINAL salvo: {report_path}\")\n",
    "print(f\"‚è±Ô∏è Dura√ß√£o da etapa: {stage_duration:.2f}s\")\n",
    "print(f\"‚è±Ô∏è Dura√ß√£o total do pipeline: {total_duration:.2f}s\")\n",
    "print(f\"\\n‚úÖ Pipeline Status: {report['summary']['pipeline_status']}\")\n",
    "print(f\"üìà Data Flow Validation: {report['summary']['validation']['overall']}\")\n",
    "\n",
    "# Mostrar resumo\n",
    "print(f\"\\nüìã RESUMO DO PIPELINE:\")\n",
    "print(f\"  ‚Ä¢ Arquivos baixados: {input_files}\")\n",
    "print(f\"  ‚Ä¢ Documentos processados: {processed_docs}\")\n",
    "print(f\"  ‚Ä¢ Chunks criados: {total_chunks}\")\n",
    "print(f\"  ‚Ä¢ Embeddings gerados: {embeddings_gen}\")\n",
    "print(f\"  ‚Ä¢ Vetores no Qdrant: {qdrant_vectors}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
