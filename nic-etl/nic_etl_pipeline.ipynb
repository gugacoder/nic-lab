{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIC ETL Pipeline\n",
    "Complete document processing pipeline from GitLab repository ingestion to Qdrant vector database storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment Configuration and Constants\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from enum import Enum\n",
    "\n",
    "class Environment(Enum):\n",
    "    DEVELOPMENT = \"development\"\n",
    "    STAGING = \"staging\"\n",
    "    PRODUCTION = \"production\"\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, env_file: Optional[str] = None):\n",
    "        self.env_file = env_file or \".env\"\n",
    "        self.environment = self._detect_environment()\n",
    "        \n",
    "        # Load environment variables\n",
    "        self._load_environment_variables()\n",
    "        \n",
    "        # Initialize configuration\n",
    "        self.config = self._build_configuration()\n",
    "        \n",
    "        # Validate configuration\n",
    "        self._validate_configuration()\n",
    "    \n",
    "    def _detect_environment(self) -> Environment:\n",
    "        \"\"\"Detect current environment\"\"\"\n",
    "        env_name = os.getenv('NIC_ENVIRONMENT', 'development').lower()\n",
    "        \n",
    "        try:\n",
    "            return Environment(env_name)\n",
    "        except ValueError:\n",
    "            print(f\"Unknown environment '{env_name}', defaulting to development\")\n",
    "            return Environment.DEVELOPMENT\n",
    "    \n",
    "    def _load_environment_variables(self):\n",
    "        \"\"\"Load environment variables from .env file\"\"\"\n",
    "        env_path = Path(self.env_file)\n",
    "        \n",
    "        if env_path.exists():\n",
    "            load_dotenv(env_path)\n",
    "            print(f\"Loaded configuration from {env_path}\")\n",
    "        else:\n",
    "            print(f\"No .env file found at {env_path}, using environment variables and defaults\")\n",
    "    \n",
    "    def _build_configuration(self) -> Dict[str, Any]:\n",
    "        \"\"\"Build complete configuration from multiple sources\"\"\"\n",
    "        config = {\n",
    "            'environment': self.environment.value,\n",
    "            \n",
    "            # GitLab Configuration\n",
    "            'gitlab': {\n",
    "                'url': os.getenv('GITLAB_URL', 'http://gitlab.processa.info'),\n",
    "                'token': os.getenv('GITLAB_TOKEN', 'glpat-zycwWRydKE53SHxxpfbN'),\n",
    "                'project': os.getenv('GITLAB_PROJECT', 'nic/documentacao/base-de-conhecimento'),\n",
    "                'branch': os.getenv('GITLAB_BRANCH', 'main'),\n",
    "                'folder': os.getenv('GITLAB_FOLDER', '30-Aprovados')\n",
    "            },\n",
    "            \n",
    "            # Qdrant Configuration\n",
    "            'qdrant': {\n",
    "                'url': os.getenv('QDRANT_URL', 'https://qdrant.codrstudio.dev/'),\n",
    "                'api_key': os.getenv('QDRANT_API_KEY', '93f0c9d6b9a53758f2376decf318b3ae300e9bdb50be2d0e9c893ee4469fd857'),\n",
    "                'collection': os.getenv('QDRANT_COLLECTION', 'nic')\n",
    "            },\n",
    "            \n",
    "            # Processing Configuration\n",
    "            'docling': {\n",
    "                'enable_ocr': os.getenv('DOCLING_ENABLE_OCR', 'true').lower() == 'true',\n",
    "                'ocr_languages': os.getenv('DOCLING_OCR_LANGUAGES', 'pt,en').split(','),\n",
    "                'confidence_threshold': float(os.getenv('DOCLING_CONFIDENCE_THRESHOLD', '0.75'))\n",
    "            },\n",
    "            \n",
    "            # Chunking Configuration\n",
    "            'chunking': {\n",
    "                'size': int(os.getenv('CHUNK_SIZE', '500')),\n",
    "                'overlap': int(os.getenv('CHUNK_OVERLAP', '100')),\n",
    "                'min_size': int(os.getenv('CHUNK_MIN_SIZE', '100'))\n",
    "            },\n",
    "            \n",
    "            # Embedding Configuration\n",
    "            'embedding': {\n",
    "                'model': os.getenv('EMBEDDING_MODEL', 'BAAI/bge-m3'),\n",
    "                'batch_size': int(os.getenv('EMBEDDING_BATCH_SIZE', '32')),\n",
    "                'device': os.getenv('EMBEDDING_DEVICE', 'cpu'),\n",
    "                'normalize': os.getenv('EMBEDDING_NORMALIZE', 'true').lower() == 'true'\n",
    "            },\n",
    "            \n",
    "            # Cache Configuration\n",
    "            'cache': {\n",
    "                'dir': Path(os.getenv('CACHE_DIR', './cache')),\n",
    "                'state_file': Path(os.getenv('CACHE_STATE_FILE', './cache/pipeline_state.json')),\n",
    "                'max_size_gb': float(os.getenv('CACHE_MAX_SIZE_GB', '10')),\n",
    "                'cleanup_interval_hours': int(os.getenv('CACHE_CLEANUP_INTERVAL', '24'))\n",
    "            },\n",
    "            \n",
    "            # Logging Configuration\n",
    "            'logging': {\n",
    "                'level': os.getenv('LOG_LEVEL', 'INFO'),\n",
    "                'file': os.getenv('LOG_FILE', './logs/nic_etl.log'),\n",
    "                'max_size_mb': int(os.getenv('LOG_MAX_SIZE_MB', '100')),\n",
    "                'backup_count': int(os.getenv('LOG_BACKUP_COUNT', '5'))\n",
    "            },\n",
    "            \n",
    "            # Performance Configuration\n",
    "            'performance': {\n",
    "                'max_workers': int(os.getenv('MAX_WORKERS', '4')),\n",
    "                'timeout_seconds': int(os.getenv('TIMEOUT_SECONDS', '300')),\n",
    "                'retry_attempts': int(os.getenv('RETRY_ATTEMPTS', '3')),\n",
    "                'batch_size': int(os.getenv('BATCH_SIZE', '100'))\n",
    "            },\n",
    "            \n",
    "            # Feature Flags\n",
    "            'features': {\n",
    "                'enable_caching': os.getenv('FEATURE_ENABLE_CACHING', 'true').lower() == 'true',\n",
    "                'enable_parallel_processing': os.getenv('FEATURE_PARALLEL_PROCESSING', 'true').lower() == 'true',\n",
    "                'enable_quality_checks': os.getenv('FEATURE_QUALITY_CHECKS', 'true').lower() == 'true',\n",
    "                'enable_metrics': os.getenv('FEATURE_METRICS', 'true').lower() == 'true'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Environment-specific overrides\n",
    "        config = self._apply_environment_overrides(config)\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def _apply_environment_overrides(self, config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Apply environment-specific configuration overrides\"\"\"\n",
    "        \n",
    "        if self.environment == Environment.DEVELOPMENT:\n",
    "            # Development overrides\n",
    "            config['logging']['level'] = 'DEBUG'\n",
    "            config['embedding']['batch_size'] = 8  # Smaller batches for development\n",
    "            config['performance']['max_workers'] = 2\n",
    "            config['cache']['max_size_gb'] = 2\n",
    "            \n",
    "        elif self.environment == Environment.STAGING:\n",
    "            # Staging overrides\n",
    "            config['logging']['level'] = 'INFO'\n",
    "            config['embedding']['batch_size'] = 16\n",
    "            config['performance']['max_workers'] = 4\n",
    "            config['features']['enable_quality_checks'] = True\n",
    "            \n",
    "        elif self.environment == Environment.PRODUCTION:\n",
    "            # Production overrides\n",
    "            config['logging']['level'] = 'WARNING'\n",
    "            config['embedding']['batch_size'] = 32\n",
    "            config['performance']['max_workers'] = 8\n",
    "            config['performance']['timeout_seconds'] = 600\n",
    "            config['features']['enable_metrics'] = True\n",
    "            \n",
    "            # Production security settings\n",
    "            if not config['gitlab']['token'] or config['gitlab']['token'].startswith('glpat-'):\n",
    "                print(\"WARNING: Using development token in production\")\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def _validate_configuration(self):\n",
    "        \"\"\"Validate configuration values\"\"\"\n",
    "        validation_errors = []\n",
    "        \n",
    "        # Validate required fields\n",
    "        required_fields = [\n",
    "            ('gitlab.url', self.config['gitlab']['url']),\n",
    "            ('gitlab.token', self.config['gitlab']['token']),\n",
    "            ('qdrant.url', self.config['qdrant']['url']),\n",
    "            ('qdrant.api_key', self.config['qdrant']['api_key'])\n",
    "        ]\n",
    "        \n",
    "        for field_name, value in required_fields:\n",
    "            if not value or value.strip() == '':\n",
    "                validation_errors.append(f\"Required field '{field_name}' is empty\")\n",
    "        \n",
    "        # Validate numeric ranges\n",
    "        if self.config['chunking']['size'] <= 0:\n",
    "            validation_errors.append(\"Chunk size must be positive\")\n",
    "        \n",
    "        if self.config['chunking']['overlap'] >= self.config['chunking']['size']:\n",
    "            validation_errors.append(\"Chunk overlap must be less than chunk size\")\n",
    "        \n",
    "        if self.config['embedding']['batch_size'] <= 0:\n",
    "            validation_errors.append(\"Embedding batch size must be positive\")\n",
    "        \n",
    "        # Validate paths\n",
    "        cache_dir = self.config['cache']['dir']\n",
    "        if not cache_dir.parent.exists():\n",
    "            validation_errors.append(f\"Cache directory parent does not exist: {cache_dir.parent}\")\n",
    "        \n",
    "        # Report validation errors\n",
    "        if validation_errors:\n",
    "            print(\"Configuration validation errors:\")\n",
    "            for error in validation_errors:\n",
    "                print(f\"  - {error}\")\n",
    "            raise ValueError(f\"Configuration validation failed: {len(validation_errors)} errors\")\n",
    "        \n",
    "        print(f\"Configuration validation passed for {self.environment.value} environment\")\n",
    "    \n",
    "    def get(self, key: str, default: Any = None) -> Any:\n",
    "        \"\"\"Get configuration value using dot notation\"\"\"\n",
    "        keys = key.split('.')\n",
    "        value = self.config\n",
    "        \n",
    "        for k in keys:\n",
    "            if isinstance(value, dict) and k in value:\n",
    "                value = value[k]\n",
    "            else:\n",
    "                return default\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def set(self, key: str, value: Any):\n",
    "        \"\"\"Set configuration value using dot notation\"\"\"\n",
    "        keys = key.split('.')\n",
    "        config = self.config\n",
    "        \n",
    "        for k in keys[:-1]:\n",
    "            if k not in config:\n",
    "                config[k] = {}\n",
    "            config = config[k]\n",
    "        \n",
    "        config[keys[-1]] = value\n",
    "    \n",
    "    def update_runtime_config(self, updates: Dict[str, Any]):\n",
    "        \"\"\"Update configuration at runtime\"\"\"\n",
    "        for key, value in updates.items():\n",
    "            self.set(key, value)\n",
    "        \n",
    "        # Re-validate after updates\n",
    "        self._validate_configuration()\n",
    "    \n",
    "    def export_config(self, include_secrets: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"Export configuration (optionally excluding secrets)\"\"\"\n",
    "        config_copy = self.config.copy()\n",
    "        \n",
    "        if not include_secrets:\n",
    "            # Mask sensitive values\n",
    "            sensitive_keys = [\n",
    "                'gitlab.token',\n",
    "                'qdrant.api_key'\n",
    "            ]\n",
    "            \n",
    "            for key in sensitive_keys:\n",
    "                value = self.get(key)\n",
    "                if value:\n",
    "                    self.set(key, value[:8] + '***' if len(value) > 8 else '***')\n",
    "        \n",
    "        return config_copy\n",
    "    \n",
    "    def create_env_template(self, output_path: str = '.env.template'):\n",
    "        \"\"\"Create .env template file with all configuration options\"\"\"\n",
    "        template_content = '''# NIC ETL Pipeline Configuration\n",
    "# Copy this file to .env and update values as needed\n",
    "\n",
    "# Environment (development, staging, production)\n",
    "NIC_ENVIRONMENT=development\n",
    "\n",
    "# GitLab Configuration\n",
    "GITLAB_URL=http://gitlab.processa.info\n",
    "GITLAB_TOKEN=your_gitlab_token_here\n",
    "GITLAB_PROJECT=nic/documentacao/base-de-conhecimento\n",
    "GITLAB_BRANCH=main\n",
    "GITLAB_FOLDER=30-Aprovados\n",
    "\n",
    "# Qdrant Configuration\n",
    "QDRANT_URL=https://qdrant.codrstudio.dev/\n",
    "QDRANT_API_KEY=your_qdrant_api_key_here\n",
    "QDRANT_COLLECTION=nic\n",
    "\n",
    "# Processing Configuration\n",
    "DOCLING_ENABLE_OCR=true\n",
    "DOCLING_OCR_LANGUAGES=pt,en\n",
    "DOCLING_CONFIDENCE_THRESHOLD=0.75\n",
    "\n",
    "# Chunking Configuration\n",
    "CHUNK_SIZE=500\n",
    "CHUNK_OVERLAP=100\n",
    "CHUNK_MIN_SIZE=100\n",
    "\n",
    "# Embedding Configuration\n",
    "EMBEDDING_MODEL=BAAI/bge-m3\n",
    "EMBEDDING_BATCH_SIZE=32\n",
    "EMBEDDING_DEVICE=cpu\n",
    "EMBEDDING_NORMALIZE=true\n",
    "\n",
    "# Cache Configuration\n",
    "CACHE_DIR=./cache\n",
    "CACHE_STATE_FILE=./cache/pipeline_state.json\n",
    "CACHE_MAX_SIZE_GB=10\n",
    "CACHE_CLEANUP_INTERVAL=24\n",
    "\n",
    "# Logging Configuration\n",
    "LOG_LEVEL=INFO\n",
    "LOG_FILE=./logs/nic_etl.log\n",
    "LOG_MAX_SIZE_MB=100\n",
    "LOG_BACKUP_COUNT=5\n",
    "\n",
    "# Performance Configuration\n",
    "MAX_WORKERS=4\n",
    "TIMEOUT_SECONDS=300\n",
    "RETRY_ATTEMPTS=3\n",
    "BATCH_SIZE=100\n",
    "\n",
    "# Feature Flags\n",
    "FEATURE_ENABLE_CACHING=true\n",
    "FEATURE_PARALLEL_PROCESSING=true\n",
    "FEATURE_QUALITY_CHECKS=true\n",
    "FEATURE_METRICS=true\n",
    "'''\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(template_content)\n",
    "        \n",
    "        print(f\"Environment template created at {output_path}\")\n",
    "\n",
    "# Initialize global configuration\n",
    "config_manager = ConfigurationManager()\n",
    "CONFIG = config_manager.config\n",
    "\n",
    "# Export commonly used constants\n",
    "GITLAB_URL = CONFIG['gitlab']['url']\n",
    "GITLAB_TOKEN = CONFIG['gitlab']['token']\n",
    "GITLAB_PROJECT = CONFIG['gitlab']['project']\n",
    "GITLAB_BRANCH = CONFIG['gitlab']['branch']\n",
    "GITLAB_FOLDER = CONFIG['gitlab']['folder']\n",
    "\n",
    "QDRANT_URL = CONFIG['qdrant']['url']\n",
    "QDRANT_API_KEY = CONFIG['qdrant']['api_key']\n",
    "QDRANT_COLLECTION = CONFIG['qdrant']['collection']\n",
    "\n",
    "CACHE_DIR = CONFIG['cache']['dir']\n",
    "STATE_FILE = CONFIG['cache']['state_file']\n",
    "\n",
    "# Ensure cache directory exists\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Print configuration summary\n",
    "print(f\"\\n=== NIC ETL Configuration ===\")\n",
    "print(f\"Environment: {CONFIG['environment']}\")\n",
    "print(f\"GitLab: {CONFIG['gitlab']['url']}\")\n",
    "print(f\"Qdrant: {CONFIG['qdrant']['url']}\")\n",
    "print(f\"Cache: {CONFIG['cache']['dir']}\")\n",
    "print(f\"Embedding Model: {CONFIG['embedding']['model']}\")\n",
    "print(f\"Chunk Size: {CONFIG['chunking']['size']} tokens\")\n",
    "print(f\"Features: {', '.join([k for k, v in CONFIG['features'].items() if v])}\")\n",
    "print(f\"================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Dependencies Installation and Imports\nimport subprocess\nimport sys\nimport importlib\nimport pkg_resources\nfrom packaging import version\nimport warnings\nfrom typing import List, Dict, Tuple, Optional\n\nclass DependencyManager:\n    \"\"\"Manages package dependencies and imports for the NIC ETL pipeline\"\"\"\n    \n    def __init__(self):\n        self.required_packages = {\n            # Core dependencies\n            'requests': '>=2.28.0',\n            'python-dotenv': '>=0.19.0',\n            'pathlib': None,  # Built-in\n            'json': None,     # Built-in\n            \n            # GitLab integration\n            'python-gitlab': '>=3.0.0',\n            'urllib3': '>=1.26.0',\n            \n            # Document processing\n            'docling': '>=1.0.0',\n            'pypdf': '>=3.0.0',\n            'python-docx': '>=0.8.11',\n            'pillow': '>=9.0.0',\n            \n            # Text processing and embeddings\n            'transformers': '>=4.21.0',\n            'torch': '>=1.12.0',\n            'sentence-transformers': '>=2.2.0',\n            'tokenizers': '>=0.13.0',\n            \n            # Vector database\n            'qdrant-client': '>=1.6.0',\n            \n            # Data processing\n            'pandas': '>=1.5.0',\n            'numpy': '>=1.21.0',\n            \n            # Utilities\n            'tqdm': '>=4.64.0',\n            'psutil': '>=5.9.0',\n            'filelock': '>=3.8.0',\n            'hashlib': None,  # Built-in\n            'datetime': None, # Built-in\n            'logging': None,  # Built-in\n            'concurrent.futures': None,  # Built-in\n            'multiprocessing': None,     # Built-in\n            'threading': None,           # Built-in\n            'time': None,                # Built-in\n            'os': None,                  # Built-in\n            'sys': None,                 # Built-in\n            'shutil': None,              # Built-in\n            'tempfile': None,            # Built-in\n            'zipfile': None,             # Built-in\n            'mimetypes': None,           # Built-in\n        }\n        \n        self.failed_imports = []\n        self.successful_imports = []\n    \n    def check_package_version(self, package_name: str, required_version: Optional[str] = None) -> Tuple[bool, str]:\n        \"\"\"Check if package is installed and meets version requirements\"\"\"\n        try:\n            if required_version is None:\n                # Built-in module or no version requirement\n                importlib.import_module(package_name)\n                return True, \"built-in\"\n            \n            installed_version = pkg_resources.get_distribution(package_name).version\n            \n            if required_version.startswith('>='):\n                min_version = required_version[2:]\n                if version.parse(installed_version) >= version.parse(min_version):\n                    return True, installed_version\n                else:\n                    return False, f\"installed: {installed_version}, required: {required_version}\"\n            else:\n                # Exact version match\n                if installed_version == required_version:\n                    return True, installed_version\n                else:\n                    return False, f\"installed: {installed_version}, required: {required_version}\"\n        \n        except (importlib.util.find_spec, pkg_resources.DistributionNotFound, ImportError):\n            return False, \"not installed\"\n    \n    def install_package(self, package_name: str) -> bool:\n        \"\"\"Install a package using pip\"\"\"\n        try:\n            print(f\"Installing {package_name}...\")\n            result = subprocess.run(\n                [sys.executable, '-m', 'pip', 'install', package_name, '--quiet'],\n                capture_output=True,\n                text=True,\n                timeout=300\n            )\n            \n            if result.returncode == 0:\n                print(f\"âœ“ Successfully installed {package_name}\")\n                return True\n            else:\n                print(f\"âœ— Failed to install {package_name}: {result.stderr}\")\n                return False\n        \n        except subprocess.TimeoutExpired:\n            print(f\"âœ— Timeout installing {package_name}\")\n            return False\n        except Exception as e:\n            print(f\"âœ— Error installing {package_name}: {e}\")\n            return False\n    \n    def ensure_dependencies(self, auto_install: bool = True) -> bool:\n        \"\"\"Ensure all required dependencies are installed\"\"\"\n        print(\"Checking dependencies...\")\n        \n        missing_packages = []\n        version_mismatches = []\n        \n        for package, required_version in self.required_packages.items():\n            is_ok, version_info = self.check_package_version(package, required_version)\n            \n            if is_ok:\n                self.successful_imports.append(f\"{package} ({version_info})\")\n            else:\n                if \"not installed\" in version_info:\n                    missing_packages.append(package)\n                else:\n                    version_mismatches.append(f\"{package}: {version_info}\")\n                \n                self.failed_imports.append(f\"{package}: {version_info}\")\n        \n        # Report status\n        print(f\"âœ“ {len(self.successful_imports)} packages available\")\n        \n        if missing_packages:\n            print(f\"âœ— {len(missing_packages)} packages missing: {', '.join(missing_packages)}\")\n        \n        if version_mismatches:\n            print(f\"âš  {len(version_mismatches)} version mismatches:\")\n            for mismatch in version_mismatches:\n                print(f\"  - {mismatch}\")\n        \n        # Auto-install missing packages\n        if auto_install and missing_packages:\n            print(\"\\\\nInstalling missing packages...\")\n            \n            for package in missing_packages:\n                required_version = self.required_packages[package]\n                package_spec = f\"{package}{required_version}\" if required_version else package\n                \n                if self.install_package(package_spec):\n                    # Re-check after installation\n                    is_ok, version_info = self.check_package_version(package, required_version)\n                    if is_ok:\n                        self.successful_imports.append(f\"{package} ({version_info})\")\n                        self.failed_imports = [x for x in self.failed_imports if not x.startswith(package)]\n        \n        return len(self.failed_imports) == 0\n    \n    def import_dependencies(self) -> bool:\n        \"\"\"Import all required dependencies\"\"\"\n        print(\"\\\\nImporting dependencies...\")\n        \n        imports_successful = True\n        \n        try:\n            # Core Python modules\n            global os, sys, json, hashlib, datetime, logging, time, shutil, tempfile, zipfile, mimetypes\n            global threading, multiprocessing, concurrent\n            import os\n            import sys\n            import json\n            import hashlib\n            import datetime\n            import logging\n            import time\n            import shutil\n            import tempfile\n            import zipfile\n            import mimetypes\n            import threading\n            import multiprocessing\n            import concurrent.futures\n            \n            print(\"âœ“ Core Python modules\")\n            \n            # Environment and configuration\n            global Path, load_dotenv\n            from pathlib import Path\n            from dotenv import load_dotenv\n            \n            print(\"âœ“ Environment modules\")\n            \n            # HTTP and API clients\n            global requests, gitlab, urllib3\n            import requests\n            import gitlab\n            import urllib3\n            \n            print(\"âœ“ HTTP and API modules\")\n            \n            # Document processing\n            global docling, pypdf, docx, PIL\n            try:\n                import docling\n                print(\"âœ“ Docling (OCR and document processing)\")\n            except ImportError:\n                print(\"âš  Docling not available - OCR features will be limited\")\n                docling = None\n            \n            import pypdf\n            import docx\n            from PIL import Image\n            \n            print(\"âœ“ Document processing modules\")\n            \n            # Text processing and embeddings\n            global transformers, torch, sentence_transformers, tokenizers\n            import transformers\n            import torch\n            import sentence_transformers\n            import tokenizers\n            \n            print(\"âœ“ Text processing and embedding modules\")\n            \n            # Vector database\n            global qdrant_client\n            import qdrant_client\n            from qdrant_client.models import Distance, VectorParams, PointStruct\n            \n            print(\"âœ“ Vector database modules\")\n            \n            # Data processing\n            global pd, np\n            import pandas as pd\n            import numpy as np\n            \n            print(\"âœ“ Data processing modules\")\n            \n            # Utilities\n            global tqdm, psutil, filelock\n            from tqdm import tqdm\n            import psutil\n            import filelock\n            \n            print(\"âœ“ Utility modules\")\n            \n        except ImportError as e:\n            print(f\"âœ— Import error: {e}\")\n            imports_successful = False\n        \n        return imports_successful\n    \n    def validate_critical_dependencies(self) -> bool:\n        \"\"\"Validate that critical dependencies are working correctly\"\"\"\n        print(\"\\\\nValidating critical dependencies...\")\n        \n        validations_passed = True\n        \n        try:\n            # Test configuration loading\n            from dotenv import load_dotenv\n            print(\"âœ“ Environment configuration\")\n            \n            # Test HTTP requests\n            import requests\n            print(\"âœ“ HTTP client\")\n            \n            # Test GitLab client (without actual connection)\n            import gitlab\n            print(\"âœ“ GitLab client\")\n            \n            # Test document processing\n            import pypdf\n            import docx\n            from PIL import Image\n            print(\"âœ“ Document processors\")\n            \n            # Test embeddings\n            import transformers\n            import torch\n            print(\"âœ“ Embedding models\")\n            \n            # Test vector database\n            import qdrant_client\n            print(\"âœ“ Vector database client\")\n            \n            # Test data processing\n            import pandas as pd\n            import numpy as np\n            print(\"âœ“ Data processing\")\n            \n        except Exception as e:\n            print(f\"âœ— Validation error: {e}\")\n            validations_passed = False\n        \n        return validations_passed\n    \n    def print_summary(self):\n        \"\"\"Print dependency summary\"\"\"\n        print(\"\\\\n\" + \"=\"*60)\n        print(\"DEPENDENCY SUMMARY\")\n        print(\"=\"*60)\n        \n        print(f\"Total packages: {len(self.required_packages)}\")\n        print(f\"Successfully loaded: {len(self.successful_imports)}\")\n        print(f\"Failed/Missing: {len(self.failed_imports)}\")\n        \n        if self.failed_imports:\n            print(\"\\\\nFailed imports:\")\n            for failed in self.failed_imports:\n                print(f\"  âœ— {failed}\")\n        \n        print(\"\\\\nSuccessful imports:\")\n        for success in self.successful_imports:\n            print(f\"  âœ“ {success}\")\n        \n        print(\"=\"*60 + \"\\\\n\")\n\n# Initialize dependency manager\ndep_manager = DependencyManager()\n\n# Ensure all dependencies are available\nif dep_manager.ensure_dependencies(auto_install=True):\n    print(\"âœ… All dependencies satisfied\")\nelse:\n    print(\"âš ï¸ Some dependencies are missing or have version conflicts\")\n\n# Import all dependencies\nif dep_manager.import_dependencies():\n    print(\"âœ… All imports successful\")\nelse:\n    print(\"âŒ Some imports failed\")\n\n# Validate critical functionality\nif dep_manager.validate_critical_dependencies():\n    print(\"âœ… All validations passed\")\nelse:\n    print(\"âŒ Some validations failed\")\n\n# Print summary\ndep_manager.print_summary()\n\n# Configure warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\n\nprint(\"\\\\nðŸš€ Dependencies loaded successfully - Ready for pipeline operations!\")"
  },
  {
   "cell_type": "code",
   "source": "# Error Handling and Monitoring Framework\nimport logging\nimport time\nimport traceback\nimport functools\nfrom typing import Dict, Any, Optional, Callable, List\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nimport json\nfrom pathlib import Path\n\nclass ErrorSeverity(Enum):\n    LOW = \\\"low\\\"\n    MEDIUM = \\\"medium\\\"\n    HIGH = \\\"high\\\"\n    CRITICAL = \\\"critical\\\"\n\nclass ErrorCategory(Enum):\n    NETWORK = \\\"network\\\"\n    PROCESSING = \\\"processing\\\"\n    VALIDATION = \\\"validation\\\"\n    RESOURCE = \\\"resource\\\"\n    CONFIGURATION = \\\"configuration\\\"\n    EXTERNAL_SERVICE = \\\"external_service\\\"\n\n@dataclass\nclass ErrorEvent:\n    timestamp: datetime\n    severity: ErrorSeverity\n    category: ErrorCategory\n    component: str\n    message: str\n    exception_type: str\n    stack_trace: str\n    context: Dict[str, Any] = field(default_factory=dict)\n    retry_count: int = 0\n    resolved: bool = False\n\nclass ErrorHandler:\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.error_log: List[ErrorEvent] = []\n        self.retry_policies: Dict[str, Dict[str, Any]] = {}\n        \n        # Setup logging\n        self._setup_logging()\n        \n        # Initialize default retry policies\n        self._setup_retry_policies()\n        \n        # Performance metrics\n        self.metrics = {\n            'total_errors': 0,\n            'errors_by_category': {},\n            'errors_by_severity': {},\n            'retry_success_rate': 0.0,\n            'avg_resolution_time': 0.0\n        }\n    \n    def _setup_logging(self):\n        \\\"\\\"\\\"Setup comprehensive logging system\\\"\\\"\\\"\n        log_config = self.config.get('logging', {})\n        \n        # Create logs directory\n        log_file = Path(log_config.get('file', './logs/nic_etl.log'))\n        log_file.parent.mkdir(parents=True, exist_ok=True)\n        \n        # Configure logging\n        logging.basicConfig(\n            level=getattr(logging, log_config.get('level', 'INFO')),\n            format='%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',\n            handlers=[\n                logging.FileHandler(log_file),\n                logging.StreamHandler()\n            ]\n        )\n        \n        self.logger = logging.getLogger('NIC_ETL_ErrorHandler')\n        self.logger.info(\\\"Error handling system initialized\\\")\n    \n    def _setup_retry_policies(self):\n        \\\"\\\"\\\"Setup retry policies for different error types\\\"\\\"\\\"\n        self.retry_policies = {\n            'network': {\n                'max_retries': 5,\n                'base_delay': 1.0,\n                'exponential_backoff': True,\n                'max_delay': 60.0\n            },\n            'processing': {\n                'max_retries': 3,\n                'base_delay': 2.0,\n                'exponential_backoff': True,\n                'max_delay': 30.0\n            },\n            'external_service': {\n                'max_retries': 4,\n                'base_delay': 5.0,\n                'exponential_backoff': True,\n                'max_delay': 120.0\n            },\n            'default': {\n                'max_retries': 2,\n                'base_delay': 1.0,\n                'exponential_backoff': False,\n                'max_delay': 10.0\n            }\n        }\n    \n    def handle_error(self,\n                    exception: Exception,\n                    component: str,\n                    category: ErrorCategory,\n                    severity: ErrorSeverity = ErrorSeverity.MEDIUM,\n                    context: Optional[Dict[str, Any]] = None) -> ErrorEvent:\n        \\\"\\\"\\\"Handle and log error event\\\"\\\"\\\"\n        \n        error_event = ErrorEvent(\n            timestamp=datetime.now(),\n            severity=severity,\n            category=category,\n            component=component,\n            message=str(exception),\n            exception_type=type(exception).__name__,\n            stack_trace=traceback.format_exc(),\n            context=context or {}\n        )\n        \n        # Log error\n        self._log_error(error_event)\n        \n        # Store error\n        self.error_log.append(error_event)\n        \n        # Update metrics\n        self._update_metrics(error_event)\n        \n        # Check if alert is needed\n        if severity in [ErrorSeverity.HIGH, ErrorSeverity.CRITICAL]:\n            self._trigger_alert(error_event)\n        \n        return error_event\n    \n    def _log_error(self, error_event: ErrorEvent):\n        \\\"\\\"\\\"Log error with appropriate severity level\\\"\\\"\\\"\n        log_message = f\\\"[{error_event.category.value}] {error_event.component}: {error_event.message}\\\"\n        \n        if error_event.severity == ErrorSeverity.CRITICAL:\n            self.logger.critical(log_message)\n        elif error_event.severity == ErrorSeverity.HIGH:\n            self.logger.error(log_message)\n        elif error_event.severity == ErrorSeverity.MEDIUM:\n            self.logger.warning(log_message)\n        else:\n            self.logger.info(log_message)\n        \n        # Log context if available\n        if error_event.context:\n            self.logger.debug(f\\\"Error context: {json.dumps(error_event.context)}\\\")\n    \n    def _update_metrics(self, error_event: ErrorEvent):\n        \\\"\\\"\\\"Update error metrics\\\"\\\"\\\"\n        self.metrics['total_errors'] += 1\n        \n        # Update category metrics\n        category = error_event.category.value\n        self.metrics['errors_by_category'][category] = (\n            self.metrics['errors_by_category'].get(category, 0) + 1\n        )\n        \n        # Update severity metrics\n        severity = error_event.severity.value\n        self.metrics['errors_by_severity'][severity] = (\n            self.metrics['errors_by_severity'].get(severity, 0) + 1\n        )\n    \n    def _trigger_alert(self, error_event: ErrorEvent):\n        \\\"\\\"\\\"Trigger alert for high-severity errors\\\"\\\"\\\"\n        alert_message = f\\\"ALERT: {error_event.severity.value.upper()} error in {error_event.component}\\\"\n        self.logger.critical(alert_message)\n        \n        # In production, this would integrate with alerting systems\n        # For now, we'll write to a special alert log\n        alert_file = Path('./logs/alerts.log')\n        with open(alert_file, 'a') as f:\n            f.write(f\\\"{datetime.now().isoformat()} - {alert_message}\\\\n\\\")\n    \n    def retry_with_backoff(self, func: Callable, *args, **kwargs) -> Any:\n        \\\"\\\"\\\"Execute function with retry logic and exponential backoff\\\"\\\"\\\"\n        component = kwargs.pop('_component', func.__name__)\n        category = kwargs.pop('_category', ErrorCategory.PROCESSING)\n        \n        # Get retry policy\n        policy_name = category.value if category.value in self.retry_policies else 'default'\n        policy = self.retry_policies[policy_name]\n        \n        last_exception = None\n        \n        for attempt in range(policy['max_retries'] + 1):\n            try:\n                result = func(*args, **kwargs)\n                \n                # Log successful retry\n                if attempt > 0:\n                    self.logger.info(f\\\"Retry successful for {component} on attempt {attempt + 1}\\\")\n                \n                return result\n                \n            except Exception as e:\n                last_exception = e\n                \n                if attempt < policy['max_retries']:\n                    # Calculate delay\n                    if policy['exponential_backoff']:\n                        delay = min(\n                            policy['base_delay'] * (2 ** attempt),\n                            policy['max_delay']\n                        )\n                    else:\n                        delay = policy['base_delay']\n                    \n                    # Log retry attempt\n                    self.logger.warning(\n                        f\\\"Attempt {attempt + 1} failed for {component}, \\\"\n                        f\\\"retrying in {delay:.1f}s: {str(e)}\\\"\n                    )\n                    \n                    time.sleep(delay)\n                else:\n                    # Final failure\n                    error_event = self.handle_error(\n                        e, component, category, ErrorSeverity.HIGH\n                    )\n                    error_event.retry_count = attempt + 1\n        \n        # All retries exhausted\n        raise last_exception\n    \n    def get_error_summary(self, last_hours: int = 24) -> Dict[str, Any]:\n        \\\"\\\"\\\"Get error summary for specified time period\\\"\\\"\\\"\n        cutoff_time = datetime.now() - timedelta(hours=last_hours)\n        \n        recent_errors = [\n            error for error in self.error_log\n            if error.timestamp >= cutoff_time\n        ]\n        \n        summary = {\n            'time_period_hours': last_hours,\n            'total_errors': len(recent_errors),\n            'errors_by_category': {},\n            'errors_by_severity': {},\n            'errors_by_component': {},\n            'critical_errors': [],\n            'top_error_types': {}\n        }\n        \n        # Analyze recent errors\n        for error in recent_errors:\n            # By category\n            category = error.category.value\n            summary['errors_by_category'][category] = (\n                summary['errors_by_category'].get(category, 0) + 1\n            )\n            \n            # By severity\n            severity = error.severity.value\n            summary['errors_by_severity'][severity] = (\n                summary['errors_by_severity'].get(severity, 0) + 1\n            )\n            \n            # By component\n            component = error.component\n            summary['errors_by_component'][component] = (\n                summary['errors_by_component'].get(component, 0) + 1\n            )\n            \n            # Critical errors\n            if error.severity == ErrorSeverity.CRITICAL:\n                summary['critical_errors'].append({\n                    'timestamp': error.timestamp.isoformat(),\n                    'component': error.component,\n                    'message': error.message\n                })\n            \n            # Error types\n            error_type = error.exception_type\n            summary['top_error_types'][error_type] = (\n                summary['top_error_types'].get(error_type, 0) + 1\n            )\n        \n        return summary\n\nclass HealthMonitor:\n    \\\"\\\"\\\"Monitor system health and performance\\\"\\\"\\\"\n    \n    def __init__(self, error_handler: ErrorHandler):\n        self.error_handler = error_handler\n        self.health_checks: Dict[str, Callable] = {}\n        self.performance_metrics: Dict[str, List[float]] = {}\n        self.last_health_check = None\n    \n    def register_health_check(self, name: str, check_func: Callable[[], bool]):\n        \\\"\\\"\\\"Register a health check function\\\"\\\"\\\"\n        self.health_checks[name] = check_func\n    \n    def run_health_checks(self) -> Dict[str, Any]:\n        \\\"\\\"\\\"Run all registered health checks\\\"\\\"\\\"\n        results = {\n            'timestamp': datetime.now().isoformat(),\n            'overall_health': 'healthy',\n            'checks': {},\n            'failed_checks': []\n        }\n        \n        for name, check_func in self.health_checks.items():\n            try:\n                start_time = time.time()\n                is_healthy = check_func()\n                duration = time.time() - start_time\n                \n                results['checks'][name] = {\n                    'status': 'healthy' if is_healthy else 'unhealthy',\n                    'duration_ms': round(duration * 1000, 2)\n                }\n                \n                if not is_healthy:\n                    results['failed_checks'].append(name)\n                    results['overall_health'] = 'degraded'\n                    \n            except Exception as e:\n                results['checks'][name] = {\n                    'status': 'error',\n                    'error': str(e)\n                }\n                results['failed_checks'].append(name)\n                results['overall_health'] = 'degraded'\n        \n        self.last_health_check = results\n        return results\n    \n    def record_performance_metric(self, name: str, value: float):\n        \\\"\\\"\\\"Record a performance metric\\\"\\\"\\\"\n        if name not in self.performance_metrics:\n            self.performance_metrics[name] = []\n        \n        self.performance_metrics[name].append(value)\n        \n        # Keep only last 1000 measurements\n        if len(self.performance_metrics[name]) > 1000:\n            self.performance_metrics[name] = self.performance_metrics[name][-1000:]\n    \n    def get_performance_summary(self) -> Dict[str, Any]:\n        \\\"\\\"\\\"Get performance metrics summary\\\"\\\"\\\"\n        summary = {}\n        \n        for name, values in self.performance_metrics.items():\n            if values:\n                summary[name] = {\n                    'count': len(values),\n                    'avg': sum(values) / len(values),\n                    'min': min(values),\n                    'max': max(values),\n                    'latest': values[-1] if values else 0\n                }\n        \n        return summary\n\n# Decorator for automatic error handling\ndef handle_errors(component: str, \n                 category: ErrorCategory = ErrorCategory.PROCESSING,\n                 severity: ErrorSeverity = ErrorSeverity.MEDIUM,\n                 retry: bool = False):\n    \\\"\\\"\\\"Decorator for automatic error handling\\\"\\\"\\\"\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                if retry:\n                    return error_handler.retry_with_backoff(\n                        func, *args, _component=component, _category=category, **kwargs\n                    )\n                else:\n                    return func(*args, **kwargs)\n            except Exception as e:\n                error_handler.handle_error(e, component, category, severity)\n                raise\n        return wrapper\n    return decorator\n\n# Initialize global error handler\nerror_handler = ErrorHandler(CONFIG)\nhealth_monitor = HealthMonitor(error_handler)\n\n# Register default health checks\ndef check_cache_directory():\n    return CACHE_DIR.exists() and CACHE_DIR.is_dir()\n\ndef check_disk_space():\n    import shutil\n    free_space_gb = shutil.disk_usage(CACHE_DIR).free / (1024**3)\n    return free_space_gb > 1.0  # At least 1GB free\n\nhealth_monitor.register_health_check('cache_directory', check_cache_directory)\nhealth_monitor.register_health_check('disk_space', check_disk_space)\n\nprint(\\\"âœ… Error handling and monitoring system initialized\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: GitLab Connection and Authentication Functions\n",
    "# TO BE IMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Document Retrieval and Caching Functions\n",
    "# TO BE IMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Docling Processing and OCR Functions\n",
    "# TO BE IMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Text Chunking Functions\n",
    "# TO BE IMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Embedding Generation Functions\n",
    "# TO BE IMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Qdrant Integration Functions\n",
    "# TO BE IMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Metadata Management (NIC Schema)\n",
    "# TO BE IMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Main Pipeline Orchestration\n",
    "# TO BE IMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Testing and Validation Functions\n",
    "# TO BE IMPLEMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Pipeline Execution and Monitoring\n",
    "# TO BE IMPLEMENTED"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}